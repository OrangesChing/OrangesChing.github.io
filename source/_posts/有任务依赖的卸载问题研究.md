---
title: 有任务依赖的卸载问题研究
date: 2019-11-21 17:33:02
tags:
  - 任务卸载
  - 论文笔记
categories:
  - 论文
---

# Computation Offloading in Multi-Access Edge Computing Using a Deep Sequential Model Based on Reinforcement Learning

## 场景

任务模型：DAG表示任务依赖

可执行任务角色：MEC / 本地设备

执行调度角色：设备

任务执行方式：MEC服务器一个一个执行

假设： 所有子任务都能卸载

{% asset_img framework.png This is an example image %}

## 目标

最小化代价（例如：延迟（文中），能耗）

## 方法

基于深度强化学习的方法

使用recurrent neural network(RNN)来编码任务信息，使用sequence-to-sequence neural network来为卸载策略建模，输入是已编码任务信息，使用Proximal Policy Optimization(PPO)方法来训练

强化学习三要素的定义：

- 状态： $s=\left(\mathrm{G}, A_{1: i}\right)$， $G$为DAG图，$A_{1: i}$为前 $i$ 个卸载决策
- 动作集：$\mathcal{A}=\{1,0\}$，1为卸载，0为不卸载
- 奖励函数：当做一个决策后（是否卸载），延迟的负增长估计值

## 实验

1. 对比算法

   穷举（Optimal）

   贪婪（HEFT-based）——还引了一篇文章

   轮询（Round-robin）

2. 环境及参数设置

   环境：tensorflow

   任务传输数据大小：5KB到50KB

   任务所需的CPU周期：10到100megacycles

   任务集大小：100个DAG，每个DAG有10个任务

   传输速率：1.2/7/20/30/60Mbps   7Mbps

   计算能力：1GHz(设备)，10GHz(MEC)

3. 实验过程

   用参数产生DAG任务构成任务集，在不同传输速率下对比不同算法的延迟（柱状图）

   用参数产生DAG任务构成任务集，任务集大小以5个步长递增，在不同任务集大小下对比不同算法的延迟（表格）

## 创新点

1. 使用马尔可夫决策过程来构建卸载问题

   状态：任务的卸载决策和DAG图

   动作：每个子任务的卸载决策

   奖励函数：当做一个决策后（是否卸载），延迟的负增长估计值

2. 提出了一个基于S2S的卸载算法框架

   {% asset_img S2S.png This is an example image %}



# Knowledge-Driven Service Offloading Decision for Vehicular Edge Computing: A Deep Reinforcement Learning Approach



这篇论文提的概念是服务卸载，一个服务内有多个任务，任务之间有依赖性，而边缘结点只提供其中几个服务

## 场景

- 可提供服务的角色：云（BS） / MEC（AP） / Ad-hoc对等车辆 （三层结构！）

  BS具有大规模覆盖范围和高性能计算，通过3G / 4G接入对应的较远节点网络，影响QoS

  AP覆盖范围有限，计算性能中等，如果太多的车辆访问同一无线同时网络，稀缺的带宽可能会影响应用程序的QoS
  
- 执行调度角色：设备

  {% asset_img 2_environment.png This is an example image %}

- 任务模型：DAG（并行的任务算最长的那个任务的时间）

- 对卸载的描述：

> The heterogeneous resources provided by vehicular edge computing nodes are abstracted to several containers with specific functions and parameters. The multi-task in a complicated application is modeled as a specific dataflow graph and denoted by DAG, depicted in Fig. 2. Due to the differences existing in the edge computing nodes, including several cloudlets, BS that contains computing and storage capability and the neighboring vehicles on the road, each task in a service has multiple offloading destinations.

  {% asset_img 2_service.png This is an example image %}

## 目标

最小化延迟

## 架构 / 模型

### KD service offloading decision framework

- 决策模块(decision model)：DRL算法

- 观察函数(observation function)：根据计算能力排序这三类的所有可接入结点，作为候选卸载目标

### 构建的模型

#### 任务延迟

$$
D_{i}=\frac{f_{i}^{t}}{f_{c_{i}}}+\frac{d_{i}^{u}}   {b_{c_{i}}}+\frac{d_{i-1, i}}{b_{c_{i-1}, c_{i}}}
$$

计算时间 + 任务$i-1$传输结果到$i$所需的时间 + 互动数据的传输时间

注：这里的计算量$f_{i}^{t}$是任务$i$的指令条数

#### 通信模型

- BS（使用4G通信）
  $$
  b_{c_{i}}=w \log _{2} \frac{1+q_{v} g_{v, c_{i}}}{\varpi_{0}+\sum_{u \in U_{c_{i}}, u \neq v} q_{u} g_{u, c_{i}}}, c_{i} \in B S
  $$

- AP（使用WLAN通信）
  $$
  b_{c_{i}}=\frac{h p_{t} L}{\left(1-p_{t}\right)\left(1+\tau_{s}\right)+\left[\left(1-p_{t}\right)^{1-h}- \left(1-p_{t}-h p_{t}\right)\right] p_{t}}, c_{i} \in AP
  $$

  $$
  p_{t}=\frac{2\left(1-2 p_{c}\right)}{\left(1-2 p_{c}\right)\left(W_{m i n}+1\right)+p_{c} W_{m i n}\left[1-\left(2 p_{c}\right)^{m_{b}}\right]}
  $$

  $p_{t}$是设备在一个时隙传输一个包的概率

#### 移动带来的性能开销

- BS/AP结点——切换时间

  BS和AP可以根据车辆的移动，将卸载的任务迁移到新节点，这个时间叫**切换时间**（$D_h$）

  **切换的次数**由任务执行时间（假设为指数分布），在范围的滞留时间（符合一个概率分布$f_{res}(x)$），公式如下：
  $$
  N _ { c _ { i } } ^ { h } = \sum _ { k = 0 } ^ { \infty } k P ( k ) = \sum _ { k = 1 } ^ { \infty } \frac { k \mu _ { i } } { \eta _ { c _ { i } } } \left[ 1 - f _ { r e s } ^ { * } \left( \mu _ { i } \right) \right] ^ { 2 } \left[ f _ { r e s } ^ { * } \mu _ { i } \right] ^ { k - 1 }=\frac{\eta_{c_i}}{\mu_i}
  $$
  一个任务的总切换时间为$D_hN_{c_i}^{h}$

- 车结点——任务重启时间

   1. 使用了离散时间有限状态马尔可夫链为车距建模

     状态：$X=\{x_1,x_2,x_3,\cdots,x\}$其中$x_i \in [z_{min},z_{max}]$是任务执行期间每个时间步长上，车与邻居结点的距离

     状态转移概率：$X_j$在下一时间戳可转化为前一个状态($q _ { j }$)、当前状态($l_j$)、后一个状态($p _ { j }$)，转移概率公式为
   $$
     \begin{array} { l } { p _ { j } = p \left[ 1 - \beta \left( 1 - \frac { z _ { \min } + j z } { z _ { \max } } \right) \right] } \\ { q _ { j } = q \left[ 1 - \beta \left( 1 - \frac { \left( z _ { \min } + j z \right.} { z _ { \max } } \right) \right] } \\ { l _ { j } = 1 - p _ { j } - q _ { j } , 0 \leq p , q , \beta \leq 1 } \end{array}
   $$
     其中 $p，q，\beta$ 根据车辆密度设定。直观的看如果$\beta$为0，则状态转移概率依赖于当前状态

     单步状态转移概率矩阵：
   $$
     Q = \left( \begin{array} { c c c c c c } { l _ { 0 } } & { p _ { 0 } } & { 0 } & { \cdots } & { \cdots } & { 0 } \\ { q _ { 0 } } & { l _ { 1 } } & { p _ { 1 } } & { \cdots } & { \cdots } & { 0 } \\ { \cdots } & { \cdots } & { \cdots } & { \cdots } & { \cdots } & { 0 } \\ { 0 } & { 0 } & { 0 } & { \cdots } & { l _ { z ^ { * } } } & { 0 } \\ { 0 } & { 0 } & { 0 } & { \cdots } & { \cdots } & { 0 } \end{array} \right)
   $$
     状态转移概率矩阵：

     使用 $P_j^I$ 来表示状态$X_j$在第$I$步的状态转移概率。则状态转移概率为
   $$
     \pi(\zeta) = \left( P _ { 0 } ^ { \zeta } , P _ { 1 } ^ { \zeta } , P _ { 2 } ^ { \zeta } , \ldots , P _ { X _ { z } } ^ { \zeta } , \ldots , P _ { X _ { \max } } ^ { \zeta } \right)
   $$
     $\zeta$ 为时间步数，$0 < \zeta \leq \mathbb { 1 } / k \mu _ { i }$
     递推公式为
   $$
     \pi ( \zeta ) = \pi ( 0 ) Q ^ { \zeta } = \pi ( 0 ) \times \underbrace { ( Q \times \cdots \times Q ) } _ { \zeta }
   $$
   
   2. 建模完成后，可得到车结点$c_i$对任务$i$的可用性为（是个概率）
      $$
      R _ { c _ { i } } = \sum _ { j = 0 } ^ { z ^ { * } } P _ { j } ^ { \zeta }
      $$
当车离开范围时，任务需重新执行，时间为$R _ { c _ { i } }\left( \frac { f _ { i } ^ { t } } { f ^ { c _ { i } = l o } } + \frac { d _ { i - 1 , i } } { b _ { i - 1 , c _ { i } = l o } } \right)$
#### 实际任务执行延迟

$$
D _ { i } ^ { \prime } = \left\{ \begin{array} { l l } { D _ { i } + D _ { h } N _ { c _ { i } } ^ { h } , } & { c _ { i } \in B S \cup A P } \\ { D _ { i } + R _ { c _ { i } } \left( \frac { f _ { i } ^ { t } } { f ^ { c _ { i } = l o } } + \frac { d _ { i - 1 , i } } { b _ { i - 1 , c _ { i } = l o } } \right) , } & { c _ { i } \in V N } \end{array} \right.
$$
即在原$D_i$的基础上考虑了车辆移动所带来的影响：对于BS\AP，会有切换迁移时间；对于车结点，会有任务失败重新执行的时间


## 方法

使用A3C算法

> 异步的优势行动者评论家算法（Asynchronous Advantage Actor-Critic，A3C）是Mnih等人根据异步强化学习（Asynchronous Reinforcement Learning， ARL） 的思想，提出的一种轻量级的 DRL 框架，该框架可以使用异步的梯度下降法来优化网络控制器的参数。基于AC框架，该框架集成了值函数估计算法和策略搜索算法。

强化学习三要素的定义：

- 状态：$S=(T,C,v)$

  $T$：任务信息向量。$T=(f_i^t,d_i^u,d_{i-1,i})$，分别为计算量，互动信息大小，输入数据大小

  $C$：目标卸载点信息。$C=(c_{type},f_{c_i},b{c_i},N_{c_i}^h,R_{c_i})$，分别为结点类型，计算速度，传输速率，切换时间，结点可用性

  $v$：车辆行驶速度

- 动作集：$A=(a_{local},a_1,\cdots,a_m,\cdots)$，$a=1$表示卸载

- 奖励函数：真实世界的执行延迟（例如：$D_i'$）

## 实验

1. 对比算法
   贪婪算法

2. 环境及参数设置

   - 结点信息：
   
     
     
|                                 | BS      | AP      | vehicle                      |
| ------------------------------- | ------- | ------- | ---------------------------- |
| 结点个数                        | 2       | 2       | 6                            |
| 计算能力（标准差为5的正态分布） | 560,676 | 526,430 | 124, 120, 177, 144, 165, 130 |


​    
   - 带宽：
   
     BS->BS/AP AP->AP/车：100M
     
     BS->车：50M
     
     车->车：300M
     
      - 任务信息：
      
        在docker中跑了对象检测，特征学习，图像自动注释，图像分割和基于位置的推荐 等应用
      
        根据这些服务生成数据集，包括每个服务的任务信息。数据包含任务所需的CPU周期，传输数据量和相关性。此外，在实验中，两个或三个任务合并为一个任务，将它们部署在一个节点上。因此，实验中任务个数会从5到30
      
      - 车辆移动数据
      
        根据前面说的移动模型产生？？？？？
      
        

3. 实验过程

   - 车辆移动对卸载的影响（测试切换时间和重启时间）
   
     滞留时间对切换时间的影响
   
     任务执行时间对切换时间的影响
   
     不同$q，\beta$（前面建模公式的参数） 对可用性的影响
   
     最大通信方位对可用性的影响
     
   - DEL模型分析
   
     收敛性
     
     各种参数下的算法性能
     
   - 服务延迟对比
     
     对比算法：贪婪
     
     任务大小对延迟的影响
     
     任务个数对延迟的影响
     
     

## 创新点

1. 提出了一种基于DRL的卸载决策模型，模型考虑了资源需求，接入网络，用户移动性和任务依赖性
2. 构建了车辆移动性模型用于卸载决策，通过所访问的车辆边缘计算节点，来构建车辆移动对任务延迟的影响程度
3. 使用A3C算法，实现移动车辆在线优化卸载决策