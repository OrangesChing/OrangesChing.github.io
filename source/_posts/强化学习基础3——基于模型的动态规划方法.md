---
title: 强化学习基础3——基于模型的动态规划方法
date: 2019-11-30 14:32:36
tags:
  - 机器学习
  - 强化学习
categories:
  - 机器学习
---

上一章我们将强化学习的问题纳入了马尔可夫决策过程的框架下解决。马尔可夫决策过程可以利用元组$<S,A,P,R,\gamma>$来描述，而根据转移概率P是否已知，可以分为基于模型的动态规划方法和基于无模型的强化学习方法。这两类都包括策略迭代算法、值迭代算法、策略搜索算法。

本讲着重讲解了利用动态规划来进行强化学习，具体是进行强化学习中的“规划”问题，也就是在已知模型的基础上判断一个策略的价值函数，并在此基础上寻找到最优的策略和最优价值函数，或者直接寻找最优策略和最优价值函数。本讲是整个强化学习课程核心内容的引子。

# 简介

**动态规划算法**是解决复杂问题的一个方法，算法通过把复杂问题分解为子问题，通过求解子问题进而得到整个问题的解。在解决子问题的时候，其结果通常需要存储起来被用来解决后续复杂问题。当问题具有下列特性时，通常可以考虑使用动态规划来求解：第一个特性是一个复杂问题的最优解由数个小问题的最优解构成，可以通过寻找子问题的最优解来得到复杂问题的最优解；子问题在复杂问题内重复出现，使得子问题的解可以被存储起来重复利用。

马尔科夫决定过程（MDP）具有上述两个属性：Bellman方程把问题递归为求解子问题，价值函数就相当于存储了一些子问题的解，可以复用。因此可以使用动态规划来求解MDP。

我们用动态规划算法来求解一类称为“规划”的问题。“规划”指的是在了解整个MDP的基础上求解最优策略，也就是清楚模型结构的基础上：包括状态行为空间、转换矩阵、奖励等。这类问题不是典型的强化学习问题，我们可以用**规划**来进行**预测**和**控制**。

具体的数学描述是这样：

**预测：**给定一个MDP $<S,A,P,R,\gamma>$ 和策略 $\pi$  ，或者给定一个MRP$<S,P^{\pi},R^{\pi},\gamma>$  ，要求输出基于当前策略π的价值函数 $V_{\pi}$ 

**控制：**给定一个MDP$<S,A,P,R,\gamma>$  ，要求确定最优价值函数 $V_{*}$ 和最优策略 $\pi_{*}$ 

# 策略迭代 Policy Iteration

在讨论策略迭代算法前，先讨论两个问题：策略评估和策略改善

## 策略评估

### 理论

**问题：**评估一个给定的策略π，也就是解决“预测”问题。

**解决方案：**反向迭代应用Bellman期望方程

**具体方法：**

- **同步反向迭代**，即在每次迭代过程中，对于第$k+1$ 次迭代，所有的状态s的价值用$v_k(s')$ 计算并更新该状态第$k+1$  次迭代中使用的价值$v_k(S)$ ，其中$s’$是$s$的后继状态。此种方法通过反复迭代最终将收敛至$V_{\pi}$  。
  $$
  v _ { k + 1 } ( s ) = \sum _ { a \in A } \pi ( a | s ) \left( R _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in S } P _ { s s ^ { \prime } } ^ { a } v _ { k } \left( s ^ { \prime } \right) \right)
  $$
  即：一次迭代内，状态s的价值等于前一次迭代该状态的即时奖励与所有s的下一个可能状态s' 的价值与其概率乘积的和，如图示：
  {% asset_img policy_iteration1.png This is an example image %}
  公式的矩阵形式是：
  $$
  \mathbf { v } ^ { k + 1 } = \mathcal { R } ^ { \pi } + \gamma \mathcal { P } ^ { \pi } \mathbf { v } ^ { k }
  $$
  
- **异步反向迭代**，即在第k次迭代使用**当次**迭代的状态价值来更新状态价值。

### 示例——方格世界
{% asset_img policy_iteration_example1.png This is an example image %}
**已知：**
- **状态空间S**：如图。S1 - S14非终止状态，ST终止状态，下图灰色方格所示两个位置；
- **行为空间A**：$\{n, e, s, w\} $对于任何非终止状态可以有东南西北移动四个行为；
- **转移概率P：**任何试图离开方格世界的动作其位置将不会发生改变，其余条件下将100%地转移到动作指向的状态；
- **即时奖励R：**任何在非终止状态间的转移得到的即时奖励均为-1，进入终止状态即时奖励为0；
- **衰减系数γ：**1；
- **当前策略π：**Agent采用随机行动策略，在任何一个非终止状态下有均等的几率采取任一移动方向这个行为，即$π(n|\cdot) = π(e|\cdot) = π(s|\cdot) = π(w|\cdot) = 1/4$。

**问题：**评估在这个方格世界里给定的策略。
   该问题等同于：求解该方格世界在给定策略下的（状态）价值函数，也就是求解在给定策略下，该方格世界里每一个状态的价值。

**求解：迭代法进行策略评估**
{% asset_img policy_iteration_example2.png This is an example image %}
状态价值在第153次迭代后收敛

## 策略改善

### 理论

通过方格世界的例子，我们得到了一个优化策略的办法，分为两步：首先我们在一个给定的策略下迭代更新价值函数：
$$
v _ { \pi } ( s ) = \mathbb { E } \left[ R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots | S _ { t } = s \right]
$$
随后，在当前策略基础上，贪婪地选取行为，使得后继状态价值增加最多：
$$
\pi' = greedy(v_{\pi})
$$
在刚才的格子世界中，基于给定策略的价值迭代最终收敛得到的策略就是最优策略，但通过一个回合的迭代计算价值联合策略改善就能找到最优策略不是普遍现象。通常，还需在改善的策略上继续评估，反复多次。不过这种方法总能收敛至最优策略$\pi^*$ 。

### 理论证明

1. 考虑一个确定的策略（$a=\pi(s)$）： 

   为什么证明这个要考虑确定的策略？因为策略改善使用的贪婪策略，是将一个普通策略（在这个状态下做哪个行动的概率是***）变成了贪婪策略（在这个状态下就做这个行动，这个行动是所有行动里值函数最大的那个）。即为确定策略。

   对于确定策略，由于$\pi(s)=a$（即在这个情况下一定选择这个策略），值函数公式 $v _ { \pi } ( s ) = \sum _ { a \in A } \pi ( a | s ) q _ { \pi } ( s , a )$等于$q _ { \pi } ( s , a )$ 等于$R_s^a + \gamma\sum _ { s' \in S } P_{ss'}^a v _ { \pi } ( s' )$（上一章的知识点）

2. 通过贪婪计算优化策略（贪的行为价值函数）：
   $$
   \pi'(s) = \underset { a \in \mathcal { A } } { \operatorname { argmax } } q _ { \pi } ( s , a )
   $$

3. 接着推导
   $$
   q_{\pi}(s,\pi'(s)) = \underset { a \in \mathcal { A } } { \operatorname { max } } q _ { \pi } ( s , a ) \geq q _ { \pi } ( s , \pi(s) ) = v_{\pi}(s)
   $$
   由于策略是贪婪q值得到的，则这个更新策略后的q值一定不小于更新前的q值，而根据1所述，$q _ { \pi } ( s , \pi(s) ) = v_{\pi}(s)$。

   

4. 而根据上一章的Bellman最优方程，我们找最优策略的目标是最大化v值，现在的贪婪策略在最大化比v值还大的值，等同于在最大化v值，即v值在每一次迭代都会更大。当$q_{\pi}(s,\pi'(s)) = \underset { a \in \mathcal { A } } { \operatorname { max } } q _ { \pi } ( s , a ) = q _ { \pi } ( s , \pi(s) ) = v_{\pi}(s)$时就可以结束迭代了，因为满足Bellman最优方程，说明当前策略下的状态价值就是最优状态价值。因而此时的策略就是最优策略。

   （$v_{\pi'}(s)\geq v_{\pi}(s)$证明见David Silver视频PPT）

## **策略迭代算法=策略评估+策略改善**

### 算法

在当前策略上迭代计算$v$值，再根据$v$值贪婪地更新策略，如此反复多次，最终得到最优策略$\pi^*$ 和最优状态价值函数$V^*$  

**贪婪** 指的是仅采取那个（些）使得状态价值得到最大的行为。

{% asset_img policy_iteration2.png This is an example image %}

{% asset_img policy_iteration_algorithm.png This is an example image %}

### 示例——连锁汽车租赁

举了一个汽车租赁的例子，说明如何在给定策略下得到基于该策略的价值函数，并根据更新的价值函数来调整策略，直至得到最优策略和最优价值函数。

一个连锁汽车租赁公司有两个地点提供汽车租赁，由于不同的店车辆租赁的市场条件不一样，为了能够实现利润最大化，该公司需要在每天下班后在两个租赁点转移车辆，以便第二天能最大限度的满足两处汽车租赁服务。

**已知**

**状态空间：**2个地点，每个地点最多20辆车供租赁

**行为空间：**每天下班后最多转移5辆车从一处到另一处；

**即时奖励：**每租出1辆车奖励10元，必须是有车可租的情况；不考虑在两地转移车辆的支出。

**转移概率：**求租和归还是随机的，但是满足泊松分布  。第一处租赁点平均每天租车请求3次，归还3次；第二处租赁点平均每天租车4次，归还2次。

**衰减系数** $\gamma$：0.9；

**问题：**怎样的策略是最优策略？

**求解方法：**从一个确定的策略出发进行迭代，该策略可以是较为随意的，比如选择这样的策略：不管两地租赁业务市场需求，不移动车辆。以此作为给定策略进行价值迭代，当迭代收敛至一定程度后，改善策略，随后再次迭代，如此反复，直至最终收敛。

在这个问题中，状态用两个地点的汽车存量来描述，比如分别用c1,c2表示租赁点1,2两处的可租汽车数量，可租汽车数量同时参与决定夜间可转移汽车的最大数量。

解决该问题的核心就是依据泊松分布确定状态<c1,c2>的即时奖励，进而确定每一个状态的价值。

{% asset_img policy_iteration_example3.png This is an example image %}

## 改进策略迭代

有时候不需要持续迭代至最有价值函数，可以设置一些条件提前终止迭代

- 比如设定一个$ \epsilon $，比较两次迭代的价值函数平方差
- 直接设置迭代次数

# 价值迭代 Value Iteration

## 引入

### 优化原则 Principle of Optimality

一个最优策略可以被分解为两部分

1. 从状态 $s$ 到下一个状态 $s’$ 采取了最优行为$A_*$  
2. 在状态 $s’$ 时遵循一个最优策略

> **定理**
>
> 一个策略能够使得状态s获得最优价值，当且仅当：对于从状态$s$可以到达的任何状态$s’$，该策略能够使得状态s’的价值是最优价值：
>

### 确定性的价值迭代

如果我们知道了子问题$v_*(s')$，则$v_*(s)$只需要通过一步就能得到
$$
v _ { * } ( s ) \leftarrow \max _ { a \in \mathcal { A } } \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)
$$
**示例——最短路径**

{% asset_img value_iteration_example3.png This is an example image %}

**问题：**如何在一个4*4的方格世界中，找到任一一个方格到最左上角方格的最短路径

如果我们清楚地知道我们期望的最终（goal）状态的位置以及反推需要明确的状态间关系，那么可以认为是一个**确定性的价值迭代**。此时，我们可以把问题分解成一些列的子问题，从最终目标状态开始分析，逐渐往回推，直至推至所有状态。

简要思路：在已知左上角为最终目标的情况下，我们可以从与左上角相邻的两个方格开始计算，因为这两个方格是可以仅通过1步就到达目标状态的状态，或者说目标状态是这两个状态的后继状态。最短路径可以量化为：每移动一步获得一个-1的即时奖励。为此我们可以更新与目标方格相邻的这两个方格的状态价值为-1。如此依次向右下角倒推，直至所有状态找到最短路径。

## 价值迭代算法 value iteration

**问题：**寻找最优策略π

**解决方案：**采用Bellman最优方程，从初始状态价值开始同步迭代计算，最终收敛，整个过程中没有遵循任何策略
$$
V _ { k + 1 } ( s ) = \max _ { a } \sum _ { s ^ { \prime } , r } P \left( s ^ { \prime } , r | s , a \right) \left( r + \gamma V _ { k } \left( s ^ { \prime } \right) \right)
$$
{% asset_img value_iteration_algorithm.png This is an example image %}

对每一个当前状态 $s$ ,对每个可能的动作 $a$ 都计算一下采取这个动作后到达的下一个状态的期望价值。看看哪个动作可以到达的状态的期望价值函数最大，就将这个最大的期望价值函数作为当前状态的价值函数 $V(s)$ 循环执行这个步骤，直到价值函数收敛。

## 价值迭代算法与策略迭代算法的区别

- 策略迭代有一个策略直接作用于value空间（即不会有value值来构建策略，策略再构建value值的过程）；而价值迭代过程其间得到的价值函数，不对应任何策略
- 值迭代是根据状态期望值选择动作，而策略迭代是先估计状态值然后修改策略 

# 同步动态规划问题总结

{% asset_img conclusion.png This is an example image %}

- **预测问题**：在给定策略下迭代计算价值函数。

- **控制问题**：策略迭代寻找最优策略问题则先在给定或随机策略下计算状态价值函数，根据状态函数贪婪更新策略，多次反复找到最优策略；单纯使用价值迭代，全程没有策略参与也可以获得最优策略，但需要知道状态转移矩阵，即状态s在行为a后到达的所有后续状态及概率。

   使用状态价值函数或行为价值函数两种价值迭代的算法时间复杂度都较大，为 $O(mn^2)$ 或$O(m^2n^2)$  。一种改进方案是使用异步动态规划，其他的方法即放弃使用动态规划，随后的几讲中将详细讲解其他方法。

# 动态规划的一些扩展

## 异步动态规划 Asynchronous Dynamic Programming

几个可能改进的点子

### 原位动态规划(In-place dynamic programming)

直接原地更新下一个状态的v值，而不像同步迭代那样需要额外存储新的v值。在这种情况下，**按何种次序更新状态价值有时候会比较有意义**。
$$
v ( s ) \leftarrow \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v \left( s ^ { \prime } \right) \right)
$$

### 重要状态优先更新(Priortised Sweeping)

对那些重要的状态优先更新。

使用Bellman error：

$$
\max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v \left( s ^ { \prime } \right) \right) - v ( s )
$$
来确定哪些状态是比较重要的。Bellman error 反映的是当前的状态价值与更新后的状态价值差的绝对值。Bellman error越大，越有必要优先更新。对那些Bellman error较大的状态进行备份。这种算法使用优先级队列能够较得到有效的实现。

### 实时动态规划(Real-time dynamic programming)

更新那些仅与个体关系密切的状态，同时使用个体的经验来知道更新状态的选择。有些状态虽然理论上存在，但在现实中几乎不会出现。利用已有现实经验。
$$
v \left( S _ { t } \right) \leftarrow \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { S _ { t } } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { S _ { t s ^ { \prime } } } ^ { a } v \left( s ^ { \prime } \right) \right)
$$
St是实际与Agent相关或者说Agent经历的状态，可以省去关于那些仅存在理论上的状态的计算。



## 采样更新 Sample Backups

动态规划使用full-width backups。意味着使用DP算法，对于每一次状态更新，都要考虑到其所有后继状态及所有可能的行为，同时还要使用MDP中的状态转移矩阵、奖励函数（信息）。DP解决MDP问题的这一特点决定了其对中等规模（百万级别的状态数）的问题较为有效，对于更大规模的问题，会带来Bellman维度灾难。

因此在面对大规模MDP问题是，需要寻找更加实际可操作的算法，主要的思想是Sample Backups，后续会详细介绍。这类算法的优点是不需要完整掌握MDP的条件（例如奖励机制、状态转移矩阵等），通过Sampling（举样）可以打破维度灾难，反向更新状态函数的开销是常数级别的，与状态数无关。



## 近似动态规划 Approximate Dynamic Programming

使用其他技术手段（例如神经网络）建立一个参数较少，消耗计算资源较少、同时虽然不完全精确但却够用的近似价值函数：
$$
\bar { v } _ { k } ( s ) = \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v \left( s ^ { \prime } , w _ { k } \right) \right)
$$


注：本讲的内容主要还是在于理解强化学习的基本概念，各种Bellman方程，在实际应用中，很少使用动态规划来解决大规模强化学习问题。