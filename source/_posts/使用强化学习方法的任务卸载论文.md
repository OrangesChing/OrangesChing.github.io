---
title: 使用强化学习方法的任务卸载论文
date: 2020-01-03 18:01:14
tags:
---

# Deep Reinforcement Learning for Online Computation Offloading in Wireless Powered Mobile-Edge Computing Networks

## 场景

可执行任务角色：MEC / 本地设备

执行调度角色：设备

{% asset_img example.png This is an example image %}

## 目标

研究问题：在时分无线通道下，合理分配各设备的卸载、传输、无线充电（WPT）时间

最大化所有设备的加权总计算速率

## 模型

### 系统模型

系统时间以时长T划分为相等的一段一段

## 方法

基于深度强化学习的方法

使用recurrent neural network(RNN)来编码任务信息，使用sequence-to-sequence neural network来为卸载策略建模，输入是已编码任务信息，使用Proximal Policy Optimization(PPO)方法来训练

强化学习三要素的定义：

- 状态： $s=\left(\mathrm{G}, A_{1: i}\right)$， $G$为DAG图，$A_{1: i}$为前 $i$ 个卸载决策
- 动作集：$\mathcal{A}=\{1,0\}$，1为卸载，0为不卸载
- 奖励函数：当做一个决策后（是否卸载），延迟的负增长估计值

## 实验

1. 对比算法

   穷举（Optimal）

   贪婪（HEFT-based）——还引了一篇文章

   轮询（Round-robin）

2. 环境及参数设置

   环境：tensorflow

   任务传输数据大小：5KB到50KB

   任务所需的CPU周期：10到100megacycles

   任务集大小：100个DAG，每个DAG有10个任务

   传输速率：1.2/7/20/30/60Mbps   7Mbps

   计算能力：1GHz(设备)，10GHz(MEC)

3. 实验过程

   用参数产生DAG任务构成任务集，在不同传输速率下对比不同算法的延迟（柱状图）

   用参数产生DAG任务构成任务集，任务集大小以5个步长递增，在不同任务集大小下对比不同算法的延迟（表格）

## 创新点

1. 使用马尔可夫决策过程来构建卸载问题

   状态：任务的卸载决策和DAG图

   动作：每个子任务的卸载决策

   奖励函数：当做一个决策后（是否卸载），延迟的负增长估计值

2. 提出了一个基于S2S的卸载算法框架

   {% asset_img S2S.png This is an example image %}

