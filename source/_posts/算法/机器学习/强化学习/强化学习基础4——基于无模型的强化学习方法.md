---
title: 强化学习基础4——基于无模型的强化学习方法
date: 2019-12-02 15:07:43
tags:
  - 强化学习
categories:
  - 算法
  - 机器学习
  - 强化学习
---

# 简介 Introduction

马尔可夫决策过程可以利用元组$<S,A,P,R,\gamma>$来描述，而根据转移概率P是否已知，可以分为基于模型的动态规划方法和基于无模型的强化学习方法。这两类都包括策略迭代算法、值迭代算法、策略搜索算法。

上一章学习了基于模型的动态规划方法，学习了如何从理论上解决一个**已知**的MDP：通过动态规划来评估一个给定的策略，并且得到最优价值函数，根据最优价值函数来确定最优策略；也可以直接进行不基于任何策略的状态价值迭代得到最优价值函数和最优策略。

这一章我们将讨论解决一个可以被认为是MDP、但却不掌握MDP具体细节（不知道环境是怎样的）的问题，也就是讲述如何直接从Agent与环境的交互来得得到一个估计的最优价值函数和最优策略。这部分内容分为两部分

- 第一部分聚焦于策略评估，也就是预测，直白的说就是在给定的策略同时不清楚MDP细节的情况下，估计得到值函数。

- 第二部分将利用第一部分的策略评估的主要观念来进行控制进而找出最优策略，最大化Agent的奖励。

# 策略评估（无模型的预测）

这部分内容分为三个小部分，分别是蒙特卡洛强化学习、时序差分强化学习和介于两者之间的$\lambda$时序差分强化学习

## 蒙特卡洛强化学习 Monte-Carlo Reinforcement Learning

### 启发（为什么会想到这个方法）

前面讲的主要内容是整个问题可以转换成一个马尔科夫决策过程(MDP)五元组，但是，在现实世界中，我们无法同时知道这个5元组。比如P，状态转移概率就很难知道，P不知道，我们就无法使用bellman方程来求解V和Q值。但是我们依然要去解决这个问题。怎么办？

一个想法是，虽然我不知道状态转移概率P，但是这个概率是真实存在的。我们可以直接去尝试，不断采样，然后会得到奖赏，通过奖赏来评估值函数。这个想法与蒙特卡罗方法的思想是一致的。我们可以尝试很多次，最后估计的V值就会很接近真实的V值了。

{% asset_img monte_carlo.png This is an example image %}

比如上图，矩形的面积我们可以轻松得到，但是对于阴影部分的面积，我们积分是比较困难的。所以为了计算阴影部分的面积，我们可以在矩形上均匀地撒豆子，然后统计在阴影部分的豆子数占总的豆子数的比例，就可以估算出阴影部分的面积了

### 定义

**蒙特卡洛强化学习**：又叫统计模拟方法，在不清楚MDP状态转移及即时奖励的情况下，直接从经历过的**完整Episode**来学习状态价值，通常情况下**某状态的价值等于在多个Episode中以该状态算得到的所有收益的平均**。

**目标：**在给定策略下，从一系列的完整Episode经历中学习得到该策略下的状态价值函数。

**蒙特卡洛强化学习的特点**：不基于模型本身，直接从经历过的Episode中学习，必须是**完整的Episode**

**蒙特卡洛强化学习使用的思想**就是**使用平均收获值代替价值**。理论上Episode越多，结果越准确。

> **Episode**
>
> episode就是经历，每条episode就是一条从起始状态到结束状态的经历。例如在走迷宫，一条episode就是从你开始进入迷宫，到最后走出迷宫的路径。
>
> 首先我们要得到的是某一个状态$s$的平均收获。所以我们说的episode要经过状态$s$。所以下图中第二条路径没有经过状态s，对于s来说就不能使用它了。而且最后我们episode都是要求达到终点的，才能算是一个episode。
>
> {% asset_img monte_carlo.png This is an example image %}
>
> Episode其包含的信息有：状态的转移、使用的行为序列、中间状态获得的即时奖励以及到达终止状态时获得的即时奖励。
>
> 完整的Episode 指必须从某一个状态开始，Agent与Environment交互直到终止状态，环境给出终止状态的即时收获为止。
>
> 完整的Episode不要求起始状态一定是某一个特定的状态，但是要求个体最终进入环境认可的某一个终止状态。

数学描述如下：

- 基于特定策略 $\pi$  的一个Episode信息可以表示为如下的一个序列：

$$
S _ { 1 } , A _ { 1 } , R _ { 2 } , S _ { 2 } , A _ { 2 } , \ldots , S _ { t } , A _ { t } , R _ { t + 1 } , \ldots , S _ { k } \sim \pi
$$

- $t$时刻状态 $S_t$ 的收益：

$$
G _ { t } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { T - 1 } R _ { T }
$$

其中 $T$ 为终止时刻。

- 该策略下某一状态 $s$ 的价值：

$$
v _ { \pi } ( s ) = E _ { \pi } \left[ G _ { t } | S _ { t } = s \right]
$$

#### 重复状态的收获计算

在状态转移过程中，可能发生一个状态经过一定的转移后又一次或多次返回该状态，此时在一个Episode里如何计算这个状态发生的次数和计算该Episode的收获呢？可以有如下两种方法：

##### 首次访问蒙特卡洛策略评估(First-Visit Monte-Carlo Policy Evaluation)

在给定一个策略，使用一系列完整Episode评估某一个状态$s$时，对于每一个Episode，仅当该状态**第一次**出现时列入计算

状态出现的次数加1： 
$$
N ( s ) \leftarrow N ( s ) + 1
$$
总的收获值更新：
$$
S ( s ) \leftarrow S ( s ) + G_t
$$
状态s的价值：
$$
V(s) = \frac {S(s)}{N(s)}
$$

当$N ( s ) \rightarrow \infty $ 时，$V(s) \leftarrow v_{\pi}(s)$ 

{% asset_img First_visit_mc.jpg This is an example image %}

##### 每次访问蒙特卡洛策略评估

在给定一个策略，使用一系列完整Episode评估某一个状态$s$时，对于每一个Episode，状态s**每次**出现在状态转移链时，计算的具体公式与上面的一样，但具体意义不一样。

状态出现的次数加1： 
$$
N ( s ) \leftarrow N ( s ) + 1
$$
总的收获值更新：
$$
S ( s ) \leftarrow S ( s ) + G_t
$$
状态s的价值：
$$
V(s) = \frac {S(s)}{N(s)}
$$

当$N ( s ) \rightarrow \infty $ 时，$V(s) \leftarrow v_{\pi}(s)$ 



####　示例：二十一点游戏 Blackjack Example

该示例解释了Model-Free下的策略评估问题和结果，没有说具体的学习过程。

游戏规则：你会得到一副手牌，一开始是两张，你需要得到尽量靠近21点但不能超过21点的手牌点数和。越接近21点越有可能打赢庄家，打赢庄家就算赢。庄家会亮一张牌给玩家看

**状态空间**：（多达200种，根据对状态的定义可以有不同的状态空间，这里采用的定义是牌的分数，不包括牌型）

- 当前牌的分数（12 - 21），低于12时，你可以安全的再叫牌，所以没意义。
- 庄家出示的牌（A - 10），庄家会显示一张牌面给玩家
- 我有“useable” ace吗？（是或否）A既可以当1点也可以当11点。

**行为空间**：

- 停止要牌 stick
- 继续要牌 twist

**奖励（停止要牌）**：

- +1：如果你的牌分数大于庄家分数
- 0： 如果两者分数相同
- -1：如果你的牌分数小于庄家分数

**奖励（继续要牌）**：

- -1：如果牌的分数>21，并且进入终止状态
- 0：其它情况

**状态转换（Transitions）**：如果牌分小于12时，自动要牌

**当前策略**：牌分只要小于20就继续要牌。

**问题：**评估该策略的好坏。

**评估过程：**使用庄家显示的牌面值、玩家当前牌面总分值来确定一个二维状态空间，区分手中有无A分别处理。统计每一牌局下决定状态的庄家和玩家牌面的状态数据，同时计算其最终收获。通过模拟多次牌局，计算每一个状态下的平均值，得到如下图示。

**最终结果：**无论玩家手中是否有A牌，该策略在绝大多数情况下各状态价值都较低，只有在玩家拿到21分时状态价值有一个明显的提升。

{% asset_img monte_carlo_example.png This is an example image %}

### 蒙特卡洛累进更新

在使用蒙特卡洛方法求解平均收获时，需要计算平均值。通常计算平均值要预先存储所有的数据，最后使用总和除以此次数。这里介绍了一种更简单实用的方法：

**累进更新平均值 Incremental Mean**

这里提到了在实际操作时常用的一个实时更新均值的办法，使得在计算平均收获时不需要存储所有既往收获，而是每得到一次收获，就计算其平均收获。

理论公式如下：
$$
\begin{aligned} \mu _ { k } & = \frac { 1 } { k } \sum _ { j = 1 } ^ { k } x _ { j } \\ & = \frac { 1 } { k } \left( x _ { k } + \sum _ { j = 1 } ^ { k - 1 } x _ { j } \right) \\ & = \frac { 1 } { k } \left( x _ { k } + ( k - 1 ) \mu _ { k - 1 } \right) \\ & = \mu _ { k - 1 } + \frac { 1 } { k } \left( x _ { k } - \mu _ { k - 1 } \right) \end{aligned}
$$
把这个方法应用于蒙特卡洛策略评估，就得到蒙特卡洛累进更新。

**蒙特卡洛累进更新**

对于一系列Episodes中的每一个：
$$
S _ { 1 } , A _ { 1 } , R _ { 2 } , S _ { 2 } , A _ { 2 } , \ldots , S _ { t } , A _ { t } , R _ { t + 1 } , \ldots , S _ { k }
$$
对于Episode里的每一个状态 $S_t$ ，有一个收获 $G_t$ ，每碰到一次 $S_t$ ，使用下式计算状态的平均价值 $V(S_t)$ ：
$$
V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \frac { 1 } { N \left( S _ { t } \right) } \left( G _ { t } - V \left( S _ { t } \right) \right)
$$
其中：
$$
N ( S_t ) \leftarrow N ( S_t ) + 1
$$
在处理非静态问题时，使用这个方法跟踪一个实时更新的平均值是非常有用的，可以扔掉那些已经计算过的Episode信息。此时可以引入参数 $\alpha$  来更新状态价值：
$$
V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( G _ { t } - V \left( S _ { t } \right) \right)
$$


以上就是蒙特卡洛学习方法的主要思想和描述，由于蒙特卡洛学习方法有许多缺点，因此实际应用并不多。接下来介绍实际常用的TD学习方法。



## 时序差分学习 Temporal-Difference Learning（TD(0)）

### 算法

时序差分算法简称TD学习，是一种无模型的强化学习算法。它继承了动态规划和蒙特卡罗方法的优点，从而对状态值和策略进行预测。从本质上来说，时序差分算法和动态规划一样，是一种**bootstrapping的算法**。同时，也和蒙特卡罗方法一样，从Episode学习，但是它可以学习**不完整**的Episode，是一种**无模型的强化学习算法**，其原理也是基于了试验。

在Monte-Carlo学习中，使用实际的收获（return）$G_t$  来更新价值（Value）：
$$
V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( G _ { t } - V \left( S _ { t } \right) \right)
$$
其中$G _ { t } - V \left( S _ { t } \right)$是实际收获与预估收货的**误差**

在TD学习中，算法在估计某一个状态的价值时，用的是离开该状态的即刻奖励 $R_{t+1}$  与下一状态 $S_{t+1}$  的预估状态价值乘以衰减系数($\gamma$)组成，这符合Bellman方程的描述：
$$
V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right) - V \left( S _ { t } \right) \right)
$$
$R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right)$ 称为**TD目标值**，是对实际收获的估计，即代替了MC中的$G_t$
$\delta _ { t } = R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right) - V \left( S _ { t } \right)$称为TD**误差**

> 回顾：Bellman期望方程（见强化学习基础——强化学习问题描述）
> $$
> v _ { \pi } ( s ) = \mathbb { E } _ { \pi } \left[ R _ { t + 1 } + \gamma v _ { \pi } \left( S _ { t + 1 } \right) | S _ { t } = s \right]
> $$

**BootStrapping** 指的就是TD目标值 $R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right)$  代替收获 $G_t$  的过程



### MC与TD的区别

#### 区别一：是否需要完整Episode

- TD 在知道结果之前可以学习，MC必须等到最后结果才能学习；

- TD 可以在没有结果时学习，可以在持续进行的环境里学习。

**示例——驾车返家**

想象一下你下班后开车回家，需要预估整个行程花费的时间。TD算法相当于在整个返家的过程中（一个Episode），根据已经消耗的时间和预期还需要的时间来不断更新最终回家需要消耗的时间。

{% asset_img td_example1.png 区别一例：TD不需要整个Episode完成才更新 %}

基于上表所示的数据，下图展示了蒙特卡洛学习和TD学习两种不同的学习策略来**更新**价值函数（各个状态的价值）。这里使用的是**从某个状态预估的到家还需耗时**来**间接**反映某状态的价值：某位置预估的到家时间越长，该位置价值越低，在优化决策时需要避免进入该状态。

{% asset_img td_example2.png 区别一例：TD不需要整个Episode完成才更新 %}

对于蒙特卡洛学习过程，驾驶员在路面上碰到各种情况时，他不会更新对于回家的预估时间，等他回到家得到了真实回家耗时后，他会重新估计在返家的路上着每一个主要节点状态到家的时间，在下一次返家的时候用新估计的时间来帮助决策；

对于TD学习，在一开始离开办公室的时候你可能会预估总耗时30分钟，但是当你取到车发现下雨的时候，你会立刻想到原来的预计过于乐观，因为既往的经验告诉你下雨会延长你的返家总时间，此时你会更新目前的状态价值估计，从原来的30分钟提高到40分钟。同样当你驾车离开高速公路时，会一路根据当前的状态（位置、路况等）对应的预估返家剩余时间，直到返回家门得到实际的返家总耗时。这一过程中，你会根据状态的变化实时更新该状态的价值。

#### 区别二：偏差方差

MC $G_t$ ：实际收获，是基于某一策略状态价值的**无偏**估计
$$
G _ { t } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { T - 1 } R _ { T }
$$
TD target：TD目标值，是基于下一状态**预估**价值计算的当前预估收获，是当前状态实际价值的**有偏**估计
$$
R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right)
$$
True TD target： 真实TD目标值，是基于下一状态的**实际**价值对当前状态实际价值的无偏估计
$$
R _ { t + 1 } + \gamma v _ { \pi } \left( S _ { t + 1 } \right)
$$


- MC 没有偏差（bias），但有着较高的方差（Variance），且对初始值不敏感；
- TD 低方差, 但有一定程度的偏差，对初始值较敏感，通常比 MC 更高效；
- 因为MC有T步的环境噪音，而TD只有一步，所以MC的方差高


> **偏差**指的是距离期望的距离，预估的平均值与实际平均值的偏离程度；
>
> **方差**是指评估单次采样结果相对于与平均值变动的范围大小
>
> 基本就是统计学上均值与方差的概念。



**示例——随机行走**

{% asset_img td_example3.png example %}

**状态空间**：如下图：A、B、C、D、E为中间状态，C同时作为起始状态。灰色方格表示终止状态；

**行为空间**：除终止状态外，任一状态可以选择向左、向右两个行为之一；

**即时奖励：**右侧的终止状态得到即时奖励为1，左侧终止状态得到的即时奖励为0，在其他状态间转化得到的即时奖励是0；

**状态转移**：100%按行为进行状态转移，进入终止状态即终止；

**衰减系数：**1；

**给定的策略**：随机选择向左、向右两个行为。

**问题：**评估随机行走这个策略的价值，也就是计算该策略下每个状态的价值，也就是确定该MDP问题的状态价值函数。

**求解：**下图是使用TD算法得到的结果。横坐标显示的是状态，纵坐标是各状态的价值估计，一共5条折线，数字表明的是实际经历的Episode数量，true value所指的那根折线反映的是各状态的实际价值。第0次时，各状态的价值被初始化为0.5，经过1次、10次、100次后得到的价值函数越来越接近实际状态价值函数。

{% asset_img td_example4.png example %}

下图比较了MC和TD算法的效率。横坐标是经历的Episode数量，纵坐标是计算得到的状态函数和实际状态函数下各状态价值的均方差。黑色是MC算法在不同step-size下的学习曲线，灰色的曲线使用TD算法。可以看出TD较MC更高效。此图还可以看出当step-size不是非常小的情况下，TD有可能得不到最终的实际价值，将会在某一区间震荡。

{% asset_img td_example5.png example %} 



#### 区别三：当Episode有限时

当Episode有限时（比如只有三个），两种算法的区别：

- MC算法试图收敛至一个能够最小化状态价值与实际收获的均方差的解决方案，这一均方差用公式表示为：

   $$
   \sum _ { k = 1 } ^ { K } \sum _ { t = 1 } ^ { T _ { k } } \left( G _ { t } ^ { k } - V \left( s _ { t } ^ { k } \right) \right) ^ { 2 }
   $$

   式中， $k$ 表示的是Episode序号， $K$ 为总的Episode数量， $t$ 为一个Episode内状态序号（第1,2,3...个状态等），$T_k$ 表示的是第 $k$ 个Episode总的状态数， $G_t^k$ 表示第 $k$ 个Episode里时 $t$ 刻状态  $S_t$ 获得的最终收获，$V(S_t^k)$ 表示的是第 $k$ 个Episode里算法估计的 $t$ 时刻状态  $S_t$ 的价值。

- TD算法则收敛至一个根据已有经验构建的最大可能的马尔科夫模型的状态价值，也就是说TD算法将首先根据已有经验估计状态间的转移概率：

   $$
   \hat { P } _ { s , s ^ { \prime } } ^ { a } = \frac { 1 } { N ( s , a ) } \sum _ { k = 1 } ^ { K } \sum _ { t = 1 } ^ { T _ { k } } \mathbf { 1 } \left( s _ { t } ^ { k } , a _ { t } ^ { k } , s _ { t + 1 } ^ { k } = s , a , s ^ { \prime } \right)
   $$

   同时估计某一个状态的即时奖励：
   $$
   \hat { \mathcal { R } } _ { s } ^ { a } = \frac { 1 } { N ( s , a ) } \sum _ { k = 1 } ^ { K } \sum _ { t = 1 } ^ { T _ { k } } \mathbf { 1 } \left( s _ { t } ^ { k } , a _ { t } ^ { k } = s , a \right) r _ { t } ^ { k }
   $$
   最后计算该MDP的状态函数。

**示例——AB**

**已知：**现有两个状态(A和B)，MDP未知，衰减系数为1，有如下表所示8个完整Episode的经验及对应的即时奖励，其中除了第1个Episode有状态转移外，其余7个均只有一个状态。

{% asset_img td_example6.png example %} 

**问题：依据仅有的Episode，计算状态A，B的价值分别是多少，即**V(A)=？， V(B)=？

**答案：**V(B) = 6/8，V(A)根据不同算法结果不同，用MC算法结果为0，TD则得出6/8。

**解释：**

应用MC算法：由于需要完整的Episode,因此仅Episode1可以用来计算A的状态价值，很明显是0；同时B的价值是6/8。

应用TD算法：TD算法试图利用现有的Episode经验构建一个MDP（如下图），由于存在一个Episode使得状态A有后继状态B，因此状态A的价值是通过状态B的价值来计算的，同时经验表明A到B的转移概率是100%，且A状态的即时奖励是0，并且没有衰减，因此A的状态价值等于B的状态价值。

{% asset_img td_example7.png example %} 



#### 区别四：Markov环境的表现

- TD算法使用了MDP问题的马尔科夫性，在Markov 环境下更有效
- 但是MC算法并不利用马尔科夫性，通常在非Markov环境下更有效。



### 小结——MC, TD ,DP

|                       | 动态规划                                                     | 蒙特卡洛                                                     | 时序差分                                                     |
| --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 是否需要知道Model     | 是                                                           | 否                                                           | 否                                                           |
| 是否是Bootstrap算法   | 是                                                           | 否                                                           | 是                                                           |
| 是否用样本来计算      | 否（利用模型直接得到V）                                      | 是                                                           | 是                                                           |
| 是否需要完整的Episode | 不基于采样                                                   | 是                                                           | 否                                                           |
| 适用环境              |                                                              | 非Markov环境<br />不适用Episode少的<br />不适用持续环境（无终结状态） | Markov环境<br />                                             |
| backup图              | {% asset_img td2.png 时序差分算法backup图 %}<br />用实际收获更新状态预估价值 | {% asset_img td1.png 蒙特卡洛方法backup图 %}<br />用喜爱状态的预估状态价值预估收获再更新预估价值 | {% asset_img td3.png 动态规划算法backup图 %}<br />根据完整模型，依靠预估数据更新状态价值 |

{% asset_img td4.png 总对比图 %}

上图从两个维度解释了四种算法的差别，多了一个穷举法。这两个维度分别是：采样深度和广度。当使用单个采样，同时不走完整个Episode就是TD；当使用单个采样但走完整个Episode就是MC；当考虑全部样本可能性，但对每一个样本并不走完整个Episode时，就是DP；当既考虑所有Episode又把Episode从开始到终止遍历完，就变成了穷举法。

需要提及的是：DP利用的是整个MDP问题的模型，也就是状态转移概率，虽然它并不实际利用样本，但是它利用了整个模型的规律，因此认为是Full Width的。



## 时序差分学习 Temporal-Difference Learning （TD(λ)）

先前介绍了TD(0)算法，括号内的数字0表示的是在当前状态下往前多看1步，要是往前多看2步更新状态价值会怎样？这就引入了n-step的概念。

### n步预测 n-Step Prediction

所谓的n-step TD，其实就是说要往前多少步再来估计V值。如果往前n步直到终点，那么就等价于蒙特卡罗方法了。

{% asset_img tdn1.png n步预测图 %}

注：图中空心大圆圈表示状态，实心小圆圈表示行为

### n-步收获

TD(0)是基于1-步预测的，MC则是基于$\infty$步预测的：

{% asset_img tdn2.png n步收货 %}

注意：n=2时不写成TD(2)。

**n-步收获**定义为：
$$
G _ { t } ^ { ( n ) } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { n - 1 } R _ { t + n } + \gamma ^ { n } V \left( S _ { t + n } \right)
$$
同样TD target 也由2部分组成，已走的步数使用确定的即时reward，剩下的使用估计的状态价值替代，n步TD学习状态价值函数的更新公式为：

$$
V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( G _ { t } ^ { ( n ) } - V \left( S _ { t } \right) \right)
$$


### λ收获

既然存在n-步预测，那么n=？时效果最好呢，下面的例子试图回答这个问题：

#### 实验——n步预测的n取什么效果最好

这个示例研究了使用多个不同步数预测联合不同步长(step-size，公式里的系数α）时，分别在在线和离线状态时状态函数均方差的差别。所有研究使用了10个Episode。离线与在线的区别在于，离线是在经历所有10个Episode后进行状态价值更新；而在线则至多经历一个Episode就更新依次状态价值。

{% asset_img tdn2.png n步实验 %}

结果如图表明，离线和在线之间曲线的形态差别不明显；从步数上来看，步数越长，越接近MC算法，均方差越大。对于这个大规模随机行走示例，在线计算比较好的步数是3-5步，离线计算比较好的是6-8步。但是不同的问题其对应的比较高效的步数不是一成不变的。因此选择多少步数作为一个较优的计算参数也是一个问题。



#### λ收获定义

这里我们引入了一个新的参数：λ。通过引入这个新的参数，可以做到在不增加计算复杂度的情况下综合考虑所有步数的预测。这就是**λ预测**和**λ收获。**

λ-收获 $G_t^{\lambda}$ 综合考虑了从 $1$ 到 $\infty$ 的所有步收获，它给其中的任意一个$n-1$ 步收获施加一定的权重$(1-\lambda)\lambda^{n-1}$ 。通过这样的权重设计，得到如下的公式：
$$
G _ { \mathrm { t } } ^ { \lambda } = ( 1 - \lambda ) \sum _ { n = 1 } ^ { \infty } \lambda ^ { n - 1 } G _ { t } ^ { ( n ) }
$$
对应的λ-预测写成TD(λ):
$$
v \left( S _ { t } \right) \leftarrow v \left( S _ { t } \right) + \alpha \left( G _ { t } ^ { \lambda } - V \left( S _ { t } \right) \right)
$$
下图是各步收获的权重分配图，图中最后一列λ的指数是$T-t-1$  。$T$ 为终止状态的时刻步数，$t$  为当前状态的时刻步数，所有的权重加起来为1。

{% asset_img tdn3.png n步实验 %}

- **TD(λ)对于权重分配的图解**

{% asset_img tdn4.png n步实验 %}

这张图还是比较好理解，例如对于n=3的3-步收获，赋予其在 λ 收获中的权重如左侧阴影部分面积，对于终止状态的T-步收获，T以后的**所有**阴影部分面积。而所有节段面积之和为1。这种几何级数的设计也考虑了算法实现的计算方便性。

经过这个$(1-\lambda)\lambda^{n-1}$的作用，各个步数的权重就像上图这样衰减。相当于离状态s越远的，权重就越小。这也符合我们一般的想法，离得远的作用就小

### 两个方向理解TD(λ)

TD((λ)的设计使得Episode中，后一个状态的状态价值与之前所有状态的状态价值有关，同时也可以说成是一个状态价值参与决定了后续所有状态的状态价值。但是每个状态的价值对于后续状态价值的影响权重是不同的。我们可以从两个方向来理解TD(λ)：

#### 前向认识TD(λ)

引入了λ之后，会发现要更新一个状态的状态价值，必须要走完整个Episode获得每一个状态的即时奖励以及最终状态获得的即时奖励。这和MC算法的要求一样，因此TD(λ)算法有着和MC方法一样的劣势。λ取值区间为[0,1]，当λ=1时对应的就是MC算法。这给实际计算带来了不便。

{% asset_img tdn_forward.png n步实验 %}

前向视角的解释：假设一个人坐在状态流上拿着望远镜看向前方，前方是那些将来的状态。当估计当前状态的值函数时，从$TD(λ)$的定义中可以看到，它需要用到将来时刻的值函数。
$$
\begin{array} { c } { V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( G _ { t } ^ { \lambda } - V \left( S _ { t } \right) \right) } \\ 
{ G _ { t } ^ { \lambda } = ( 1 - \lambda ) \sum _ { n = 1 } ^ { \infty } \lambda ^ { n - 1 } G _ { t } ^ { ( n ) } } \\ 
G _ { t } ^ { ( n ) } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { n - 1 } R _ { t + n } + \gamma ^ { n } V \left( S _ { t + n } \right)
\end{array}
$$

#### 反向认识TD(λ)（重要）

TD(λ)从另一方面提供了一个单步更新的机制，通过下面的示例来说明。



**示例——被电击的原因**

老鼠在连续接受了3次响铃和1次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？

{% asset_img tdn5.png n步实验 %}

两个概念：

**频率启发 Frequency heuristic：**将原因归因于出现频率最高的状态

**就近启发 Recency heuristic：**将原因归因于较近的几次状态

给每一个状态引入一个数值：**效用追踪**（**Eligibility Traces, ES**），可以结合上述两个启发。定义：
$$
\begin{array} { c } { E _ { 0 } ( s ) = 0 } \\ { E _ { t } ( s ) = \gamma \lambda E _ { t - 1 } ( s ) + 1 \left( S _ { t } = s \right) } \end{array}
$$
其中 $1 \left( S _ { t } = s \right)$ 是一个条件判断表达式。

下图给出了 $E_t(s)$  对于 $t$  的一个可能的曲线图：

{% asset_img tdn6.png n步实验 %}

该图横坐标是时间，横坐标下有竖线的位置代表当前进入了状态 $s$  ，纵坐标是效用追踪值 $E$  。可以看出当某一状态连续出现，E值会在一定衰减的基础上有一个单位数值的提高，此时将增加该状态对于最终收获贡献的比重，因而在更新该状态价值的时候可以较多地考虑最终收获的影响。同时如果该状态距离最终状态较远，则其对最终收获的贡献越小，在更新该状态时也不需要太多的考虑最终收获。

特别的，$E$  值并不需要等到完整的Episode结束才能计算出来，它可以每经过一个时刻就得到更新。$E$  值存在饱和现象，有一个瞬时最高上限：$E_{max}=\frac {1}{1-\gamma\lambda}$ 



{% asset_img tdn_backward.png n步实验 %}

$TD(λ)$的后向视角解释：有个人坐在状态流上，手里拿着话筒，面朝着已经经历过的状态，获得当前回报并利用下一个状态的值函数得到TD偏差之后，此人会想已经经历过的状态喊话告诉这些已经经历过的状态处的值函数需要利用当前时刻的TD偏差进行更新。此时过往的每个状态值函数更新的大小应该跟距离当前状态的步数有关。假设当前状态为$s_t$，TD偏差为$\delta _ { t }$，那么$s_{t−1}$处的值函数更新应该乘以一个衰减因子$\gamma\lambda$，状态$s_{t−2}$处的值函数更新应该乘以$(\gamma\lambda)^2$，以此类推。

后向TD(λ)的更新过程：
$$
\delta _ { t } = R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right) - V \left( S _ { t } \right)
$$
计算当前状态的TD偏差，**效用追踪**（**Eligibility Traces, ES**）：
$$
E _ { t } ( s ) = \gamma \lambda E _ { t - 1 } ( s ) + 1 \left( S _ { t } = s \right)
$$
引入效能主要是为了可以在线学习，可以方便的迭代

更新eligibility trace（对状态空间中的每一个已经经历过的状态s，更新价值函数）
$$
V ( s ) \leftarrow V ( s ) + \alpha \delta _ { t } E _ { t } ( s )
$$
前向误差和反向误差一样，有证明的，不要看着好像不一样就觉得是两个公式



相较于MC算法，TD算法应用更广，是一个非常有用的强化学习方法，在下一讲讲解控制相关的算法时会详细介绍TD算法的实现。

# 策略求解（无模型的控制）

上一节主要讲解了在模型未知的情况下如何进行预测。所谓的预测就是评估一个给定的策略，也就是确定一给定策略下的状态（或状态行为对）的价值函数。这一讲的内容主要是**在模型未知的条件下如何优化价值函数**，这一过程也称作模型无关的控制。

现实中有很多此类的例子，比如控制一个大厦内的多个电梯使得效率最高；控制直升机的特技飞行，机器人足球世界杯上控制机器人球员，围棋游戏等等。所有的这些问题要么我们对其模型运行机制未知；要么是虽然问题模型是已知的，但问题的规模太大以至于计算机无法高效的计算，除非使用采样的办法。本节的内容就专注于解决这些问题。

根据优化控制过程中是否利用已有或他人的经验策略来改进我们自身的控制策略，我们可以将这种优化控制分为两类：

- **现时策略学习（On-policy Learning）**，其基本思想是个体已有一个策略，并且遵循这个策略进行采样，或者说采取一系列该策略下产生的行为，根据这一系列行为得到的奖励，更新状态函数，最后根据该更新的价值函数来优化策略得到较优的策略。

- **离线策略学习（Off-policy Learning）**: 其基本思想是，虽然个体有一个自己的策略，但是个体并不针对这个策略进行采样，而是基于另一个策略进行采样，这另一个策略可以是先前学习到的策略，也可以是人类的策略等一些较为优化成熟的策略，通过观察基于这类策略的行为，或者说通过对这类策略进行采样，得到这类策略下的各种行为，继而得到一些奖励，然后更新价值函数，即在自己的策略形成的价值函数的基础上观察别的策略产生的行为，以此达到学习的目的。这种学习方式类似于“站在别人的肩膀上可以看得更远”。



## 通用策略迭代

{% asset_img 2_1.png 通用策略迭代 %}

**通用策略迭代的核心**是在两个交替的过程之间进行策略优化。一个过程是策略评估，另一个是改善策略。

如上图的三角形区域所示，从一个策略π和一个价值函数Ｖ开始，每一次箭头向上代表着利用当前策略进行价值函数的更新，每一次箭头向下代表着根据更新的价值函数贪婪地选择新的策略，说它是贪婪的，是因为每次都采取转移到可能的、状态函数最高的新状态的行为。最终将收敛至最优策略和最优价值函数。

## 现时策略学习

现时策略学习的特点就是当前遵循的策略就是个体学习改善的策略。

根据是否经历完整的Episode可以将其分为基于蒙特卡洛的和基于TD的。

## 现时策略蒙特卡洛控制

根据通用策略迭代的步骤是评估策略和改善策略交替进行，下面对两个步骤进行讨论

### 评估策略

动态规划算法来改善策略是需要知道某一状态的所有后续状态及状态间转移概率：
$$
\pi ^ { \prime } ( s ) = \underset { a \in \mathcal { A } } { \operatorname { argmax } } \mathcal { R } _ { s } ^ { a } + \mathcal { P } _ { s s ^ { \prime } } ^ { a } V \left( s ^ { \prime } \right)
$$
但这种方法不适用于模型未知的蒙特卡洛学习，因为不知道执行完某个动作后的转移概率和所有后续的状态,就不知道哪个行动会使价值最大。



可以使用状态-行为对下的价值$Q(s,a)$来代替状态价值 ，这样就可以改善策略而不需要知道整个模型：
$$
\pi ^ { \prime } ( s ) = \underset { a \in \mathcal { A } } { \operatorname { argmax } } Q ( s , a )
$$

> Q函数复习
> $$
> q _ { \pi } ( s , a ) = \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s , A _ { t } = a \right]
> $$

这样做的目的是可以改善策略而不用知道整个模型，只需要知道在某个状态下采取什么什么样的行为价值最大即可。具体是这样：我们从一个初始的 $Q$ 和策略 $\pi$ 开始，先根据这个策略更新每一个状态行为对的 $q$ 值，$s$ 随后基于更新的 $Q$确定改善的贪婪算法。

### 改善策略

即使这样，至少还存在一个问题，即当我们每次都使用贪婪算法来改善策略的时候，将很有可能由于没有足够的采样经验而导致产生一个并不是最优的策略，我们需要不时的尝试一些新的行为，这就是探索（Exploration）

完全使用贪婪算法改善策略可能会陷入局部最优，通常不能得到最优策略。为了解决这一问题，我们需要引入一个随机机制，以一定的概率选择当前最好的策略，同时给以其它可能的行为一定的几率，这就是Ɛ-贪婪探索。

#### Ɛ-贪婪探索

Ɛ-贪婪探索的目标使得某一状态下所有可能的行为都有一定非零几率被选中执行，也就保证了持续的探索，$1 - \epsilon$ 的概率下选择当前认为最好的行为，而 $\epsilon$ 的概率在所有可能的行为中选择（也包括那个当前最好的行为）。数学表达式如下：
$$
\pi ( a | s ) = \left\{ \begin{array} { l l } { \epsilon / m + 1 - \epsilon } & { \text { if } a ^ { * } = \underset { a \in \mathcal { A } } { \operatorname { argmax } } Q ( s , a ) } \\ { \epsilon / m } & { \text { otherwise } } \end{array} \right.
$$


**定理：**使用Ɛ-贪婪探索策略，对于任意一个给定的策略 $\pi$，我们在评估这个策略的同时也总在改善它（$V_{\pi'}(s) \geq V_{\pi}(s)$）

证明见David Silver视频



### 蒙特卡洛控制=评估策略+改善策略

解决了上述两个问题，我们最终看到蒙特卡洛控制的全貌：使用Ｑ函数进行策略评估，使用Ɛ-贪婪探索来改善策略。该方法最终可以收敛至最优策略。如下图所示：

{% asset_img 2_2.png 通用策略迭代 %}

图中每一个向上或向下的箭头都对应着多个Episode。也就是说我们一般在经历了多个Episode之后才进行依次Ｑ函数更新或策略改善。实际上我们也可以在每经历一个Episode之后就更新Ｑ函数或改善策略。但不管使用哪种方式，在Ɛ-贪婪探索算法下我们始终只能得到基于某一策略下的近似Ｑ函数，且该算法没没有一个终止条件，因为它一直在进行探索。

因此我们必须关注以下两个方面：一方面我们不想丢掉任何更好信息和状态，另一方面随着我们策略的改善我们最终希望能终止于某一个最优策略，因为事实上最优策略不应该包括一些随机行为选择。为此引入了另一个理论概念：**GLIE**。



### 基于GLIE的蒙特卡洛控制

#### GLIE(Greedy in the Limit with Infinite Exploration)

**GLIE**(Greedy in the Limit with Infinite Exploration)，直白的说是**在有限的时间内进行无限可能的探索**。具体表现为：

- 所有已经经历的状态行为对（state-action pair）会被无限次探索

- 策略会收敛于贪婪策略。即随着探索的无限延伸，贪婪算法中Ɛ值趋向于０。例如如果我们取 $\epsilon=\frac {1}{k}$ （$k$ 为探索的Episode数目），那么该Ɛ贪婪蒙特卡洛控制就具备GLIE特性。

#### 算法

1. 对于给定策略 $\pi$ ，采样第 $k$ 个Episode：$S _ { 1 } , A _ { 1 } , R _ { 2 } , \ldots , S _ { T } \sim \pi$ 

2. 对于该Episode里出现的每一个状态行为对 $S_t$ 和 $A_t$ 更新其计数和Ｑ函数：
   $$
   \begin{array} { l } { N \left( S _ { t } , A _ { t } \right) \leftarrow N \left( S _ { t } , A _ { t } \right) + 1 } \\ { Q \left( S _ { t } , A _ { t } \right) \leftarrow + \frac { 1 } { N \left( S _ { t } , A _ { t } \right) } \left( G _ { t } - Q \left( S _ { t } , A _ { t } \right) \right) } \end{array}
   $$

3. 基于新的Ｑ函数改善以如下方式改善策略：
   $$
   \epsilon=\frac {1}{k} \\
   \pi \leftarrow greedy(Q)
   $$

**定理：GLIE蒙特卡洛控制能收敛至最优的状态行为价值函数。**



## 现时策略时序差分控制 On-Policy Temporal-Difference Control

上一讲提到TD相比MC有很多优点：低方差，可以在线实时学习，可以学习不完整Episode等。因此很自然想到是否可以在控制问题上使用TD学习$Q(S,A)$而不是MC学习？这就是下文要讲解的SARSA

### SARSA

SARSA的名称来源于下图所示的序列描述：针对一个状态$S$，以及一个特定的行为，$A$进而产生一个状态行为对($SA$)，与环境交互，环境收到个体的行为后会告诉个体即时奖励 $R$ 以及后续进入的状态 $S'$；接下来个体遵循**现有策略**产生一个行为 $A'$，根据当前的**状态行为价值函数**得到后一个状态行为对($S'A'$)的价值（$Q$），利用这个 $Q$ 值更新前一个状态行为对( $SA$ )的价值。

{% asset_img 2_3.png SARSA %}

与蒙特卡洛控制不同的时，每一个时间步，也就是在单个Episode内每一次个体在状态 $S_t$ 采取一个行为后都要更新 $Q$ 值，同样使用**Ɛ-贪婪探索**的形式来改善策略。
$$
Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma Q \left( S ^ { \prime } , A ^ { \prime } \right) - Q ( S , A ) \right)
$$

#### 现时策略控制的SARSA算法

{% asset_img 2_4.png SARSA %}

注：

1. 算法中的 $Q(s,a)$是以一张大表存储的，这不适用于解决规模很大的问题；
2. 对于每一个Episode，在 $S$ 状态时采用的行为是 $A$ 基于当前策略的，同时该行为也是实际Episode发生的行为，在更新 $SA$ 行为状态价值循环里，个体并不实际执行在 $S'$ 下的 $A'$ 行为，而是将行为 $A'$ 留到下一个循环执行。

#### 示例——有风格子世界

**已知：**如图所示，环境是一个10*7的长方形格子世界，同时有一个起始位置S和一个终止目标位置G，水平下方的数字表示对应的列中有一定强度的风，当该数字是1时，个体进入该列的某个格子时，会按图中箭头所示的方向自动移动一格，当数字为2时，表示顺风移动2格，以此类推模拟风的作用。对于个体来说，它不清楚整个格子世界的构造，即它不知道格子是长方形的，也不知道边界在哪里。也不清楚起始位置、终止目标位置的具体为止。对于它来说，每一个格子就相当于一个封闭的房间，在没推开门离开当前房间之前它无法知道会进入哪个房间。个体具备记住曾经去过的格子的能力。格子可以执行的行为是朝上、下、左、右移动一步。

{% asset_img 2_5.png SARSA %}

**问题：**个体如何才能找到最短从起始格子S到终止目标格子G的最短路线？

**解答：**可以设置个体每行走一步获得即时奖励为-1，直到到达终止目标位置的即时奖励为0，借此希望找到最优策略。衰减系数λ可设为1。

其最优路线如下图所示：

{% asset_img 2_6.png SARSA %}

在个体找到这个最优行为序列的早期，由于个体对环境一无所知，SARSA算法需要尝试许多不同的行为，因此在一开始的2000多步里，个体只能完成少数几个完整的Episode，但随着个体找到一条链接起点到终点的路径，其快速优化策略的能力就显现的很明显了，因为它不需要走完一个Episode才能更新行为价值，而是每走一步就根据下一个状态能够得到的最好价值来更新当前状态的价值。

### SARSA(λ)

#### n-步SARSA

在之前，我们学习了n-步收获（见TD(λ)），这里类似的引出一个n-步Sarsa的概念。观察下面一些列的式子：

{% asset_img 2_7.png SARSA %}



定义**n-步Q收获（Q-return）：**
$$
q _ { t } ^ { ( n ) } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { n - 1 } R _ { t + n } + \gamma ^ { n } Q \left( S _ { t + n } \right)
$$
则可以把n-步Sarsa用n-步Q收获来表示，如下式：
$$
Q \left( S _ { t } , A _ { t } \right) \leftarrow Q \left( S _ { t } , A _ { t } \right) + \alpha \left( q _ { t } ^ { ( n ) } - Q \left( S _ { t } , A _ { t } \right) \right)
$$
假如我们给n-步Q收获的每一个收获分配一个权重，如下引入参数λ分配权重，并按权重对每一步Q收获求和，那么将得到  $q^{\lambda}$ 收获，它结合了所有n-步Q收获：
$$
q _ { t } ^ { \lambda } = ( 1 - \lambda ) \sum _ { n = 1 } ^ { \infty } \lambda ^ { n - 1 } q _ { t } ^ { ( n ) }
$$

#### 两个方向理解SARSA(λ)

##### Sarsa(λ)前向认识

如果用某一状态的 $q^{\lambda}$ 收获来更新状态行为对的Q值，那么可以表示称如下的形式：
$$
Q \left( S _ { t } , A _ { t } \right) \leftarrow Q \left( S _ { t } , A _ { t } \right) + \alpha \left( q _ { t } ^ { \lambda } - Q \left( S _ { t } , A _ { t } \right) \right)
$$
这就是前向认识Sarsa(λ)，使用它更新Q价值需要遍历完整的Episode，我们同样可以反向理解Sarsa(λ).



##### Sarsa(λ)反向认识

与上一节对于TD(λ)的反向认识一样，引入效用追踪（Eligibility Trace）概念，不同的是这次的E值针对的不是一个状态，而是一个状态行为对：
$$
\begin{array} { c } { E _ { 0 } ( s , a ) = 0 } \\ { E _ { t } ( s , a ) = \gamma \lambda E _ { t - 1 } ( s , a ) + 1 \left( S _ { t } = s , A _ { t } = a \right) } \end{array}
$$
它体现的是一个结果与某一个状态行为对的因果关系，与得到结果最近的状态行为对，以及那些在此之前频繁发生的状态行为对对得到这个结果的影响最大。

下式是引入ET概念的SARSA(λ)之后的Q值更新描述：
$$
\begin{aligned} 
E _ { t } ( s , a ) &= \gamma \lambda E _ { t - 1 } ( s , a ) + 1 \left( S _ { t } = s , A _ { t } = a \right)\\
\delta _ { t } = & R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , A _ { t + 1 } \right) - Q \left( S _ { t } , A _ { t } \right) \\ 
& Q ( s , a ) \leftarrow Q ( s , a ) + \alpha \delta _ { t } E _ { t } ( s , a ) 
\end{aligned}
$$
引入ET概念，同时使用SARSA(λ)将可以更有效的在线学习，因为不必要学习完整的Episode，数据用完即可丢弃。ET通常也是更多应用在在线学习算法中(online algorithm)。

#### 具体的SARSA(λ)算法

{% asset_img 2_8.png SARSA %}

这里要提及一下的是$E(s,a)$在每浏览完一个Episode后需要重新置0，这体现了ET仅在一个Episode中发挥作用；其次要提及的是算法更新Q和E的时候针对的不是某个Episode里的Q或E，而是针对个体掌握的整个状态空间和行为空间产生的Q和E。

实际如果是基于查表的方式实现该算法，其速度明显比Sarsa要慢。毕竟带E的算法主要应用于在线更新。

### 例——SARSA与SARSA(λ)的区别

假定图描述的路线是个体采取两种算法中的一个得到的一个完整Episode的路径。为了下文更方便描述、解释两个算法之间的区别，先做几个合理的小约定：

1. 认定每一步的即时奖励为0，直到终点处即时奖励为1；

2. 根据算法，除了终点以外的任何状态行为对的Q值可以是任意的，但我们设定所有的Q值均为0；

3. 该路线是第一次找到终点的路线。

   {% asset_img 2_9.png SARSA %}

|        | Sarsa(0)                                                     | Sarsa(λ)                                                     |
| ------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 第一次 | Q表中的值均为0<br />依据当前策略 $A$ 右移动一步，到达$S'$位置<br />依据当前策略产生一个行为$A'$，在Q表中查找$Q(S',A')$得到$Q'$<br /><br />依据更新公式更新表$Q(S,A)$值（在每到终点前都是0）<br />个体到达终点，获得奖励1，（从$S_H$向上走$A_{up}$到达的终点）<br />更新Q表中的$Q(S_H,A_{up})$<br />完成一个Episode（只有一次有意义的行为价值函数更新） | E表中的值均为0<br />依据当前策略 $A$ 右移动一步，到达$S'$位置<br />E(S,A)加1（表明经历了这个事件）<br />计算TD误差，估计这个事件对整个问题的价值（结果是0，说明这个举动没有说明积极帮助）<br />更新Q表中的$Q(S,A)$值<br />个体到达终点，获得奖励1，，（从$S_H$向上走$A_{up}$到达的终点）<br />计算TD误差为1,<br />更新**整个过程经历过**的$Q(s,a)$，其中与$<S_H,A_{up}>$越近发生的状态（体现在E值），价值提升越明显（个体在这个Episode中经历的所有状态行为对的Q值都将得到一个非0的更新） |
| 第二次 | 一开始与首次一样都是盲目随机的<br />直到其进入终点位置下方的位置 $S_H$，在这个位置，个体更新的策略要求其选择向上的行为直接进入终点位置$S_G$ | 起点处向右走的价值不再是0<br />过程同上                      |
| 第n次  | 如果采用greedy策略更新，个体最终将得到一条到达终点的路径，不过这条路径的倒数第二步永远是在终点位置的下方。<br />如果采用Ɛ-greedy策略更新，那么个体还会尝试到终点位置的左上右等其它方向的相邻位置价值也比较大，此时个体每次完成的路径可能都不一样。 | 如果采用greedy策略更新，个体将根据上次经验得到的新策略直接选择右走，并且一直按照原路找到终点。<br />如果采用Ɛ-greedy策略更新，那么个体还会尝试新的路线。 |



## **离线策略学习 Off-Policy Learning**

离线策略学习（Off-Policy Learning）则指的是在遵循一个策略 $\mu(a|s)$ 的同时评估另一个策略 $\pi(a|s)$，也就是计算确定这另一个策略下的状态价值函数 $v_{\pi}(s)$ 或状态行为价值函数 $q_{\pi}(s,a)$。

这样可以较容易的从人类经验或其他个体的经验中学习，也可以从一些旧的策略中学习，可以比较两个策略的优劣。可以遵循一个探索式策略的基础上优化现有的策略。

根据是否经历完整的Episode可以将其分为基于蒙特卡洛的和基于TD的。

## 离线策略蒙特卡洛控制

基于蒙特卡洛的离线策略学习仅有理论上的研究价值，在实际中毫无用处。

## 离线策略时序差分控制 Off-Policy Temporal-Difference Control

离线策略TD学习的任务就是使用TD方法在遵循一个策略 $\mu(a|s)$ 的同时评估另一个策略 $\pi(a|s)$ 。具体数学表示为：
$$
V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( \frac { \pi \left( A _ { t } | S _ { t } \right) } { \mu \left( A _ { t } | S _ { t } \right) } \left( R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right) \right) - V \left( S _ { t } \right) \right) 
$$
这个公式可以这样解释：

个体处在状态 $S_t$中，基于策略 $\mu$ 产生了一个行为 $A_t$ ，执行该行为后进入新的状态 $S_{t+1}$ ，那么在当前策略下如何根据新状态的价值调整原来状态的价值呢？离线策略的方法就是，在状态时 $S_t$ 比较分别依据另一个策略 $\pi$和当前遵循的策略 $\mu$ 产生行为 $A_t$ 的概率大小。

- 如果策略 $\pi$ 得到的概率值与遵循当前策略 $\mu$ 得到的概率值接近，说明根据状态 $S_{t+1}$ 价值来更新 $S_t$ 的价值同时得到两个策略的支持，这一更新操作比较有说服力。同时也说明在 $S_t$ 状态时，两个策略有接近的概率选择行为 $A_t$ 。

- 如果这一概率比值很小，则表明如果依照被评估的策略，选择 $A_t$ 的机会很小，这时候我们在更新  $S_t$ 价值的时候就不能过多的考虑基于当前策略得到的状态 $S_{t+1}$ 的价值。同样概率比值大于1时的道理也类似。这就相当于借鉴被评估策略的经验来更新我们自己的策略。



### Q学习

应用上述思想最好的方法是基于TD(0)的Q-学习（Q-learning）。它的要点在于，更新一个状态-行为的Q价值时，遵循策略 $\pi $ 得到的下一个状态行为对的Q价值。公式如下：
$$
Q \left( S _ { t } , A _ { t } \right) \leftarrow Q \left( S _ { t } , A _ { t } \right) + \alpha \left( R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , A ^ { \prime } \right) - Q \left( S _ { t } , A _ { t } \right) \right)
$$
式中，$R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , A ^ { \prime } \right)$ TD目标是基于自己的策略(待评估) $\pi$ 产生的行为 $A'$ 得到的$Q$价值。

**Q学习最主要的表现形式是**：个体遵循的策略 $\mu$ 是基于当前状态行为价值函数 $Q(s,a)$ 的一个Ɛ-贪婪策略，而目标策略 $\pi$ 是基于当前状态行为价值函数 $Q(s,a)$ 不包含Ɛ的单纯贪婪策略：
$$
\pi \left( S _ { t + 1 } \right) = \underset { a ^ { \prime } } { \operatorname { argmax } } Q \left( S _ { t + 1 } , a ^ { \prime } \right)
$$
这样Q学习的TD目标值可以被大幅简化：
$$
\begin{aligned} & R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , A ^ { \prime } \right) \\ = & R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , \underset { a ^ { \prime } } { \operatorname { argmax } } Q \left( S _ { t + 1 } , a ^ { \prime } \right) \right) \\ = & R _ { t + 1 } + \max _ { x ^ { \prime } } \gamma Q \left( S _ { t + 1 } , a ^ { \prime } \right) \end{aligned}
$$
**定理：**Q学习控制将收敛至最优状态行为价值函数：$Q(s,a) \rightarrow q_*(s,a)$



下图是Q学习具体的更新公式和图解：

{% asset_img 2_10.png SARSA %}

$$
Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma \max _ { a ^ { \prime } } Q \left( S ^ { \prime } , a ^ { \prime } \right) - Q ( S , A ) \right)
$$
下图是Q学习的算法流程：

{% asset_img 2_11.png SARSA %}



## **总结DP与TD关系**

下面两张图概括了各种DP算法和各种TD算法，同时也揭示了各种不同算法之间的区别和联系。总的来说TD是采样+有数据引导(bootstrap)，DP是全宽度+实际数据。如果从Bellman期望方程角度看：聚焦于状态本身价值的是迭代法策略评估（DP）和TD学习，聚焦于状态行为对价值函数的则是Q-策略迭代（DP）和SARSA；如果从针对状态行为价值函数的Bellman优化方程角度看，则是Q-价值迭代（DP）和Q学习。

{% asset_img 2_12.png SARSA %}

{% asset_img 2_13.png SARSA %}


