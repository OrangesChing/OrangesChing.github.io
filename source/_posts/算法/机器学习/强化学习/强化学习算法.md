---
title: 强化学习算法
date: 2019-12-06 10:57:01
tags:
  - 强化学习
categories:
  - 算法
  - 机器学习
  - 强化学习
---

# Q学习（离线学习）

Q学习是一种基于TD(0)的离线策略时序差分算法。

## 算法

### 算法理论——Q值更新公式

Q学习使用一张Q表来存储当前状态-行为价值，即存储在当前状态做这个行为未来可获得的收益。为了更新这张表，我们需要计算这个预估的未来奖励（Q值）和我们实际获得的奖励有多大的误差。

- 预估的未来奖励，就是Q表中的数值

- 实际获得的奖励

  直观上想，可以使用从这一步出发到终点的奖励来计算。但是这样需要整个奖励序列，不利于学习（这个思想是基于蒙托卡罗的思想）。

  换一种思路，我们只利用当前步的奖励，再加上**下一步的预估未来奖励（下一步的Q值）**，不就是有点实际的实际获得奖励吗

用公式表示这个误差就是
$$
R _ { t + 1 } + \gamma Q \left( S'  , A ^ { \prime } \right) - Q \left( S _ { t } , A _ { t } \right)
$$
由于下一步的预估未来奖励与具体的行动有关系，而我们要获取的是一个具体的值，所以我们在下一步选择行动的时候才用纯贪婪算法，即在下一状态$S_{t+1}$时选择动作的策略变成了贪婪Q最大：
$$
\pi \left( S _ { t + 1 } \right) = \underset { a ^ { \prime } } { \operatorname { argmax } } Q \left( S ' , a ^ { \prime } \right)
$$

这样$Q \left( S _ { t + 1 } , A ^ { \prime } \right)$就可以变成具体的$\underset { a ^ { \prime } } { \operatorname { max } } Q \left( S _ { t + 1 } , a ^ { \prime } \right)$，Q值更新公式就变成了
$$
Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma \max _ { a ^ { \prime } } Q \left( S ^ { \prime } , a ^ { \prime } \right) - Q ( S , A ) \right)
$$


### 算法伪代码

{% asset_img Q学习算法.png Q学习算法 %}

Q学习中动作的选择策略是这样的：当前在状态$S$选择行动使用的是Ɛ-贪婪策略，获得了奖励R到达了状态 $s'$ 。现在要评估误差时，需要使用下一步的状态-行为值，这时对$s'$的策略决策使用的是纯贪婪算法，才能获得具体的下一步状态-行为值$\max _ { a ^ { \prime } } Q \left( S ^ { \prime } , a ^ { \prime } \right) $

$\alpha$是学习率, 来决定这次的误差有多少是要被学习的, $\alpha$是一个小于1 的数. 

$\gamma$ 是 reward 的衰减衰减系数



## 例

举给女朋友送礼物的例子，你女朋友的状态有三种（高兴/没反应/不高兴）下面是获得的奖励（奖励只与状态有关，离开这个状态就能获得）

| 状态   | 奖励（R） |
| ------ | --------- |
| 高兴   | 1         |
| 没反应 | 0         |
| 不高兴 | 0         |

为了记录我在这个情况下做某件事可能获得多少的好感度，这样方便我以后决策，我就可以画一个Q表，假设你当前的Q表是这样的

| 当前状态（S） | 送礼物 | 不送礼物 | 揍一顿 |
| ------------- | ------ | -------- | ------ |
| 高兴          | 5      | 1        | 0      |
| 没反应        | 4      | 1        | 0      |
| 不高兴        | 3      | 1        | 0      |

现在，你女朋友来姨妈不高兴了，你想做点什么。看看Q表。诶！送礼物的价值最大，送个礼物。然后你送了他一个热水瓶。女朋友看完就放那了（女朋友的状态取决于女朋友，你不知道他会变成什么状态）。进过这次事之后，你想更新下你的Q表。怎么更新？看看公式
$$
Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma \max _ { a ^ { \prime } } Q \left( S ^ { \prime } , a ^ { \prime } \right) - Q ( S , A ) \right)
$$
首先我理想中会获得的好感度$Q(S,A)$是3；现实是离开【不高兴】这个状态，获得了奖励0，到达了新的状态【没反应】，我预估下我在【没反应】能做的最有意义的事还是送礼，没反应-送礼这个操作Q值是4，即$\max _ { a ^ { \prime } } Q \left( S ^ { \prime } , a ^ { \prime } \right) = 4$。假设$\gamma,\alpha=1$，那现实和理想的差距就是$（0+1*4-3）$，更新$Q（不高兴，送礼物）= 3+1*(0+1*4-3)=4$



## 代码

{% asset_img Q学习算法例.png Q学习算法 %}

玩上述游戏，黄色圆为宝藏，黑色方框为黑洞，进入黑洞奖励为-1，获得宝藏奖励+1，其他情况奖励为0。

迭代代码

```python
def update():
    # 学习 100 回合
    for episode in range(100):
        # 初始化 state 的观测值
        observation = env.reset()

        while True:
            # 更新可视化环境
            env.render()

            # RL 大脑根据 state 的观测值挑选 action
            action = RL.choose_action(str(observation))

            # 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)
            observation_, reward, done = env.step(action)

            # RL 从这个序列 (state, action, reward, state_) 中学习
            RL.learn(str(observation), action, reward, str(observation_))

            # 将下一个 state 的值传到下一次循环
            observation = observation_

            # 如果掉下地狱或者升上天堂, 这回合就结束了
            if done:
                break
   
    # 结束游戏并关闭窗口
    print('game over')
    env.destroy()

if __name__ == "__main__":
    # 定义环境 env 和 RL 方式
    env = Maze()
    RL = QLearningTable(actions=list(range(env.n_actions)))

    # 开始可视化环境 env
    env.after(100, update)
    env.mainloop()
```

Q学习代码（个体类）

```python
class QLearningTable:
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        self.actions = actions  # a list
        self.lr = learning_rate #学习速率
        self.gamma = reward_decay #衰减系数
        self.epsilon = e_greedy #以多少概率去探索
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)  #Q表，Q学习的关键！

    #依据策略（ε-贪婪策略）选择一个动作
    def choose_action(self, observation):
        self.check_state_exist(observation)#如果状态没遇到过就加入Q表
        # action selection
        if np.random.uniform() < self.epsilon: 
            # choose best action
            state_action = self.q_table.loc[observation, :]
            # 不直接使用max函数，防止相同值时每次都选择同一个动作
            action = np.random.choice(state_action[state_action == np.max(state_action)].index)
        else:
            # choose random action
            action = np.random.choice(self.actions)
        return action

    #更新Q表中的值
    def learn(self, s, a, r, s_):
        self.check_state_exist(s_)
        q_predict = self.q_table.loc[s, a]  #估计值
        
        #计算现实值
        if s_ != 'terminal':
            q_target = r + self.gamma * self.q_table.loc[s_, :].max()  # next state is not terminal
        else:
            q_target = r  # next state is terminal
        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # update

    #检查是否有此状态，没有就添加
    def check_state_exist(self, state):
        if state not in self.q_table.index:
            # append new state to q table
            self.q_table = self.q_table.append(
                pd.Series(
                    [0]*len(self.actions),
                    index=self.q_table.columns,
                    name=state,
                )
            )
```



# Sarsa

## 算法

### 算法理论——Q值更新公式

SARSA的名称来源于一个的序列描述：针对一个状态$S$，以及一个特定的行为，$A$进而产生一个状态行为对($SA$)，与环境交互，环境收到个体的行为后会告诉个体即时奖励 $R$ 以及后续进入的状态 $S'$；接下来个体遵循**现有策略**产生一个行为 $A'$（下一次行动），根据当前的**状态行为价值函数**得到后一个状态行为对($S'A'$)的价值（$Q$），利用这个 $Q$ 值更新前一个状态行为对( $SA$ )的价值。Sarsa使用的是单步更新

Sarsa与Q学习类似同样使用**Ɛ-贪婪探索**的形式来改善策略，但Q表的更新公式不同。Q学习在$S’$状态的决策策略是纯贪婪策略，且决策出的动作不一定是真实执行的动作所以Q学习中Q现实的计算为$R+\gamma\underset { a ^ { \prime } } { \operatorname { max } } Q \left( S _ { t + 1 } , a ^ { \prime } \right)$；而Sarsa在$S$状态和$S'$状态的决策策略都是Ɛ-贪婪策略，且在$S'$决策出来的动作就是下一个执行的动作，所以Sarsa中Q现实的计算为$R + \gamma Q \left( S ^ { \prime } , A ^ { \prime } \right)$，有这个特性Sarsa也是一个On-policy学习算法
$$
Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma Q \left( S ^ { \prime } , A ^ { \prime } \right) - Q ( S , A ) \right)
$$

这个公式决定了Q学习比Sarsa更加大胆的特性



### 算法伪代码

{% asset_img Sarsa算法.png Sarsa算法 %}

$\alpha$是学习率, 来决定这次的误差有多少是要被学习的, $\alpha$是一个小于1 的数. 

$\gamma$ 是 reward 的衰减衰减系数

## 例

还是上面的例子，重新写一遍

举给女朋友送礼物的例子，你女朋友的状态有三种（高兴/没反应/不高兴）下面是获得的奖励（奖励只与状态有关，离开这个状态就能获得）

| 状态   | 奖励（R） |
| ------ | --------- |
| 高兴   | 1         |
| 没反应 | 0         |
| 不高兴 | 0         |

为了记录我在这个情况下做某件事可能获得多少的好感度，这样方便我以后决策，我就可以画一个Q表，假设你当前的Q表是这样的

| 当前状态（S） | 送礼物 | 不送礼物 | 揍一顿 |
| ------------- | ------ | -------- | ------ |
| 高兴          | 5      | 1        | 0      |
| 没反应        | 4      | 1        | 0      |
| 不高兴        | 3      | 1        | 0      |

现在，你女朋友来姨妈不高兴了，你想做点什么。看看Q表。诶！送礼物的价值最大，送个礼物。然后你送了他一个热水瓶。女朋友看完就放那了（女朋友的状态取决于女朋友，你不知道他会变成什么状态）。进过这次事之后，你想更新下你的Q表。怎么更新？看看公式
$$
Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma Q \left( S ^ { \prime } , A ^ { \prime } \right) - Q ( S , A ) \right)
$$
首先我理想中会获得的好感度$Q(S,A)$是3；现实是离开【不高兴】这个状态，获得了奖励0，到达了新的状态【没反应】，我在【没反应】能做的事可以有两种：一是随便做件事看看反应，二是选Q值最大的动作（送礼）。我最后决定随便做件事（揍她一顿）没反应-揍一顿这个操作Q值是0，即$Q \left( S ^ { \prime } , A ^ { \prime } \right)=0$。假设$\gamma,\alpha=1$，那现实和理想的差距就是$（0+1*0-3）$，更新$Q（不高兴，送礼）= 3+1*(0+1*0-3)=0$，下一步我要做的事是揍一顿

## 代码

{% asset_img Q学习算法例.png Q学习算法 %}

玩上述游戏，黄色圆为宝藏，黑色方框为黑洞，进入黑洞奖励为-1，获得宝藏奖励+1，其他情况奖励为0。

迭代代码

```python
def update():
    # 学习 100 回合
    for episode in range(100):
        # 初始化 state 的观测值
        observation = env.reset()

        while True:
            # 更新可视化环境
            env.render()

            # RL 大脑根据 state 的观测值挑选 action
            action = RL.choose_action(str(observation))

            # 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)
            observation_, reward, done = env.step(action)
            
            # 决策下一个行动，与Q学习不同
            action_ = RL.choose_action(str(observation_))

            # RL 从这个序列 (state, action, reward, state_) 中学习
            RL.learn(str(observation), action, reward, str(observation_), action_)

            # 将下一个 state 的值传到下一次循环
            observation = observation_
            action = action_ #与Q学习不同，这个动作真实执行了

            # 如果掉下地狱或者升上天堂, 这回合就结束了
            if done:
                break
   
    # 结束游戏并关闭窗口
    print('game over')
    env.destroy()

if __name__ == "__main__":
    # 定义环境 env 和 RL 方式
    env = Maze()
    RL = QLearningTable(actions=list(range(env.n_actions)))

    # 开始可视化环境 env
    env.after(100, update)
    env.mainloop()
```

Sarsa学习代码（个体类）

```python
class SarsaTable:
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        self.actions = actions  # a list
        self.lr = learning_rate #学习速率
        self.gamma = reward_decay #衰减系数
        self.epsilon = e_greedy #以多少概率去探索
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)  #Q表，学习的关键！

    #依据策略（ε-贪婪策略）选择一个动作
    def choose_action(self, observation):
        self.check_state_exist(observation)#如果状态没遇到过就加入Q表
        # action selection
        if np.random.uniform() < self.epsilon: 
            # choose best action
            state_action = self.q_table.loc[observation, :]
            # 不直接使用max函数，防止相同值时每次都选择同一个动作
            action = np.random.choice(state_action[state_action == np.max(state_action)].index)
        else:
            # choose random action
            action = np.random.choice(self.actions)
        return action

    #更新Q表中的值
    def learn(self, s, a, r, s_， a_):
        self.check_state_exist(s_)
        q_predict = self.q_table.ix[s, a]  #估计值
        
        #计算现实值
        if s_ != 'terminal':
            q_target = r + self.gamma * self.q_table.ix[s_, a_] # next state is not terminal
        else:
            q_target = r  # next state is terminal
        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # update

    #检查是否有此状态，没有就添加
    def check_state_exist(self, state):
        if state not in self.q_table.index:
            # append new state to q table
            self.q_table = self.q_table.append(
                pd.Series(
                    [0]*len(self.actions),
                    index=self.q_table.columns,
                    name=state,
                )
            )
```

# Sarsa(λ)

## 算法

### 算法理论

Q-Learning 和 Sarsa 都是在得到奖励后只更新上一步状态和动作对应的 Q 表值，是单步更新算法，也就是 Sarsa(0)。但是在得到当前奖励值后之前所走的每一步（即一个轨迹）都多多少少和最终得到的奖励值有关，所以不应该只更新上一步状态对应的 Q 值。于是就有了多步更新算法——Sarsa(n)。当 n 的值为一个回合（episode）的步数时就变成了回合更新。对于多步更新的 Sarsa 算法我们用 Sarsa(λ)来统一表示，其中 $\lambda$ 的取值范围是 [ 0 , 1 ]，其本质是一个衰减值。


#### n步sarsa

之前的sarsa只使用一步的真实奖励来估计现实Q收货，如果我多用几步呢？

定义**n-步Q收获（Q-return）**为前n个奖励的总和加上在n个步骤中达到的状态的估计值，每个步骤都适当地折扣：
$$
G _ { t:t+n }= R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { n - 1 } R _ { t + n } + \gamma ^ { n } Q_{t+n-1} \left( S _ { t + n } , A_{t+n}\right)
$$
则可以把Sarsa用n-步Q收获来表示得到n步Sarsa更新公式，如下式：
$$
Q \left( S _ { t } , A _ { t } \right) \leftarrow Q \left( S _ { t } , A _ { t } \right) + \alpha \left( G _ { t:t+n } - Q \left( S _ { t } , A _ { t } \right) \right)
$$

#### sarsa(λ)

后来注意到有效更新不仅可以针对任何 $n$ 步回报， 而且可以针对不同 $n$ 的任何平均 $n$ 步返回。 例如，可以对目标进行更新，该目标是两步回报的一半和四步回报的一半： $\frac{1}{2}G_{t:t+2}+\frac{1}{2}G_{t:t+4}$。 任何一组 $n$ 步返回都可以用这种方式平均，即使是无限集，**只要分量返回的权重为正并且总和为1**。

sarsa(λ)算法可以理解为平均 $n$ 步更新的一种特定方式。 该平均值包含所有 $n$ 步更新，每个更新按比例加权到 $\lambda^{n-1}$（其中$ \lambda \in [0,1]$），并按因子 $1−\lambda$ 归一化，以确保权重总和为1。 结果更新是针对回报，称为 λ回报：
$$
q _ { t } ^ { \lambda } = ( 1 - \lambda ) \sum _ { n = 1 } ^ { \infty } \lambda ^ { n - 1 } G _ { t:t+n }
$$

同上面说的$\frac{1}{2}G_{t:t+2}+\frac{1}{2}G_{t:t+4}$例子一样，这个公式只是给每一步的G加了权值而已，且这个权值按照距离当前状态的远近程度赋值，距离远的权值大，距离近的权值小

现在用这个λ回报来更新Q值，则可以得到Sarsa(λ)的更新公式
$$
Q \left( S _ { t } , A _ { t } \right) \leftarrow Q \left( S _ { t } , A _ { t } \right) + \alpha \left( q _ { t } ^ { \lambda } - Q \left( S _ { t } , A _ { t } \right) \right)
$$
但这个公式太复杂了，还得存所有Episode，不方便学习。为了解决这个问题，我们引入效用追踪（Eligibility Trace）概念。不同的是这次的E值针对的不是一个状态，而是一个状态行为对：
$$
\begin{array} { c } { E _ { 0 } ( s , a ) = 0 } \\ { E _ { t } ( s , a ) = \gamma \lambda E _ { t - 1 } ( s , a ) + 1 \left( S _ { t } = s , A _ { t } = a \right) } \end{array}
$$
它体现的是一个结果与某一个状态行为对的因果关系，与得到结果最近的状态行为对，以及那些在此之前频繁发生的状态行为对对得到这个结果的影响最大。

下式是引入ET概念的SARSA(λ)之后的Q值更新描述：
$$
\begin{aligned} 
E _ { t } ( s , a ) &= \gamma \lambda E _ { t - 1 } ( s , a ) + 1 \left( S _ { t } = s , A _ { t } = a \right)\\
\delta _ { t } = & R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , A _ { t + 1 } \right) - Q \left( S _ { t } , A _ { t } \right) \\ 
& Q ( s , a ) \leftarrow Q ( s , a ) + \alpha \delta _ { t } E _ { t } ( s , a ) 
\end{aligned}
$$
引入ET概念，同时使用SARSA(λ)将可以更有效的在线学习，因为不必要学习完整的Episode，数据用完即可丢弃。ET通常也是更多应用在在线学习算法中(online algorithm)。

### 算法伪代码

{% asset_img Sarsa(λ)算法.png Sarsa(λ)算法 %}

Sarsa(λ)在算法中引入了E表，更新时要更新Q表和E表

## 例



## 代码

{% asset_img Q学习算法例.png Q学习算法 %}

玩上述游戏，黄色圆为宝藏，黑色方框为黑洞，进入黑洞奖励为-1，获得宝藏奖励+1，其他情况奖励为0。

迭代代码

```python
def update():
    # 学习 100 回合
    for episode in range(100):
        # 初始化 state 的观测值
        observation = env.reset()
	    RL.eligibility_trace *= 0   #eligibility trace 只是记录每个回合的每一步, 新回合开始的时候需要将 Trace 清零
        while True:
            # 更新可视化环境
            env.render()

            # RL 大脑根据 state 的观测值挑选 action
            action = RL.choose_action(str(observation))

            # 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)
            observation_, reward, done = env.step(action)
            
            # 决策下一个行动，与Q学习不同
            action_ = RL.choose_action(str(observation_))

            # RL 从这个序列 (state, action, reward, state_) 中学习
            RL.learn(str(observation), action, reward, str(observation_), action_)

            # 将下一个 state 的值传到下一次循环
            observation = observation_
            action = action_ #与Q学习不同，这个动作真实执行了

            # 如果掉下地狱或者升上天堂, 这回合就结束了
            if done:
                break
   
    # 结束游戏并关闭窗口
    print('game over')
    env.destroy()

if __name__ == "__main__":
    # 定义环境 env 和 RL 方式
    env = Maze()
    RL = QLearningTable(actions=list(range(env.n_actions)))

    # 开始可视化环境 env
    env.after(100, update)
    env.mainloop()
```

Sarsa(λ)学习代码（个体类）

```python
class SarsaLambdaTable:
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, trace_decay=0.1):
        self.actions = actions  # a list
        self.lr = learning_rate  # 学习速率
        self.gamma = reward_decay  # 衰减系数
        self.epsilon = e_greedy  # 以多少概率去探索
        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)  # Q表，学习的关键！
        self.lambda_ = trace_decay
        self.eligibility_trace = self.q_table.copy()

    # 依据策略（ε-贪婪策略）选择一个动作
    def choose_action(self, observation):
        self.check_state_exist(observation)  # 如果状态没遇到过就加入Q表
        # action selection
        if np.random.uniform() < self.epsilon:
            # choose best action
            state_action = self.q_table.loc[observation, :]
            # 不直接使用max函数，防止相同值时每次都选择同一个动作
            action = np.random.choice(state_action[state_action == np.max(state_action)].index)
        else:
            # choose random action
            action = np.random.choice(self.actions)
        return action

    # 更新Q表中的值
    def learn(self, s, a, r, s_, a_):
        self.check_state_exist(s_)
        q_predict = self.q_table.ix[s, a]  # 估计值

        # 计算现实值
        if s_ != 'terminal':
            q_target = r + self.gamma * self.q_table.ix[s_, a_]  # next state is not terminal
        else:
            q_target = r  # next state is terminal
        error = q_target - q_predict  # update

        # Method 1:
        self.eligibility_trace.ix[s, a] += 1

        # Method 2:
        self.eligibility_trace.ix[s, a] *= 0
        self.eligibility_trace.ix[s, a] = 1

        # Q update
        self.q_table += self.lr * error * self.eligibility_trace

        self.eligibility_trace *= self.gamma * self.lambda_


    # 检查是否有此状态，没有就添加
    def check_state_exist(self, state):
        if state not in self.q_table.index:
            to_be_append = pd.Series(
                [0] * len(self.actions),
                index=self.q_table.columns,
                name=state,
            )
            # append new state to q table
            self.q_table = self.q_table.append(to_be_append)
            # append new state to e table
            self.eligibility_trace = self.eligibility_trace.append(to_be_append)
```

{assert_img Sarsa(λ)两种更新算法.png Sarsa(λ)两种更新方法}

# DQN（Deep Q learning）



