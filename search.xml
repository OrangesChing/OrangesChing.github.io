<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>强化学习实践——程序建模模板及Gym使用</title>
      <link href="/2019/12/05/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B51%E2%80%94%E2%80%94%E7%A8%8B%E5%BA%8F%E5%BB%BA%E6%A8%A1%E6%A8%A1%E6%9D%BF%E5%8F%8AGym%E4%BD%BF%E7%94%A8/"/>
      <url>/2019/12/05/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B51%E2%80%94%E2%80%94%E7%A8%8B%E5%BA%8F%E5%BB%BA%E6%A8%A1%E6%A8%A1%E6%9D%BF%E5%8F%8AGym%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习解决问题的设计流程"><a href="#强化学习解决问题的设计流程" class="headerlink" title="强化学习解决问题的设计流程"></a>强化学习解决问题的设计流程</h1><ol><li>将实际问题建模成马尔可夫决策过程，抽象出五元组（状态集、动作集、状态转移概率、奖励函数、折扣因子），  其中奖励与实际目标相关联</li><li>根据动作是否连续选择对应的算法</li></ol><h1 id="强化学习的两类对象"><a href="#强化学习的两类对象" class="headerlink" title="强化学习的两类对象"></a>强化学习的两类对象</h1><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>需要存储的信息</p><ol><li>所有可能状态集</li><li>智能体的行动集</li><li>智能体当前状态</li></ol><p>需要做的事</p><ol><li>响应智能体行为，更新环境，给予智能体即时奖励</li><li>给予个体观测值</li><li>终止交互的条件</li></ol><p>环境模板就不给出了，gym库在设计环境和个体交互时有规范的接口，可以借鉴他们的规范和接口</p><h2 id="个体"><a href="#个体" class="headerlink" title="个体"></a>个体</h2><p>需要存储的信息</p><ol><li>环境对象信息</li><li>状态信息</li></ol><p>需要做的事：</p><ol><li>观察功能：获得环境信息，哪些行为是允许的，获得的奖励</li><li>决策功能：根据当前观测来判断下一时刻该采取什么行为，按照一个策略产生一个行动</li><li>执行行动功能</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, env: Env)</span>:</span></span><br><span class="line">        self.env = env      <span class="comment"># 个体持有环境的引用</span></span><br><span class="line">        self.state = <span class="literal">None</span>   <span class="comment"># 个体当前的观测，最好写成obs.</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">performPolicy</span><span class="params">(self, state)</span>:</span> <span class="keyword">pass</span> <span class="comment"># 执行一个策略</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">act</span><span class="params">(self, a)</span>:</span>       <span class="comment"># 执行一个行为</span></span><br><span class="line">        <span class="keyword">return</span> self.env.step(a)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learning</span><span class="params">(self)</span>:</span> <span class="keyword">pass</span>   <span class="comment"># 学习过程</span></span><br></pre></td></tr></table></figure><h1 id="Gym"><a href="#Gym" class="headerlink" title="Gym"></a>Gym</h1><p>gym主要用于生成常见的强化学习环境，方便学习，即相当于给你写好了上面的环境类，你只需要写智能体类了。</p><p>gym的官方网址在：<a href="https://link.zhihu.com/?target=https%3A//gym.openai.com/" target="_blank" rel="noopener">这里</a>，其库代码托管地址在：<a href="https://link.zhihu.com/?target=https%3A//github.com/openai/gym/tree/master/gym" target="_blank" rel="noopener">这里</a></p><h2 id="Gym的安装"><a href="#Gym的安装" class="headerlink" title="Gym的安装"></a>Gym的安装</h2><h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><p>OpenAI官网没有说支持Windows，最好不要在Window上跑，因为我看视频写博客都在Windows了，就硬着头皮装了 ，没想到居然好像能用</p><p>找到Anaconda3的开始菜单目录，找到指令终端：<strong>Anaconda Prompt</strong>。</p><p>较老版本的Anaconda3这里没有这一项，无妨，可以去Anaconda3的安装文件夹下去找。运行这个cmd.exe，切记以管理员身份运行。</p><p>在指令终端输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gym</span><br></pre></td></tr></table></figure><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p>Anaconda创建虚拟环境 conda create –-name 你要创建的名字 python=版本号，例如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create –-name gymlab python=3.5</span><br></pre></td></tr></table></figure><p>操作完此步之后，会在anaconda3/envs文件夹下多一个gymlab。Python3.5就在gymlab下得lib文件夹中。</p><p>开一个新的终端激活虚拟环境</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate gymlab</span><br></pre></td></tr></table></figure><p>下载gym</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone [openai/gym](https://github.com/openai/gym.git)</span><br></pre></td></tr></table></figure><p>安装gym，会装一系列的库，如果报错可以先安装依赖项，键入命令sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd gym</span><br><span class="line">pip install –e &apos;.[all]&apos;</span><br></pre></td></tr></table></figure><p>装完后可以将你的gym安装文件的目录写到环境变量中，打开.bashrc文件，在末尾加入语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PYTHONPATH= [your gymPath]：$PYTHONPATH</span><br></pre></td></tr></table></figure><h3 id="验证安装成功"><a href="#验证安装成功" class="headerlink" title="验证安装成功"></a>验证安装成功</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym env = gym.make(<span class="string">'CartPole-v0'</span>) </span><br><span class="line">env.reset() </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):    </span><br><span class="line">    env.render()    </span><br><span class="line">    env.step(env.action_space.sample())</span><br></pre></td></tr></table></figure><h2 id="Gym的使用"><a href="#Gym的使用" class="headerlink" title="Gym的使用"></a>Gym的使用</h2><p>使用gym编写自己的Agent代码，需要在在Agent类中声明一个env变量，指向对应的环境类，个体使用自己的代码产生一个行为，将该行为送入env的step方法中，可通过 render()显示图像</p><p>重要的四个函数</p><ul><li>env = gym.make(‘环境名’);</li><li>env.reset()</li><li>env.render()</li><li>env.step()</li></ul><h3 id="gym-make-‘环境名’-：建立环境类对象"><a href="#gym-make-‘环境名’-：建立环境类对象" class="headerlink" title="gym.make(‘环境名’)：建立环境类对象"></a>gym.make(‘环境名’)：建立环境类对象</h3><p>在自己的代码中建立环境类对象呢</p><ol><li>在gym库里注册了的对象，你只要使用下面的语句：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import gym</span><br><span class="line">env = gym.make(&quot;registered_env_name&quot;)</span><br></pre></td></tr></table></figure><p>其中不同的环境类有不同的注册名，只要把make方法内的字符串改成对应的环境名就可以了。</p><ol><li>使用自己编写的未注册的环境类，这种很简单，同一般的建立对象的语句没什么区别：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env = MyEnvClassName()</span><br></pre></td></tr></table></figure><h3 id="reset-：重新初始化函数"><a href="#reset-：重新初始化函数" class="headerlink" title="reset()：重新初始化函数"></a>reset()：重新初始化函数</h3><p>在强化学习算法中，智能体需要一次次地尝试，累积经验，然后从经验中学到好的动作。一次尝试我们称之为一条轨迹或一个episode. 每次尝试都要到达终止状态. 一次尝试结束后，智能体需要从头开始，这就需要智能体具有重新初始化的功能。函数reset()就是这个作用。</p><p>reset()的源代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_reset</span><span class="params">()</span></span></span><br><span class="line">    self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))  #利用均匀随机分布初试化环境的状态</span><br><span class="line">    self.steps_beyond_done = <span class="literal">None</span>  <span class="comment">#设置当前步数为None</span></span><br><span class="line">    <span class="keyword">return</span> np.array(self.state)  <span class="comment">#返回环境的初始化状态</span></span><br></pre></td></tr></table></figure><h3 id="render-：图像引擎"><a href="#render-：图像引擎" class="headerlink" title="render()：图像引擎"></a>render()：图像引擎</h3><p>render()函数在这里扮演图像引擎的角色。一个仿真环境必不可少的两部分是物理引擎和图像引擎。物理引擎模拟环境中物体的运动规律；图像引擎用来显示环境中的物体图像。其实，对于强化学习算法，该函数可以没有。但是，为了便于直观显示当前环境中物体的状态，图像引擎还是有必要的。另外，加入图像引擎可以方便我们调试代码。</p><h3 id="step-：物理引擎"><a href="#step-：物理引擎" class="headerlink" title="step()：物理引擎"></a>step()：物理引擎</h3><p>该函数在仿真器中扮演物理引擎的角色。它描述了智能体与环境交互的所有信息，是环境文件中最重要的函数。在该函数中，一般利用智能体的运动学模型和动力学模型计算下一步的状态和立即回报，并判断是否达到终止状态。</p><ul><li><p>输入：动作a</p></li><li><p>输出：下一步状态，立即回报，是否终止，调试项。</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">state, reward, is_done, info = env.step(a)</span><br></pre></td></tr></table></figure><p>state 是一个元组或numpy数组，其提供的信息维度应与观测空间的维度一样、每一个维度的具体指在制定的low与high之间，保证state信息符合这些条件是env类的_step方法负责的事情。</p><p>reward 则是根据环境的动力学给出的即时奖励，它就是一个数值。</p><p>is_done 是一个布尔变量，True或False，你可以根据具体的值来安排个体的后续动作。</p><p>info 提供的数据因环境的不同差异很大，通常它的结构是一个字典：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;key1&quot;:data1,&quot;key2&quot;:data2,...&#125;</span><br></pre></td></tr></table></figure><h3 id="例"><a href="#例" class="headerlink" title="例"></a>例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym <span class="keyword">import</span> time </span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)   <span class="comment">#创造环境 </span></span><br><span class="line">observation = env.reset()   <span class="comment">#初始化环境，observation为环境状态</span></span><br><span class="line">count = <span class="number">0</span> </span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):    </span><br><span class="line">    action = env.action_space.sample()  <span class="comment">#随机采样动作    </span></span><br><span class="line">    observation, reward, done, info = env.step(action)  <span class="comment">#与环境交互，获得下一步的时刻的观察值，奖励    </span></span><br><span class="line">    <span class="keyword">if</span> done:                     </span><br><span class="line">        <span class="keyword">break</span>    </span><br><span class="line">    env.render()         <span class="comment">#绘制场景    </span></span><br><span class="line">    count+=<span class="number">1</span>    </span><br><span class="line">    time.sleep(<span class="number">0.2</span>)      <span class="comment">#每次等待0.2s </span></span><br><span class="line">print(count)             <span class="comment">#打印该次尝试的步数</span></span><br></pre></td></tr></table></figure><h2 id="自定义环境类"><a href="#自定义环境类" class="headerlink" title="自定义环境类"></a>自定义环境类</h2><p>gym库的核心在文件core.py里，里面定义了两个最基本的类Env和Space。前者是所有环境类的基类，后者是所有空间类的基类。</p><h3 id="Env类"><a href="#Env类" class="headerlink" title="Env类"></a>Env类</h3><p>自定义类的时候继承这个Env基类，并重写里面的一些方法，就可以实现自己的环境类了</p><p>要实现的方法有：</p><ul><li><strong>_reset(self)</strong>：初始化，开启个体与环境交互前调用该方法，确定个体的初始状态以及其他可能的一些初始化设置</li><li><strong>_step(self, action)</strong>：物理引擎（重要部分），确定个体的下一个状态、奖励信息、是否Episode终止，以及一些额外的信息</li><li><strong>_render(self, mode=’human’, close=False)</strong>：图像引擎，如果需要将个体与环境的交互以动画的形式展示出来的话，需要重写该方法。简单的UI设计可以用gym包装好了的pyglet方法来实现，这些方法在rendering.py文件里定义。具体使用这些方法进行UI绘制需要了解基本的OpenGL编程思想和接口</li><li><strong>_seed(self, seed=None)</strong> ：设置随机数种子</li><li><strong>_close（可选）</strong>：可以不实现</li></ul><p>要设置的参数有：</p><ul><li><strong>action_space 动作空间</strong>：一个描述所有有效动作的Space对象</li><li><strong>observation_space 观察空间</strong>：一个描述所有有效观察的Space对象</li><li><strong>reward_range 奖励范围</strong>：一个包含最大最小可能奖励的元组</li></ul><p>环境基类的一段解释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">class Env(object):</span><br><span class="line">    &quot;&quot;&quot;The main OpenAI Gym class. It encapsulates an environment with</span><br><span class="line">    arbitrary behind-the-scenes dynamics. An environment can be</span><br><span class="line">    partially or fully observed.</span><br><span class="line">    The main API methods that users of this class need to know are:</span><br><span class="line">        step</span><br><span class="line">        reset</span><br><span class="line">        render</span><br><span class="line">        close</span><br><span class="line">        seed</span><br><span class="line">    When implementing an environment, override the following methods</span><br><span class="line">    in your subclass:</span><br><span class="line">        _step</span><br><span class="line">        _reset</span><br><span class="line">        _render</span><br><span class="line">        _close</span><br><span class="line">        _seed</span><br><span class="line">    And set the following attributes:</span><br><span class="line">        action_space: The Space object corresponding to valid actions</span><br><span class="line">        observation_space: The Space object corresponding to valid observations</span><br><span class="line">        reward_range: A tuple corresponding to the min and max possible rewards</span><br><span class="line">    Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.</span><br><span class="line">    The methods are accessed publicly as &quot;step&quot;, &quot;reset&quot;, etc.. The</span><br><span class="line">    non-underscored versions are wrapper methods to which we may add</span><br><span class="line">    functionality over time.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # Override in SOME subclasses</span><br><span class="line">    def _close(self):</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">    # Set these in ALL subclasses</span><br><span class="line">    action_space = None</span><br><span class="line">    observation_space = None</span><br><span class="line"></span><br><span class="line">    # Override in ALL subclasses</span><br><span class="line">    def _step(self, action): raise NotImplementedError</span><br><span class="line">    def _reset(self): raise NotImplementedError</span><br><span class="line">    def _render(self, mode=&apos;human&apos;, close=False): return</span><br><span class="line">    def _seed(self, seed=None): return []</span><br></pre></td></tr></table></figure><h3 id="Space类"><a href="#Space类" class="headerlink" title="Space类"></a>Space类</h3><p>用来描述空间的，比如行为空间，状态空间等。从Space基类衍生出几个常用的空间类，其中最主要的是<strong>Discrete类</strong>和<strong>Box</strong>类。前者对应于一维离散空间，后者对应于多维连续空间。它们既可以应用在行为空间中，也可以用来描述状态空间，具体怎么用看问题本身。</p><p>例如：</p><p>如果我要描述上篇提到的一个4*4的格子世界，其一共有16个状态，每一个状态只需要用一个数字来描述，这样我可以把这个问题的状态空间用Discrete(16)对象来描述就可以了。</p><p>对于另外一个经典的小车爬山的问题，小车的状态是用两个变量来描述的，一个是小车对应目标旗杆的水平距离，另一个是小车的速度（是沿坡度切线方向的速率还是速度在水平方向的分量这个没仔细研究），因此环境要描述小车的状态需要2个连续的变量。由于描述小车的状态数据对个体完全可见，因此小车的状态空间即是小车的观测空间，此时再用Discrete来描述就不行了，要用Box类，Box空间可以定义多维空间，每一个维度可以用一个最低值和最大值来约束。同时小车作为个体可以执行的行为只有3个：左侧加速、不加速、右侧加速。因此行为空间可以用Discrete来描述。最终，该环境类的观测空间和行为空间描述如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">self.min_position = <span class="number">-1.2</span></span><br><span class="line">self.max_position = <span class="number">0.6</span></span><br><span class="line">self.max_speed = <span class="number">0.07</span></span><br><span class="line">self.goal_position = <span class="number">0.5</span> </span><br><span class="line">self.low = np.array([self.min_position, -self.max_speed])</span><br><span class="line">self.high = np.array([self.max_position, self.max_speed])</span><br><span class="line">self.action_space = spaces.Discrete(<span class="number">3</span>)  <span class="comment">#一个参数n</span></span><br><span class="line">self.observation_space = spaces.Box(self.low, self.high)  <span class="comment">#每个维度的最小最大值</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>强化学习基础——总结</title>
      <link href="/2019/12/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%805%E2%80%94%E2%80%94%E6%80%BB%E7%BB%93/"/>
      <url>/2019/12/04/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%805%E2%80%94%E2%80%94%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习问题提出"><a href="#强化学习问题提出" class="headerlink" title="强化学习问题提出"></a>强化学习问题提出</h1><p>强化学习是解决<strong>序列决策问题</strong>，即<strong>状态-&gt;行动-&gt;新状态…..-&gt;终点</strong> 这一系列动作的决策，做出的决策要<strong>最大化总的奖励</strong>，</p><p>其中有很多概念，要理解清楚：</p><ul><li>历史：【观察，行为，奖励】构成的序列</li><li>状态：对历史信息的总结，决定奖励啊行为的信息，不等于观察值</li><li>行动：智能体做的动作</li><li>奖励：环境对智能体动作的即时反馈</li><li>策略：决定智能体行为的机制，状态到行为的映射</li><li>价值函数：未来奖励的预测，用来评价当前状态的好坏程度</li><li>模型：智能体对环境的建模。至少要解决 状态转移概率 和 预测可获得的即时奖励 两个问题</li></ul><p>其中智能体可由三个部分组成：策略、价值函数、模型</p><h1 id="马尔科夫全家桶"><a href="#马尔科夫全家桶" class="headerlink" title="马尔科夫全家桶"></a>马尔科夫全家桶</h1><h2 id="马尔科夫性"><a href="#马尔科夫性" class="headerlink" title="马尔科夫性"></a>马尔科夫性</h2><p>定义：未来只依赖于最近给定的状态，则认为这个状态有<strong>马尔科夫性</strong></p><p><strong>转移概率公式</strong>和<strong>转移概率矩阵</strong>可用来表示当前任意状态转移到其他状态的概率</p><script type="math/tex; mode=display">\mathcal{P}_{s s^{\prime}}=\mathrm{P}\left[S_{t+1}=s^{\prime} | S_{t}=s\right]</script><h2 id="马尔科夫过程（马尔科夫链）"><a href="#马尔科夫过程（马尔科夫链）" class="headerlink" title="马尔科夫过程（马尔科夫链）"></a>马尔科夫过程（马尔科夫链）</h2><p>由有限具有马尔科夫性的状态和状态转移概率组成的马尔科夫链</p><p>可以用一个元组$<s, \mathcal{p}>$表示</s,></p><ul><li>$S$ 是具有马尔科夫性的有限随机状态集$\{\mathrm{S}_1，\mathrm{S}_2，\cdots\}$</li><li>$\mathcal{P}$是状态之间的转移概率矩阵</li></ul><h2 id="马尔科夫奖励过程"><a href="#马尔科夫奖励过程" class="headerlink" title="马尔科夫奖励过程"></a>马尔科夫奖励过程</h2><p>在马尔科夫过程的基础上加入奖励和衰减系数，我们要做的是评价这个策略</p><p>可以用四元组$<s, p, r, \gamma>$表示</s,></p><ul><li>$S$ 是具有马尔科夫性的有限随机状态集$\{\mathrm{S}_1，\mathrm{S}_2，\cdots\}$</li><li>$\mathcal{P}$是状态之间的转移概率矩阵</li><li>$R$是<strong>即时奖励</strong>，即离开状态s可获得的奖励，与下一刻去哪无关，$R_{s}=E\left[R_{t+1} | S_{t}=s\right]$</li><li>$\gamma$是衰减系数，用在计算收益中</li></ul><p>由于即时奖励$R$，与具体的下一状态没有关系，引入<strong>收益（Return）$G_t$</strong>，来量化一个片段（Episode）的奖励和</p><script type="math/tex; mode=display">G_{t}=R_{t+1}+\gamma R_{t+2}+\ldots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}</script><p>又因为收益$G_t$只是针对一个Episode，不是针对长期的过程，不能用来评估整个马尔科夫奖励过程的收益，所以引入<strong>价值函数（Value Function）</strong>来衡量某一状态或行为的长期价值（计算方式：只要一个Episode出现了状态s，就把这个Episode的收益加进去，最后除以加进去Episode的数量）</p><script type="math/tex; mode=display">v(s)=E\left[G_{t} | S_{t}=s\right]</script><p>因为这个价值函数的公式算起来很复杂（要记录所有Episode），所以引入Bellman方程，得到<strong>价值函数的Bellman期望方程</strong>（计算方式：该状态的即时奖励 + 遍历该状态的各个后继状态，对于每一个后继状态：$\gamma$  × 状态转移概率  × 后继状态的值函数 求和）</p><script type="math/tex; mode=display">v(s)=\mathcal{R}_{s}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}} v\left(s^{\prime}\right)</script><p>这个公式把刚刚复杂的公式分成了两个部分：状态s的即时奖励和下一时刻的价值期望，这样就可以很方便的用递归或者迭代的方式求解</p><p>再进一步，转化为矩阵形式</p><script type="math/tex; mode=display">\begin{aligned} v & = \mathcal { R } + \gamma \mathcal { P } v \\ ( I - \gamma \mathcal { P } ) v & = \mathcal { R } \\ v & = ( l - \gamma \mathcal { P } ) ^ { - 1 } \mathcal { R } \end{aligned}</script><h2 id="马尔科夫决策过程（最终用来描述强化学习问题）"><a href="#马尔科夫决策过程（最终用来描述强化学习问题）" class="headerlink" title="马尔科夫决策过程（最终用来描述强化学习问题）"></a>马尔科夫决策过程（最终用来描述强化学习问题）</h2><p>在马尔科夫奖励过程的基础上加入动作</p><p>可以用一个五元组$M=(S,A,P,R,\gamma)$描述</p><ul><li><p>$ S $：表示状态集(states)，有$s \in S$，$s_i$ 表示第 $i$ 步的状态。</p></li><li><p>$A$：表示一组动作(actions)，有$a \in A$，$a_i$ 表示第 $i$ 步的动作。</p></li><li><p>$P$：表示状态转移概率。表示的是在当前$s \in S$状态下，经过$a \in A$作用后，会转移到的其他状态的概率分布情况。比如，在状态$s$下执行动作$a$，转移到$s’$的概率可以表示为$p(s’|s,a)$，具体的数学表达式如下：</p><script type="math/tex; mode=display">\mathcal{P}_{s s^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} | S_{t}=s, A_{t}=a\right]</script><p>（个人理解：马尔科夫奖励过程是在当前状态下选择到下一状态，而马尔可夫决策过程是在当前状态下先选择一个动作，这个动作会有不同的概率导向不同的新状态，智能体只能做这个动作，不能控制这个动作导向的状态，这个动作导向的结果由环境决定，所以马尔可夫决策过程很适合描述强化学习场景。这里的状态转移概率矩阵是三维的，第一维是当前状态，第二维是所做动作，第三维是下一状态，下面的回报函数同理）</p></li><li><p>$R$：回报函数(reward function)。$R(s, a)$ 描述了在状态 $s$做动作 $a$的奖励。与MRP的奖励与状态对应不同，<strong>MDP的奖励是与动作对应的</strong>，具体的数学表达式如下：</p><script type="math/tex; mode=display">\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} | S_{t}=s, A_{t}=a\right]</script></li><li><p>$\gamma$ ：衰减系数</p></li></ul><p>上述的状态转移概率只是做了某个动作后，从临时状态到某具体状态的转移概率。在某个状态做某一个动作的概率用<strong>策略(Policy)</strong>来表示。策略是概率的集合或分布，其元素是在某一状态s才去可能的行为a的概率：</p><script type="math/tex; mode=display">\pi ( a | s ) = \mathbb { P } \left[ A _ { t } = a | S _ { t } = s \right]</script><h3 id="评估策略"><a href="#评估策略" class="headerlink" title="评估策略"></a>评估策略</h3><h4 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h4><p>MDP的即时奖励，收益$G_t$与MRP一样，但是因为引入了动作，所以价值函数不一样了，MDP的价值函数分为两种</p><ul><li><strong>状态价值函数$v_\pi(s)$</strong>  ，表示从状态$s$开始，<strong>遵循当前策略</strong>时所获得的收获的期望；或者说在执行当前策略 $\pi$ 时，衡量个体处在状态 $s$ 时的价值大小。数学表示如下：</li></ul><script type="math/tex; mode=display">v _ { \pi } ( s ) = \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s \right]</script><ul><li><p><strong>行为价值函数$q_\pi(s,a)$</strong>，表示在执行策略 $\pi$ 时，对当前状态 $s$ 执行某一具体行为 $a$ 所能的到的收获的期望；或者说在遵循当前策略π时，衡量对当前状态执行行为a的价值大小。行为价值函数一般都是与某一特定的状态相对应的，更精细的描述是<strong>状态-行为</strong>价值函数。行为价值函数的公式描述如下：</p><script type="math/tex; mode=display">q _ { \pi } ( s , a ) = \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s , A _ { t } = a \right]</script></li><li><p>两者的关系</p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \sum _ { a \in A } \pi ( a | s ) q _ { \pi } ( s , a )\\q _ { \pi } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { \pi } \left( s ^ { \prime } \right)\\v _ { \pi } ( s ) = \sum _ { a \in \mathcal { A } } \pi ( a | s ) \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { \pi } \left( s ^ { \prime } \right) \right)\\q _ { \pi } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } \sum _ { a ^ { \prime } \in \mathcal { A } } \pi \left( a ^ { \prime } | s ^ { \prime } \right) q _ { \pi } \left( s ^ { \prime } , a ^ { \prime } \right)</script><p><strong>注:MDPs 中，任何不说明策略π 的情况下，讨论值函数都是在耍流氓！</strong></p></li></ul><h4 id="Bellman期望方程"><a href="#Bellman期望方程" class="headerlink" title="Bellman期望方程"></a>Bellman期望方程</h4><p>和 MRP 相似，MDPs 中的值函数也能分解成瞬时奖励和后继状态的值函数两部分的贝尔曼方程</p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \mathbb { E } _ { \pi } \left[ R _ { t + 1 } + \gamma v _ { \pi } \left( S _ { t + 1 } \right) | S _ { t } = s \right]</script><script type="math/tex; mode=display">q _ { \pi } ( s , a ) = \mathbb { E } _ { \pi } \left[ R _ { t + 1 } + \gamma q _ { \pi } \left( S _ { t + 1 } , A _ { t + 1 } \right) | S _ { t } = s , A _ { t } = a \right]</script><p><strong>Bellman期望方程矩阵形式</strong></p><script type="math/tex; mode=display">v _ { \pi } = \mathcal { R } ^ { \pi } + \gamma \mathcal { P } ^ { \pi } v _ { \pi }</script><script type="math/tex; mode=display">v _ { \pi } = \left( l - \gamma \mathcal { P } ^ { \pi } \right) ^ { - 1 } \mathcal { R } ^ { \pi }</script><p>下面例子可以帮助理解计算，及区分MDP和MRP</p><img title="MDP和MRP对比" data-src="/2019/12/04/强化学习基础5——总结/MDPvsMRP.jpg" class="lazyload"><p>假如，现在你女朋友生日，你纠结要不要给他送口红，你的策略是：0.4的概率不给他送，0.6的概率给他送。你做了这个动作之后，你女朋友的心情对你来说就处于”又生又死的状态“，并且你会因为在生日这天送了他一只口红得到好感度加1的奖励。当你看向你女朋友时，他的心情会坍缩到具体的一个状态，坍缩的概率如图，可能你女朋友不喜欢这个色号他就不高兴，可能正中下怀她就非常高兴的发朋友圈了。</p><p>现在你想计算女朋友生日这个状态（无论做什么事），对你刷好感度到满级有多大意义（V函数），并且向计算在女朋友生日这天送口红，对你刷高感度到满级有多大意义（Q函数）。对于V函数，可以算我从这个状态出发做所有动作的意义平均值就好了。对于Q函数，可以算我做了这个动作的奖励+最后达到的女朋友状态的意义的平均值</p><h3 id="最优化策略"><a href="#最优化策略" class="headerlink" title="最优化策略"></a>最优化策略</h3><h4 id="最优策略的定义"><a href="#最优策略的定义" class="headerlink" title="最优策略的定义"></a>最优策略的定义</h4><ul><li>什么是最优策略</li></ul><p>当对于任何状态 $s$，遵循策略π的价值不小于遵循策略 $\pi’$ 下的价值，则策略 $\pi$ 优于策略 $\pi’$：</p><script type="math/tex; mode=display">\pi \geq \pi ^ { \prime } \text { if } v _ { \pi } ( s ) \geq v _ { \pi ^ { \prime } } ( s ) , \forall s</script><ul><li><strong>如何寻找最优策略</strong></li></ul><p>可以通过最大化最优行为价值函数来找到最优策略：</p><script type="math/tex; mode=display">\pi _ { * } ( a | s ) = \left\{ \begin{array} { l l } { 1 } & { \text { if } a = \underset { a \in \mathcal { A } } { \operatorname { argmax } } q _ { * } ( s , a ) } \\ { 0 } & { \text { otherwise } } \end{array} \right.</script><p>对于任何MDP问题，总存在一个确定性的最优策略；同时如果我们知道最优行为价值函数，则表明我们找到了最优策略。</p><p>针对 $v_*$ ，一个状态的最优价值等于从该状态出发采取的所有行为产生的行为价值中最大的那个行为价值：</p><script type="math/tex; mode=display">v _ { * } ( s ) = \max _ { a } q _ { * } ( s , a )</script><h4 id="Bellman最优方程"><a href="#Bellman最优方程" class="headerlink" title="Bellman最优方程"></a>Bellman最优方程</h4><p>针对 $v _ { * }$ ，有：</p><script type="math/tex; mode=display">v _ { * } ( s ) = \max _ { a } \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)</script><p>针对 $q _ { * }$ ，有：</p><script type="math/tex; mode=display">q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } \max _ { a ^ { \prime } } q _ { * } \left( s ^ { \prime } , a ^ { \prime } \right)</script><h1 id="基于模型的动态规划问题"><a href="#基于模型的动态规划问题" class="headerlink" title="基于模型的动态规划问题"></a>基于模型的动态规划问题</h1><h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><p><strong>问题：</strong>寻找最优策略π</p><p><strong>解决方案：</strong>在当前策略上迭代计算$v$值，再根据$v$值贪婪地更新策略，如此反复多次，最终得到最优策略$\pi^<em>$ 和最优状态价值函数$V^</em>$  </p><p><strong>具体方法：</strong></p><ol><li><p>在给定的策略下迭代更新价值函数，采用Bellman期望方程，计算方法：</p><ul><li><p><strong>同步反向迭代</strong>，即在每次迭代过程中，对于第$k+1$ 次迭代，所有的状态s的价值用$v_k(s’)$ 计算并更新该状态第$k+1$  次迭代中使用的价值$v_k(S)$ ，其中$s’$是$s$的后继状态。此种方法通过反复迭代最终将收敛至$V_{\pi}$  。</p><script type="math/tex; mode=display">v _ { k + 1 } ( s ) = \sum _ { a \in A } \pi ( a | s ) \left( R _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in S } P _ { s s ^ { \prime } } ^ { a } v _ { k } \left( s ^ { \prime } \right) \right)</script><p>即：一次迭代内，状态s的价值等于前一次迭代该状态的即时奖励与所有s的下一个可能状态s’ 的价值与其概率乘积的和，如图示：</p><p>公式的矩阵形式是：</p><script type="math/tex; mode=display">\mathbf { v } ^ { k + 1 } = \mathcal { R } ^ { \pi } + \gamma \mathcal { P } ^ { \pi } \mathbf { v } ^ { k }</script></li><li><p><strong>异步反向迭代</strong>，即在第k次迭代使用<strong>当次</strong>迭代的状态价值来更新状态价值。</p></li></ul></li><li>在当前策略基础上，贪婪地选取行为，使得后继状态价值增加最多：<script type="math/tex; mode=display">\pi' = greedy(v_{\pi})</script></li></ol><img title="This is an example image" data-src="/2019/12/04/强化学习基础5——总结/policy_iteration_algorithm.png" class="lazyload"><p><strong>改进策略迭代的方法</strong></p><p>有时候不需要持续迭代至最有价值函数，可以设置一些条件提前终止迭代</p><ul><li>比如设定一个$ \epsilon $，比较两次迭代的价值函数平方差</li><li>直接设置迭代次数</li></ul><h2 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h2><p><strong>问题：</strong>寻找最优策略π</p><p><strong>解决方案：</strong>采用Bellman最优方程，过程与策略迭代差不多，但整个过程中没有遵循任何策略，对每一个当前状态 s ,对每个可能的动作 a 都计算一下采取这个动作后到达的下一个状态的期望价值。看看哪个动作可以到达的状态的期望价值函数最大，就将这个最大的期望价值函数作为当前状态的价值函数 $V(s)$ ，循环执行这个步骤，直到价值函数收敛。</p><script type="math/tex; mode=display">V _ { k + 1 } ( s ) = \max _ { a } \sum _ { s ^ { \prime } , r } P \left( s ^ { \prime } , r | s , a \right) \left( r + \gamma V _ { k } \left( s ^ { \prime } \right) \right)</script><img title="This is an example image" data-src="/2019/12/04/强化学习基础5——总结/value_iteration_algorithm.png" class="lazyload"><p>对每一个当前状态 $s$ ,对每个可能的动作 $a$ 都计算一下采取这个动作后到达的下一个状态的期望价值。看看哪个动作可以到达的状态的期望价值函数最大，就将这个最大的期望价值函数作为当前状态的价值函数 $V(s)$ 循环执行这个步骤，直到价值函数收敛。</p><h2 id="价值迭代算法与策略迭代算法的区别"><a href="#价值迭代算法与策略迭代算法的区别" class="headerlink" title="价值迭代算法与策略迭代算法的区别"></a>价值迭代算法与策略迭代算法的区别</h2><ul><li>策略迭代有一个策略直接作用于value空间（即不会有value值来构建策略，策略再构建value值的过程）；而价值迭代过程其间得到的价值函数，不对应任何策略</li><li>价值迭代是根据状态期望值选择动作，而策略迭代是先估计状态值然后修改策略 </li></ul><h1 id="基于无模型的强化学习问题"><a href="#基于无模型的强化学习问题" class="headerlink" title="基于无模型的强化学习问题"></a>基于无模型的强化学习问题</h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>强化学习基础4——基于无模型的强化学习方法</title>
      <link href="/2019/12/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%804%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E6%97%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
      <url>/2019/12/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%804%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E6%97%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="简介-Introduction"><a href="#简介-Introduction" class="headerlink" title="简介 Introduction"></a>简介 Introduction</h1><p>马尔可夫决策过程可以利用元组$<s,a,p,r,\gamma>$来描述，而根据转移概率P是否已知，可以分为基于模型的动态规划方法和基于无模型的强化学习方法。这两类都包括策略迭代算法、值迭代算法、策略搜索算法。</s,a,p,r,\gamma></p><p>上一章学习了基于模型的动态规划方法，学习了如何从理论上解决一个<strong>已知</strong>的MDP：通过动态规划来评估一个给定的策略，并且得到最优价值函数，根据最优价值函数来确定最优策略；也可以直接进行不基于任何策略的状态价值迭代得到最优价值函数和最优策略。</p><p>这一章我们将讨论解决一个可以被认为是MDP、但却不掌握MDP具体细节（不知道环境是怎样的）的问题，也就是讲述如何直接从Agent与环境的交互来得得到一个估计的最优价值函数和最优策略。这部分内容分为两部分</p><ul><li><p>第一部分聚焦于策略评估，也就是预测，直白的说就是在给定的策略同时不清楚MDP细节的情况下，估计得到值函数。</p></li><li><p>第二部分将利用第一部分的策略评估的主要观念来进行控制进而找出最优策略，最大化Agent的奖励。</p></li></ul><h1 id="策略评估（无模型的预测）"><a href="#策略评估（无模型的预测）" class="headerlink" title="策略评估（无模型的预测）"></a>策略评估（无模型的预测）</h1><p>这部分内容分为三个小部分，分别是蒙特卡洛强化学习、时序差分强化学习和介于两者之间的$\lambda$时序差分强化学习</p><h2 id="蒙特卡洛强化学习-Monte-Carlo-Reinforcement-Learning"><a href="#蒙特卡洛强化学习-Monte-Carlo-Reinforcement-Learning" class="headerlink" title="蒙特卡洛强化学习 Monte-Carlo Reinforcement Learning"></a>蒙特卡洛强化学习 Monte-Carlo Reinforcement Learning</h2><h3 id="启发（为什么会想到这个方法）"><a href="#启发（为什么会想到这个方法）" class="headerlink" title="启发（为什么会想到这个方法）"></a>启发（为什么会想到这个方法）</h3><p>前面讲的主要内容是整个问题可以转换成一个马尔科夫决策过程(MDP)五元组，但是，在现实世界中，我们无法同时知道这个5元组。比如P，状态转移概率就很难知道，P不知道，我们就无法使用bellman方程来求解V和Q值。但是我们依然要去解决这个问题。怎么办？</p><p>一个想法是，虽然我不知道状态转移概率P，但是这个概率是真实存在的。我们可以直接去尝试，不断采样，然后会得到奖赏，通过奖赏来评估值函数。这个想法与蒙特卡罗方法的思想是一致的。我们可以尝试很多次，最后估计的V值就会很接近真实的V值了。</p><img title="This is an example image" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/monte_carlo.png" class="lazyload"><p>比如上图，矩形的面积我们可以轻松得到，但是对于阴影部分的面积，我们积分是比较困难的。所以为了计算阴影部分的面积，我们可以在矩形上均匀地撒豆子，然后统计在阴影部分的豆子数占总的豆子数的比例，就可以估算出阴影部分的面积了</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><strong>蒙特卡洛强化学习</strong>：又叫统计模拟方法，在不清楚MDP状态转移及即时奖励的情况下，直接从经历过的<strong>完整Episode</strong>来学习状态价值，通常情况下<strong>某状态的价值等于在多个Episode中以该状态算得到的所有收益的平均</strong>。</p><p><strong>目标：</strong>在给定策略下，从一系列的完整Episode经历中学习得到该策略下的状态价值函数。</p><p><strong>蒙特卡洛强化学习的特点</strong>：不基于模型本身，直接从经历过的Episode中学习，必须是<strong>完整的Episode</strong></p><p><strong>蒙特卡洛强化学习使用的思想</strong>就是<strong>使用平均收获值代替价值</strong>。理论上Episode越多，结果越准确。</p><blockquote><p><strong>Episode</strong></p><p>episode就是经历，每条episode就是一条从起始状态到结束状态的经历。例如在走迷宫，一条episode就是从你开始进入迷宫，到最后走出迷宫的路径。</p><p>首先我们要得到的是某一个状态$s$的平均收获。所以我们说的episode要经过状态$s$。所以下图中第二条路径没有经过状态s，对于s来说就不能使用它了。而且最后我们episode都是要求达到终点的，才能算是一个episode。</p><img title="This is an example image" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/monte_carlo.png" class="lazyload"><p>Episode其包含的信息有：状态的转移、使用的行为序列、中间状态获得的即时奖励以及到达终止状态时获得的即时奖励。</p><p>完整的Episode 指必须从某一个状态开始，Agent与Environment交互直到终止状态，环境给出终止状态的即时收获为止。</p><p>完整的Episode不要求起始状态一定是某一个特定的状态，但是要求个体最终进入环境认可的某一个终止状态。</p></blockquote><p>数学描述如下：</p><ul><li>基于特定策略 $\pi$  的一个Episode信息可以表示为如下的一个序列：</li></ul><script type="math/tex; mode=display">S _ { 1 } , A _ { 1 } , R _ { 2 } , S _ { 2 } , A _ { 2 } , \ldots , S _ { t } , A _ { t } , R _ { t + 1 } , \ldots , S _ { k } \sim \pi</script><ul><li>$t$时刻状态 $S_t$ 的收益：</li></ul><script type="math/tex; mode=display">G _ { t } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { T - 1 } R _ { T }</script><p>其中 $T$ 为终止时刻。</p><ul><li>该策略下某一状态 $s$ 的价值：</li></ul><script type="math/tex; mode=display">v _ { \pi } ( s ) = E _ { \pi } \left[ G _ { t } | S _ { t } = s \right]</script><h4 id="重复状态的收获计算"><a href="#重复状态的收获计算" class="headerlink" title="重复状态的收获计算"></a>重复状态的收获计算</h4><p>在状态转移过程中，可能发生一个状态经过一定的转移后又一次或多次返回该状态，此时在一个Episode里如何计算这个状态发生的次数和计算该Episode的收获呢？可以有如下两种方法：</p><h5 id="首次访问蒙特卡洛策略评估-First-Visit-Monte-Carlo-Policy-Evaluation"><a href="#首次访问蒙特卡洛策略评估-First-Visit-Monte-Carlo-Policy-Evaluation" class="headerlink" title="首次访问蒙特卡洛策略评估(First-Visit Monte-Carlo Policy Evaluation)"></a>首次访问蒙特卡洛策略评估(First-Visit Monte-Carlo Policy Evaluation)</h5><p>在给定一个策略，使用一系列完整Episode评估某一个状态$s$时，对于每一个Episode，仅当该状态<strong>第一次</strong>出现时列入计算</p><p>状态出现的次数加1： </p><script type="math/tex; mode=display">N ( s ) \leftarrow N ( s ) + 1</script><p>总的收获值更新：</p><script type="math/tex; mode=display">S ( s ) \leftarrow S ( s ) + G_t</script><p>状态s的价值：</p><script type="math/tex; mode=display">V(s) = \frac {S(s)}{N(s)}</script><p>当$N ( s ) \rightarrow \infty $ 时，$V(s) \leftarrow v_{\pi}(s)$ </p><img title="This is an example image" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/First_visit_mc.jpg" class="lazyload"><h5 id="每次访问蒙特卡洛策略评估"><a href="#每次访问蒙特卡洛策略评估" class="headerlink" title="每次访问蒙特卡洛策略评估"></a>每次访问蒙特卡洛策略评估</h5><p>在给定一个策略，使用一系列完整Episode评估某一个状态$s$时，对于每一个Episode，状态s<strong>每次</strong>出现在状态转移链时，计算的具体公式与上面的一样，但具体意义不一样。</p><p>状态出现的次数加1： </p><script type="math/tex; mode=display">N ( s ) \leftarrow N ( s ) + 1</script><p>总的收获值更新：</p><script type="math/tex; mode=display">S ( s ) \leftarrow S ( s ) + G_t</script><p>状态s的价值：</p><script type="math/tex; mode=display">V(s) = \frac {S(s)}{N(s)}</script><p>当$N ( s ) \rightarrow \infty $ 时，$V(s) \leftarrow v_{\pi}(s)$ </p><h4 id="示例：二十一点游戏-Blackjack-Example"><a href="#示例：二十一点游戏-Blackjack-Example" class="headerlink" title="　示例：二十一点游戏 Blackjack Example"></a>　示例：二十一点游戏 Blackjack Example</h4><p>该示例解释了Model-Free下的策略评估问题和结果，没有说具体的学习过程。</p><p>游戏规则：你会得到一副手牌，一开始是两张，你需要得到尽量靠近21点但不能超过21点的手牌点数和。越接近21点越有可能打赢庄家，打赢庄家就算赢。庄家会亮一张牌给玩家看</p><p><strong>状态空间</strong>：（多达200种，根据对状态的定义可以有不同的状态空间，这里采用的定义是牌的分数，不包括牌型）</p><ul><li>当前牌的分数（12 - 21），低于12时，你可以安全的再叫牌，所以没意义。</li><li>庄家出示的牌（A - 10），庄家会显示一张牌面给玩家</li><li>我有“useable” ace吗？（是或否）A既可以当1点也可以当11点。</li></ul><p><strong>行为空间</strong>：</p><ul><li>停止要牌 stick</li><li>继续要牌 twist</li></ul><p><strong>奖励（停止要牌）</strong>：</p><ul><li>+1：如果你的牌分数大于庄家分数</li><li>0： 如果两者分数相同</li><li>-1：如果你的牌分数小于庄家分数</li></ul><p><strong>奖励（继续要牌）</strong>：</p><ul><li>-1：如果牌的分数&gt;21，并且进入终止状态</li><li>0：其它情况</li></ul><p><strong>状态转换（Transitions）</strong>：如果牌分小于12时，自动要牌</p><p><strong>当前策略</strong>：牌分只要小于20就继续要牌。</p><p><strong>问题：</strong>评估该策略的好坏。</p><p><strong>评估过程：</strong>使用庄家显示的牌面值、玩家当前牌面总分值来确定一个二维状态空间，区分手中有无A分别处理。统计每一牌局下决定状态的庄家和玩家牌面的状态数据，同时计算其最终收获。通过模拟多次牌局，计算每一个状态下的平均值，得到如下图示。</p><p><strong>最终结果：</strong>无论玩家手中是否有A牌，该策略在绝大多数情况下各状态价值都较低，只有在玩家拿到21分时状态价值有一个明显的提升。</p><img title="This is an example image" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/monte_carlo_example.png" class="lazyload"><h3 id="蒙特卡洛累进更新"><a href="#蒙特卡洛累进更新" class="headerlink" title="蒙特卡洛累进更新"></a>蒙特卡洛累进更新</h3><p>在使用蒙特卡洛方法求解平均收获时，需要计算平均值。通常计算平均值要预先存储所有的数据，最后使用总和除以此次数。这里介绍了一种更简单实用的方法：</p><p><strong>累进更新平均值 Incremental Mean</strong></p><p>这里提到了在实际操作时常用的一个实时更新均值的办法，使得在计算平均收获时不需要存储所有既往收获，而是每得到一次收获，就计算其平均收获。</p><p>理论公式如下：</p><script type="math/tex; mode=display">\begin{aligned} \mu _ { k } & = \frac { 1 } { k } \sum _ { j = 1 } ^ { k } x _ { j } \\ & = \frac { 1 } { k } \left( x _ { k } + \sum _ { j = 1 } ^ { k - 1 } x _ { j } \right) \\ & = \frac { 1 } { k } \left( x _ { k } + ( k - 1 ) \mu _ { k - 1 } \right) \\ & = \mu _ { k - 1 } + \frac { 1 } { k } \left( x _ { k } - \mu _ { k - 1 } \right) \end{aligned}</script><p>把这个方法应用于蒙特卡洛策略评估，就得到蒙特卡洛累进更新。</p><p><strong>蒙特卡洛累进更新</strong></p><p>对于一系列Episodes中的每一个：</p><script type="math/tex; mode=display">S _ { 1 } , A _ { 1 } , R _ { 2 } , S _ { 2 } , A _ { 2 } , \ldots , S _ { t } , A _ { t } , R _ { t + 1 } , \ldots , S _ { k }</script><p>对于Episode里的每一个状态 $S_t$ ，有一个收获 $G_t$ ，每碰到一次 $S_t$ ，使用下式计算状态的平均价值 $V(S_t)$ ：</p><script type="math/tex; mode=display">V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \frac { 1 } { N \left( S _ { t } \right) } \left( G _ { t } - V \left( S _ { t } \right) \right)</script><p>其中：</p><script type="math/tex; mode=display">N ( S_t ) \leftarrow N ( S_t ) + 1</script><p>在处理非静态问题时，使用这个方法跟踪一个实时更新的平均值是非常有用的，可以扔掉那些已经计算过的Episode信息。此时可以引入参数 $\alpha$  来更新状态价值：</p><script type="math/tex; mode=display">V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( G _ { t } - V \left( S _ { t } \right) \right)</script><p>以上就是蒙特卡洛学习方法的主要思想和描述，由于蒙特卡洛学习方法有许多缺点，因此实际应用并不多。接下来介绍实际常用的TD学习方法。</p><h2 id="时序差分学习-Temporal-Difference-Learning（TD-0-）"><a href="#时序差分学习-Temporal-Difference-Learning（TD-0-）" class="headerlink" title="时序差分学习 Temporal-Difference Learning（TD(0)）"></a>时序差分学习 Temporal-Difference Learning（TD(0)）</h2><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>时序差分算法简称TD学习，是一种无模型的强化学习算法。它继承了动态规划和蒙特卡罗方法的优点，从而对状态值和策略进行预测。从本质上来说，时序差分算法和动态规划一样，是一种<strong>bootstrapping的算法</strong>。同时，也和蒙特卡罗方法一样，从Episode学习，但是它可以学习<strong>不完整</strong>的Episode，是一种<strong>无模型的强化学习算法</strong>，其原理也是基于了试验。</p><p>在Monte-Carlo学习中，使用实际的收获（return）$G_t$  来更新价值（Value）：</p><script type="math/tex; mode=display">V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( G _ { t } - V \left( S _ { t } \right) \right)</script><p>其中$G _ { t } - V \left( S _ { t } \right)$是实际收获与预估收货的<strong>误差</strong></p><p>在TD学习中，算法在估计某一个状态的价值时，用的是离开该状态的即刻奖励 $R_{t+1}$  与下一状态 $S_{t+1}$  的预估状态价值乘以衰减系数($\gamma$)组成，这符合Bellman方程的描述：</p><script type="math/tex; mode=display">V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right) - V \left( S _ { t } \right) \right)</script><p>$R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right)$ 称为<strong>TD目标值</strong>，是对实际收获的估计，即代替了MC中的$G_t$<br>$\delta _ { t } = R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right) - V \left( S _ { t } \right)$称为TD<strong>误差</strong></p><blockquote><p>回顾：Bellman期望方程（见强化学习基础——强化学习问题描述）</p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \mathbb { E } _ { \pi } \left[ R _ { t + 1 } + \gamma v _ { \pi } \left( S _ { t + 1 } \right) | S _ { t } = s \right]</script></blockquote><p><strong>BootStrapping</strong> 指的就是TD目标值 $R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right)$  代替收获 $G_t$  的过程</p><h3 id="MC与TD的区别"><a href="#MC与TD的区别" class="headerlink" title="MC与TD的区别"></a>MC与TD的区别</h3><h4 id="区别一：是否需要完整Episode"><a href="#区别一：是否需要完整Episode" class="headerlink" title="区别一：是否需要完整Episode"></a>区别一：是否需要完整Episode</h4><ul><li><p>TD 在知道结果之前可以学习，MC必须等到最后结果才能学习；</p></li><li><p>TD 可以在没有结果时学习，可以在持续进行的环境里学习。</p></li></ul><p><strong>示例——驾车返家</strong></p><p>想象一下你下班后开车回家，需要预估整个行程花费的时间。TD算法相当于在整个返家的过程中（一个Episode），根据已经消耗的时间和预期还需要的时间来不断更新最终回家需要消耗的时间。</p><img title="区别一例：TD不需要整个Episode完成才更新" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td_example1.png" class="lazyload"><p>基于上表所示的数据，下图展示了蒙特卡洛学习和TD学习两种不同的学习策略来<strong>更新</strong>价值函数（各个状态的价值）。这里使用的是<strong>从某个状态预估的到家还需耗时</strong>来<strong>间接</strong>反映某状态的价值：某位置预估的到家时间越长，该位置价值越低，在优化决策时需要避免进入该状态。</p><img title="区别一例：TD不需要整个Episode完成才更新" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td_example2.png" class="lazyload"><p>对于蒙特卡洛学习过程，驾驶员在路面上碰到各种情况时，他不会更新对于回家的预估时间，等他回到家得到了真实回家耗时后，他会重新估计在返家的路上着每一个主要节点状态到家的时间，在下一次返家的时候用新估计的时间来帮助决策；</p><p>对于TD学习，在一开始离开办公室的时候你可能会预估总耗时30分钟，但是当你取到车发现下雨的时候，你会立刻想到原来的预计过于乐观，因为既往的经验告诉你下雨会延长你的返家总时间，此时你会更新目前的状态价值估计，从原来的30分钟提高到40分钟。同样当你驾车离开高速公路时，会一路根据当前的状态（位置、路况等）对应的预估返家剩余时间，直到返回家门得到实际的返家总耗时。这一过程中，你会根据状态的变化实时更新该状态的价值。</p><h4 id="区别二：偏差方差"><a href="#区别二：偏差方差" class="headerlink" title="区别二：偏差方差"></a>区别二：偏差方差</h4><p>MC $G_t$ ：实际收获，是基于某一策略状态价值的<strong>无偏</strong>估计</p><script type="math/tex; mode=display">G _ { t } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { T - 1 } R _ { T }</script><p>TD target：TD目标值，是基于下一状态<strong>预估</strong>价值计算的当前预估收获，是当前状态实际价值的<strong>有偏</strong>估计</p><script type="math/tex; mode=display">R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right)</script><p>True TD target： 真实TD目标值，是基于下一状态的<strong>实际</strong>价值对当前状态实际价值的无偏估计</p><script type="math/tex; mode=display">R _ { t + 1 } + \gamma v _ { \pi } \left( S _ { t + 1 } \right)</script><ul><li>MC 没有偏差（bias），但有着较高的方差（Variance），且对初始值不敏感；</li><li>TD 低方差, 但有一定程度的偏差，对初始值较敏感，通常比 MC 更高效；</li><li>因为MC有T步的环境噪音，而TD只有一步，所以MC的方差高</li></ul><blockquote><p><strong>偏差</strong>指的是距离期望的距离，预估的平均值与实际平均值的偏离程度；</p><p><strong>方差</strong>是指评估单次采样结果相对于与平均值变动的范围大小</p><p>基本就是统计学上均值与方差的概念。</p></blockquote><p><strong>示例——随机行走</strong></p><img title="example" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td_example3.png" class="lazyload"><p><strong>状态空间</strong>：如下图：A、B、C、D、E为中间状态，C同时作为起始状态。灰色方格表示终止状态；</p><p><strong>行为空间</strong>：除终止状态外，任一状态可以选择向左、向右两个行为之一；</p><p><strong>即时奖励：</strong>右侧的终止状态得到即时奖励为1，左侧终止状态得到的即时奖励为0，在其他状态间转化得到的即时奖励是0；</p><p><strong>状态转移</strong>：100%按行为进行状态转移，进入终止状态即终止；</p><p><strong>衰减系数：</strong>1；</p><p><strong>给定的策略</strong>：随机选择向左、向右两个行为。</p><p><strong>问题：</strong>评估随机行走这个策略的价值，也就是计算该策略下每个状态的价值，也就是确定该MDP问题的状态价值函数。</p><p><strong>求解：</strong>下图是使用TD算法得到的结果。横坐标显示的是状态，纵坐标是各状态的价值估计，一共5条折线，数字表明的是实际经历的Episode数量，true value所指的那根折线反映的是各状态的实际价值。第0次时，各状态的价值被初始化为0.5，经过1次、10次、100次后得到的价值函数越来越接近实际状态价值函数。</p><img title="example" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td_example4.png" class="lazyload"><p>下图比较了MC和TD算法的效率。横坐标是经历的Episode数量，纵坐标是计算得到的状态函数和实际状态函数下各状态价值的均方差。黑色是MC算法在不同step-size下的学习曲线，灰色的曲线使用TD算法。可以看出TD较MC更高效。此图还可以看出当step-size不是非常小的情况下，TD有可能得不到最终的实际价值，将会在某一区间震荡。</p><img title="example" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td_example5.png" class="lazyload"> <h4 id="区别三：当Episode有限时"><a href="#区别三：当Episode有限时" class="headerlink" title="区别三：当Episode有限时"></a>区别三：当Episode有限时</h4><p>当Episode有限时（比如只有三个），两种算法的区别：</p><ul><li><p>MC算法试图收敛至一个能够最小化状态价值与实际收获的均方差的解决方案，这一均方差用公式表示为：</p><script type="math/tex; mode=display"> \sum _ { k = 1 } ^ { K } \sum _ { t = 1 } ^ { T _ { k } } \left( G _ { t } ^ { k } - V \left( s _ { t } ^ { k } \right) \right) ^ { 2 }</script><p> 式中， $k$ 表示的是Episode序号， $K$ 为总的Episode数量， $t$ 为一个Episode内状态序号（第1,2,3…个状态等），$T_k$ 表示的是第 $k$ 个Episode总的状态数， $G_t^k$ 表示第 $k$ 个Episode里时 $t$ 刻状态  $S_t$ 获得的最终收获，$V(S_t^k)$ 表示的是第 $k$ 个Episode里算法估计的 $t$ 时刻状态  $S_t$ 的价值。</p></li><li><p>TD算法则收敛至一个根据已有经验构建的最大可能的马尔科夫模型的状态价值，也就是说TD算法将首先根据已有经验估计状态间的转移概率：</p><script type="math/tex; mode=display"> \hat { P } _ { s , s ^ { \prime } } ^ { a } = \frac { 1 } { N ( s , a ) } \sum _ { k = 1 } ^ { K } \sum _ { t = 1 } ^ { T _ { k } } \mathbf { 1 } \left( s _ { t } ^ { k } , a _ { t } ^ { k } , s _ { t + 1 } ^ { k } = s , a , s ^ { \prime } \right)</script><p> 同时估计某一个状态的即时奖励：</p><script type="math/tex; mode=display"> \hat { \mathcal { R } } _ { s } ^ { a } = \frac { 1 } { N ( s , a ) } \sum _ { k = 1 } ^ { K } \sum _ { t = 1 } ^ { T _ { k } } \mathbf { 1 } \left( s _ { t } ^ { k } , a _ { t } ^ { k } = s , a \right) r _ { t } ^ { k }</script><p> 最后计算该MDP的状态函数。</p></li></ul><p><strong>示例——AB</strong></p><p><strong>已知：</strong>现有两个状态(A和B)，MDP未知，衰减系数为1，有如下表所示8个完整Episode的经验及对应的即时奖励，其中除了第1个Episode有状态转移外，其余7个均只有一个状态。</p><img title="example" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td_example6.png" class="lazyload"> <p><strong>问题：依据仅有的Episode，计算状态A，B的价值分别是多少，即</strong>V(A)=？， V(B)=？</p><p><strong>答案：</strong>V(B) = 6/8，V(A)根据不同算法结果不同，用MC算法结果为0，TD则得出6/8。</p><p><strong>解释：</strong></p><p>应用MC算法：由于需要完整的Episode,因此仅Episode1可以用来计算A的状态价值，很明显是0；同时B的价值是6/8。</p><p>应用TD算法：TD算法试图利用现有的Episode经验构建一个MDP（如下图），由于存在一个Episode使得状态A有后继状态B，因此状态A的价值是通过状态B的价值来计算的，同时经验表明A到B的转移概率是100%，且A状态的即时奖励是0，并且没有衰减，因此A的状态价值等于B的状态价值。</p><img title="example" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td_example7.png" class="lazyload"> <h4 id="区别四：Markov环境的表现"><a href="#区别四：Markov环境的表现" class="headerlink" title="区别四：Markov环境的表现"></a>区别四：Markov环境的表现</h4><ul><li>TD算法使用了MDP问题的马尔科夫性，在Markov 环境下更有效</li><li>但是MC算法并不利用马尔科夫性，通常在非Markov环境下更有效。</li></ul><h3 id="小结——MC-TD-DP"><a href="#小结——MC-TD-DP" class="headerlink" title="小结——MC, TD ,DP"></a>小结——MC, TD ,DP</h3><div class="table-container"><table><thead><tr><th></th><th>动态规划</th><th>蒙特卡洛</th><th>时序差分</th></tr></thead><tbody><tr><td>是否需要知道Model</td><td>是</td><td>否</td><td>否</td></tr><tr><td>是否是Bootstrap算法</td><td>是</td><td>否</td><td>是</td></tr><tr><td>是否用样本来计算</td><td>否（利用模型直接得到V）</td><td>是</td><td>是</td></tr><tr><td>是否需要完整的Episode</td><td>不基于采样</td><td>是</td><td>否</td></tr><tr><td>适用环境</td><td></td><td>非Markov环境<br>不适用Episode少的<br>不适用持续环境（无终结状态）</td><td>Markov环境<br></td></tr><tr><td>backup图</td><td><img title="时序差分算法backup图" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td2.png" class="lazyload"><br>用实际收获更新状态预估价值</td><td><img title="蒙特卡洛方法backup图" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td1.png" class="lazyload"><br>用喜爱状态的预估状态价值预估收获再更新预估价值</td><td><img title="动态规划算法backup图" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td3.png" class="lazyload"><br>根据完整模型，依靠预估数据更新状态价值</td></tr></tbody></table></div><img title="总对比图" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/td4.png" class="lazyload"><p>上图从两个维度解释了四种算法的差别，多了一个穷举法。这两个维度分别是：采样深度和广度。当使用单个采样，同时不走完整个Episode就是TD；当使用单个采样但走完整个Episode就是MC；当考虑全部样本可能性，但对每一个样本并不走完整个Episode时，就是DP；当既考虑所有Episode又把Episode从开始到终止遍历完，就变成了穷举法。</p><p>需要提及的是：DP利用的是整个MDP问题的模型，也就是状态转移概率，虽然它并不实际利用样本，但是它利用了整个模型的规律，因此认为是Full Width的。</p><h2 id="时序差分学习-Temporal-Difference-Learning-（TD-λ-）"><a href="#时序差分学习-Temporal-Difference-Learning-（TD-λ-）" class="headerlink" title="时序差分学习 Temporal-Difference Learning （TD(λ)）"></a>时序差分学习 Temporal-Difference Learning （TD(λ)）</h2><p>先前介绍了TD(0)算法，括号内的数字0表示的是在当前状态下往前多看1步，要是往前多看2步更新状态价值会怎样？这就引入了n-step的概念。</p><h3 id="n步预测-n-Step-Prediction"><a href="#n步预测-n-Step-Prediction" class="headerlink" title="n步预测 n-Step Prediction"></a>n步预测 n-Step Prediction</h3><p>所谓的n-step TD，其实就是说要往前多少步再来估计V值。如果往前n步直到终点，那么就等价于蒙特卡罗方法了。</p><img title="n步预测图" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/tdn1.png" class="lazyload"><p>注：图中空心大圆圈表示状态，实心小圆圈表示行为</p><h3 id="n-步收获"><a href="#n-步收获" class="headerlink" title="n-步收获"></a>n-步收获</h3><p>TD(0)是基于1-步预测的，MC则是基于$\infty$步预测的：</p><img title="n步收货" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/tdn2.png" class="lazyload"><p>注意：n=2时不写成TD(2)。</p><p><strong>n-步收获</strong>定义为：</p><script type="math/tex; mode=display">G _ { t } ^ { ( n ) } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { n - 1 } R _ { t + n } + \gamma ^ { n } V \left( S _ { t + n } \right)</script><p>同样TD target 也由2部分组成，已走的步数使用确定的即时reward，剩下的使用估计的状态价值替代，n步TD学习状态价值函数的更新公式为：</p><script type="math/tex; mode=display">V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( G _ { t } ^ { ( n ) } - V \left( S _ { t } \right) \right)</script><h3 id="λ收获"><a href="#λ收获" class="headerlink" title="λ收获"></a>λ收获</h3><p>既然存在n-步预测，那么n=？时效果最好呢，下面的例子试图回答这个问题：</p><h4 id="实验——n步预测的n取什么效果最好"><a href="#实验——n步预测的n取什么效果最好" class="headerlink" title="实验——n步预测的n取什么效果最好"></a>实验——n步预测的n取什么效果最好</h4><p>这个示例研究了使用多个不同步数预测联合不同步长(step-size，公式里的系数α）时，分别在在线和离线状态时状态函数均方差的差别。所有研究使用了10个Episode。离线与在线的区别在于，离线是在经历所有10个Episode后进行状态价值更新；而在线则至多经历一个Episode就更新依次状态价值。</p><img title="n步实验" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/tdn2.png" class="lazyload"><p>结果如图表明，离线和在线之间曲线的形态差别不明显；从步数上来看，步数越长，越接近MC算法，均方差越大。对于这个大规模随机行走示例，在线计算比较好的步数是3-5步，离线计算比较好的是6-8步。但是不同的问题其对应的比较高效的步数不是一成不变的。因此选择多少步数作为一个较优的计算参数也是一个问题。</p><h4 id="λ收获定义"><a href="#λ收获定义" class="headerlink" title="λ收获定义"></a>λ收获定义</h4><p>这里我们引入了一个新的参数：λ。通过引入这个新的参数，可以做到在不增加计算复杂度的情况下综合考虑所有步数的预测。这就是<strong>λ预测</strong>和<strong>λ收获。</strong></p><p>λ-收获 $G_t^{\lambda}$ 综合考虑了从 $1$ 到 $\infty$ 的所有步收获，它给其中的任意一个$n-1$ 步收获施加一定的权重$(1-\lambda)\lambda^{n-1}$ 。通过这样的权重设计，得到如下的公式：</p><script type="math/tex; mode=display">G _ { \mathrm { t } } ^ { \lambda } = ( 1 - \lambda ) \sum _ { n = 1 } ^ { \infty } \lambda ^ { n - 1 } G _ { t } ^ { ( n ) }</script><p>对应的λ-预测写成TD(λ):</p><script type="math/tex; mode=display">v \left( S _ { t } \right) \leftarrow v \left( S _ { t } \right) + \alpha \left( G _ { t } ^ { \lambda } - V \left( S _ { t } \right) \right)</script><p>下图是各步收获的权重分配图，图中最后一列λ的指数是$T-t-1$  。$T$ 为终止状态的时刻步数，$t$  为当前状态的时刻步数，所有的权重加起来为1。</p><img title="n步实验" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/tdn3.png" class="lazyload"><ul><li><strong>TD(λ)对于权重分配的图解</strong></li></ul><img title="n步实验" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/tdn4.png" class="lazyload"><p>这张图还是比较好理解，例如对于n=3的3-步收获，赋予其在 λ 收获中的权重如左侧阴影部分面积，对于终止状态的T-步收获，T以后的<strong>所有</strong>阴影部分面积。而所有节段面积之和为1。这种几何级数的设计也考虑了算法实现的计算方便性。</p><p>经过这个$(1-\lambda)\lambda^{n-1}$的作用，各个步数的权重就像上图这样衰减。相当于离状态s越远的，权重就越小。这也符合我们一般的想法，离得远的作用就小</p><h3 id="两个方向理解TD-λ"><a href="#两个方向理解TD-λ" class="headerlink" title="两个方向理解TD(λ)"></a>两个方向理解TD(λ)</h3><p>TD((λ)的设计使得Episode中，后一个状态的状态价值与之前所有状态的状态价值有关，同时也可以说成是一个状态价值参与决定了后续所有状态的状态价值。但是每个状态的价值对于后续状态价值的影响权重是不同的。我们可以从两个方向来理解TD(λ)：</p><h4 id="前向认识TD-λ"><a href="#前向认识TD-λ" class="headerlink" title="前向认识TD(λ)"></a>前向认识TD(λ)</h4><p>引入了λ之后，会发现要更新一个状态的状态价值，必须要走完整个Episode获得每一个状态的即时奖励以及最终状态获得的即时奖励。这和MC算法的要求一样，因此TD(λ)算法有着和MC方法一样的劣势。λ取值区间为[0,1]，当λ=1时对应的就是MC算法。这给实际计算带来了不便。</p><img title="n步实验" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/tdn_forward.png" class="lazyload"><p>前向视角的解释：假设一个人坐在状态流上拿着望远镜看向前方，前方是那些将来的状态。当估计当前状态的值函数时，从$TD(λ)$的定义中可以看到，它需要用到将来时刻的值函数。</p><script type="math/tex; mode=display">\begin{array} { c } { V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( G _ { t } ^ { \lambda } - V \left( S _ { t } \right) \right) } \\ { G _ { t } ^ { \lambda } = ( 1 - \lambda ) \sum _ { n = 1 } ^ { \infty } \lambda ^ { n - 1 } G _ { t } ^ { ( n ) } } \\ G _ { t } ^ { ( n ) } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { n - 1 } R _ { t + n } + \gamma ^ { n } V \left( S _ { t + n } \right)\end{array}</script><h4 id="反向认识TD-λ-（重要）"><a href="#反向认识TD-λ-（重要）" class="headerlink" title="反向认识TD(λ)（重要）"></a>反向认识TD(λ)（重要）</h4><p>TD(λ)从另一方面提供了一个单步更新的机制，通过下面的示例来说明。</p><p><strong>示例——被电击的原因</strong></p><p>老鼠在连续接受了3次响铃和1次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？</p><img title="n步实验" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/tdn5.png" class="lazyload"><p>两个概念：</p><p><strong>频率启发 Frequency heuristic：</strong>将原因归因于出现频率最高的状态</p><p><strong>就近启发 Recency heuristic：</strong>将原因归因于较近的几次状态</p><p>给每一个状态引入一个数值：<strong>效用追踪</strong>（<strong>Eligibility Traces, ES</strong>），可以结合上述两个启发。定义：</p><script type="math/tex; mode=display">\begin{array} { c } { E _ { 0 } ( s ) = 0 } \\ { E _ { t } ( s ) = \gamma \lambda E _ { t - 1 } ( s ) + 1 \left( S _ { t } = s \right) } \end{array}</script><p>其中 $1 \left( S _ { t } = s \right)$ 是一个条件判断表达式。</p><p>下图给出了 $E_t(s)$  对于 $t$  的一个可能的曲线图：</p><img title="n步实验" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/tdn6.png" class="lazyload"><p>该图横坐标是时间，横坐标下有竖线的位置代表当前进入了状态 $s$  ，纵坐标是效用追踪值 $E$  。可以看出当某一状态连续出现，E值会在一定衰减的基础上有一个单位数值的提高，此时将增加该状态对于最终收获贡献的比重，因而在更新该状态价值的时候可以较多地考虑最终收获的影响。同时如果该状态距离最终状态较远，则其对最终收获的贡献越小，在更新该状态时也不需要太多的考虑最终收获。</p><p>特别的，$E$  值并不需要等到完整的Episode结束才能计算出来，它可以每经过一个时刻就得到更新。$E$  值存在饱和现象，有一个瞬时最高上限：$E_{max}=\frac {1}{1-\gamma\lambda}$ </p><img title="n步实验" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/tdn_backward.png" class="lazyload"><p>$TD(λ)$的后向视角解释：有个人坐在状态流上，手里拿着话筒，面朝着已经经历过的状态，获得当前回报并利用下一个状态的值函数得到TD偏差之后，此人会想已经经历过的状态喊话告诉这些已经经历过的状态处的值函数需要利用当前时刻的TD偏差进行更新。此时过往的每个状态值函数更新的大小应该跟距离当前状态的步数有关。假设当前状态为$s_t$，TD偏差为$\delta _ { t }$，那么$s_{t−1}$处的值函数更新应该乘以一个衰减因子$\gamma\lambda$，状态$s_{t−2}$处的值函数更新应该乘以$(\gamma\lambda)^2$，以此类推。</p><p>后向TD(λ)的更新过程：</p><script type="math/tex; mode=display">\delta _ { t } = R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right) - V \left( S _ { t } \right)</script><p>计算当前状态的TD偏差，<strong>效用追踪</strong>（<strong>Eligibility Traces, ES</strong>）：</p><script type="math/tex; mode=display">E _ { t } ( s ) = \gamma \lambda E _ { t - 1 } ( s ) + 1 \left( S _ { t } = s \right)</script><p>引入效能主要是为了可以在线学习，可以方便的迭代</p><p>更新eligibility trace（对状态空间中的每一个已经经历过的状态s，更新价值函数）</p><script type="math/tex; mode=display">V ( s ) \leftarrow V ( s ) + \alpha \delta _ { t } E _ { t } ( s )</script><p>前向误差和反向误差一样，有证明的，不要看着好像不一样就觉得是两个公式</p><p>相较于MC算法，TD算法应用更广，是一个非常有用的强化学习方法，在下一讲讲解控制相关的算法时会详细介绍TD算法的实现。</p><h1 id="策略求解（无模型的控制）"><a href="#策略求解（无模型的控制）" class="headerlink" title="策略求解（无模型的控制）"></a>策略求解（无模型的控制）</h1><p>上一节主要讲解了在模型未知的情况下如何进行预测。所谓的预测就是评估一个给定的策略，也就是确定一给定策略下的状态（或状态行为对）的价值函数。这一讲的内容主要是<strong>在模型未知的条件下如何优化价值函数</strong>，这一过程也称作模型无关的控制。</p><p>现实中有很多此类的例子，比如控制一个大厦内的多个电梯使得效率最高；控制直升机的特技飞行，机器人足球世界杯上控制机器人球员，围棋游戏等等。所有的这些问题要么我们对其模型运行机制未知；要么是虽然问题模型是已知的，但问题的规模太大以至于计算机无法高效的计算，除非使用采样的办法。本节的内容就专注于解决这些问题。</p><p>根据优化控制过程中是否利用已有或他人的经验策略来改进我们自身的控制策略，我们可以将这种优化控制分为两类：</p><ul><li><p><strong>现时策略学习（On-policy Learning）</strong>，其基本思想是个体已有一个策略，并且遵循这个策略进行采样，或者说采取一系列该策略下产生的行为，根据这一系列行为得到的奖励，更新状态函数，最后根据该更新的价值函数来优化策略得到较优的策略。</p></li><li><p><strong>离线策略学习（Off-policy Learning）</strong>: 其基本思想是，虽然个体有一个自己的策略，但是个体并不针对这个策略进行采样，而是基于另一个策略进行采样，这另一个策略可以是先前学习到的策略，也可以是人类的策略等一些较为优化成熟的策略，通过观察基于这类策略的行为，或者说通过对这类策略进行采样，得到这类策略下的各种行为，继而得到一些奖励，然后更新价值函数，即在自己的策略形成的价值函数的基础上观察别的策略产生的行为，以此达到学习的目的。这种学习方式类似于“站在别人的肩膀上可以看得更远”。</p></li></ul><h2 id="通用策略迭代"><a href="#通用策略迭代" class="headerlink" title="通用策略迭代"></a>通用策略迭代</h2><img title="通用策略迭代" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_1.png" class="lazyload"><p><strong>通用策略迭代的核心</strong>是在两个交替的过程之间进行策略优化。一个过程是策略评估，另一个是改善策略。</p><p>如上图的三角形区域所示，从一个策略π和一个价值函数Ｖ开始，每一次箭头向上代表着利用当前策略进行价值函数的更新，每一次箭头向下代表着根据更新的价值函数贪婪地选择新的策略，说它是贪婪的，是因为每次都采取转移到可能的、状态函数最高的新状态的行为。最终将收敛至最优策略和最优价值函数。</p><h2 id="现时策略学习"><a href="#现时策略学习" class="headerlink" title="现时策略学习"></a>现时策略学习</h2><p>现时策略学习的特点就是当前遵循的策略就是个体学习改善的策略。</p><p>根据是否经历完整的Episode可以将其分为基于蒙特卡洛的和基于TD的。</p><h2 id="现时策略蒙特卡洛控制"><a href="#现时策略蒙特卡洛控制" class="headerlink" title="现时策略蒙特卡洛控制"></a>现时策略蒙特卡洛控制</h2><p>根据通用策略迭代的步骤是评估策略和改善策略交替进行，下面对两个步骤进行讨论</p><h3 id="评估策略"><a href="#评估策略" class="headerlink" title="评估策略"></a>评估策略</h3><p>动态规划算法来改善策略是需要知道某一状态的所有后续状态及状态间转移概率：</p><script type="math/tex; mode=display">\pi ^ { \prime } ( s ) = \underset { a \in \mathcal { A } } { \operatorname { argmax } } \mathcal { R } _ { s } ^ { a } + \mathcal { P } _ { s s ^ { \prime } } ^ { a } V \left( s ^ { \prime } \right)</script><p>但这种方法不适用于模型未知的蒙特卡洛学习，因为不知道执行完某个动作后的转移概率和所有后续的状态,就不知道哪个行动会使价值最大。</p><p>可以使用状态-行为对下的价值$Q(s,a)$来代替状态价值 ，这样就可以改善策略而不需要知道整个模型：</p><script type="math/tex; mode=display">\pi ^ { \prime } ( s ) = \underset { a \in \mathcal { A } } { \operatorname { argmax } } Q ( s , a )</script><blockquote><p>Q函数复习</p><script type="math/tex; mode=display">q _ { \pi } ( s , a ) = \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s , A _ { t } = a \right]</script></blockquote><p>这样做的目的是可以改善策略而不用知道整个模型，只需要知道在某个状态下采取什么什么样的行为价值最大即可。具体是这样：我们从一个初始的 $Q$ 和策略 $\pi$ 开始，先根据这个策略更新每一个状态行为对的 $q$ 值，$s$ 随后基于更新的 $Q$确定改善的贪婪算法。</p><h3 id="改善策略"><a href="#改善策略" class="headerlink" title="改善策略"></a>改善策略</h3><p>即使这样，至少还存在一个问题，即当我们每次都使用贪婪算法来改善策略的时候，将很有可能由于没有足够的采样经验而导致产生一个并不是最优的策略，我们需要不时的尝试一些新的行为，这就是探索（Exploration）</p><p>完全使用贪婪算法改善策略可能会陷入局部最优，通常不能得到最优策略。为了解决这一问题，我们需要引入一个随机机制，以一定的概率选择当前最好的策略，同时给以其它可能的行为一定的几率，这就是Ɛ-贪婪探索。</p><h4 id="E-贪婪探索"><a href="#E-贪婪探索" class="headerlink" title="Ɛ-贪婪探索"></a>Ɛ-贪婪探索</h4><p>Ɛ-贪婪探索的目标使得某一状态下所有可能的行为都有一定非零几率被选中执行，也就保证了持续的探索，$1 - \epsilon$ 的概率下选择当前认为最好的行为，而 $\epsilon$ 的概率在所有可能的行为中选择（也包括那个当前最好的行为）。数学表达式如下：</p><script type="math/tex; mode=display">\pi ( a | s ) = \left\{ \begin{array} { l l } { \epsilon / m + 1 - \epsilon } & { \text { if } a ^ { * } = \underset { a \in \mathcal { A } } { \operatorname { argmax } } Q ( s , a ) } \\ { \epsilon / m } & { \text { otherwise } } \end{array} \right.</script><p><strong>定理：</strong>使用Ɛ-贪婪探索策略，对于任意一个给定的策略 $\pi$，我们在评估这个策略的同时也总在改善它（$V_{\pi’}(s) \geq V_{\pi}(s)$）</p><p>证明见David Silver视频</p><h3 id="蒙特卡洛控制-评估策略-改善策略"><a href="#蒙特卡洛控制-评估策略-改善策略" class="headerlink" title="蒙特卡洛控制=评估策略+改善策略"></a>蒙特卡洛控制=评估策略+改善策略</h3><p>解决了上述两个问题，我们最终看到蒙特卡洛控制的全貌：使用Ｑ函数进行策略评估，使用Ɛ-贪婪探索来改善策略。该方法最终可以收敛至最优策略。如下图所示：</p><img title="通用策略迭代" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_2.png" class="lazyload"><p>图中每一个向上或向下的箭头都对应着多个Episode。也就是说我们一般在经历了多个Episode之后才进行依次Ｑ函数更新或策略改善。实际上我们也可以在每经历一个Episode之后就更新Ｑ函数或改善策略。但不管使用哪种方式，在Ɛ-贪婪探索算法下我们始终只能得到基于某一策略下的近似Ｑ函数，且该算法没没有一个终止条件，因为它一直在进行探索。</p><p>因此我们必须关注以下两个方面：一方面我们不想丢掉任何更好信息和状态，另一方面随着我们策略的改善我们最终希望能终止于某一个最优策略，因为事实上最优策略不应该包括一些随机行为选择。为此引入了另一个理论概念：<strong>GLIE</strong>。</p><h3 id="基于GLIE的蒙特卡洛控制"><a href="#基于GLIE的蒙特卡洛控制" class="headerlink" title="基于GLIE的蒙特卡洛控制"></a>基于GLIE的蒙特卡洛控制</h3><h4 id="GLIE-Greedy-in-the-Limit-with-Infinite-Exploration"><a href="#GLIE-Greedy-in-the-Limit-with-Infinite-Exploration" class="headerlink" title="GLIE(Greedy in the Limit with Infinite Exploration)"></a>GLIE(Greedy in the Limit with Infinite Exploration)</h4><p><strong>GLIE</strong>(Greedy in the Limit with Infinite Exploration)，直白的说是<strong>在有限的时间内进行无限可能的探索</strong>。具体表现为：</p><ul><li><p>所有已经经历的状态行为对（state-action pair）会被无限次探索</p></li><li><p>策略会收敛于贪婪策略。即随着探索的无限延伸，贪婪算法中Ɛ值趋向于０。例如如果我们取 $\epsilon=\frac {1}{k}$ （$k$ 为探索的Episode数目），那么该Ɛ贪婪蒙特卡洛控制就具备GLIE特性。</p></li></ul><h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><ol><li><p>对于给定策略 $\pi$ ，采样第 $k$ 个Episode：$S _ { 1 } , A _ { 1 } , R _ { 2 } , \ldots , S _ { T } \sim \pi$ </p></li><li><p>对于该Episode里出现的每一个状态行为对 $S_t$ 和 $A_t$ 更新其计数和Ｑ函数：</p><script type="math/tex; mode=display">\begin{array} { l } { N \left( S _ { t } , A _ { t } \right) \leftarrow N \left( S _ { t } , A _ { t } \right) + 1 } \\ { Q \left( S _ { t } , A _ { t } \right) \leftarrow + \frac { 1 } { N \left( S _ { t } , A _ { t } \right) } \left( G _ { t } - Q \left( S _ { t } , A _ { t } \right) \right) } \end{array}</script></li><li><p>基于新的Ｑ函数改善以如下方式改善策略：</p><script type="math/tex; mode=display">\epsilon=\frac {1}{k} \\\pi \leftarrow greedy(Q)</script></li></ol><p><strong>定理：GLIE蒙特卡洛控制能收敛至最优的状态行为价值函数。</strong></p><h2 id="现时策略时序差分控制-On-Policy-Temporal-Difference-Control"><a href="#现时策略时序差分控制-On-Policy-Temporal-Difference-Control" class="headerlink" title="现时策略时序差分控制 On-Policy Temporal-Difference Control"></a>现时策略时序差分控制 On-Policy Temporal-Difference Control</h2><p>上一讲提到TD相比MC有很多优点：低方差，可以在线实时学习，可以学习不完整Episode等。因此很自然想到是否可以在控制问题上使用TD学习$Q(S,A)$而不是MC学习？这就是下文要讲解的SARSA</p><h3 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h3><p>SARSA的名称来源于下图所示的序列描述：针对一个状态$S$，以及一个特定的行为，$A$进而产生一个状态行为对($SA$)，与环境交互，环境收到个体的行为后会告诉个体即时奖励 $R$ 以及后续进入的状态 $S’$；接下来个体遵循<strong>现有策略</strong>产生一个行为 $A’$（不执行该行为），根据当前的<strong>状态行为价值函数</strong>得到后一个状态行为对($S’A’$)的价值（$Q$），利用这个 $Q$ 值更新前一个状态行为对( $SA$ )的价值。</p><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_3.png" class="lazyload"><p>与蒙特卡洛控制不同的时，每一个时间步，也就是在单个Episode内每一次个体在状态 $S_t$ 采取一个行为后都要更新 $Q$ 值，同样使用<strong>Ɛ-贪婪探索</strong>的形式来改善策略。</p><script type="math/tex; mode=display">Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma Q \left( S ^ { \prime } , A ^ { \prime } \right) - Q ( S , A ) \right)</script><h4 id="现时策略控制的SARSA算法"><a href="#现时策略控制的SARSA算法" class="headerlink" title="现时策略控制的SARSA算法"></a>现时策略控制的SARSA算法</h4><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_4.png" class="lazyload"><p>注：</p><ol><li>算法中的 $Q(s,a)$是以一张大表存储的，这不适用于解决规模很大的问题；</li><li>对于每一个Episode，在 $S$ 状态时采用的行为是 $A$ 基于当前策略的，同时该行为也是实际Episode发生的行为，在更新 $SA$ 行为状态价值循环里，个体并不实际执行在 $S’$ 下的 $A’$ 行为，而是将行为 $A’$ 留到下一个循环执行。</li></ol><h4 id="示例——有风格子世界"><a href="#示例——有风格子世界" class="headerlink" title="示例——有风格子世界"></a>示例——有风格子世界</h4><p><strong>已知：</strong>如图所示，环境是一个10*7的长方形格子世界，同时有一个起始位置S和一个终止目标位置G，水平下方的数字表示对应的列中有一定强度的风，当该数字是1时，个体进入该列的某个格子时，会按图中箭头所示的方向自动移动一格，当数字为2时，表示顺风移动2格，以此类推模拟风的作用。对于个体来说，它不清楚整个格子世界的构造，即它不知道格子是长方形的，也不知道边界在哪里。也不清楚起始位置、终止目标位置的具体为止。对于它来说，每一个格子就相当于一个封闭的房间，在没推开门离开当前房间之前它无法知道会进入哪个房间。个体具备记住曾经去过的格子的能力。格子可以执行的行为是朝上、下、左、右移动一步。</p><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_5.png" class="lazyload"><p><strong>问题：</strong>个体如何才能找到最短从起始格子S到终止目标格子G的最短路线？</p><p><strong>解答：</strong>可以设置个体每行走一步获得即时奖励为-1，直到到达终止目标位置的即时奖励为0，借此希望找到最优策略。衰减系数λ可设为1。</p><p>其最优路线如下图所示：</p><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_6.png" class="lazyload"><p>在个体找到这个最优行为序列的早期，由于个体对环境一无所知，SARSA算法需要尝试许多不同的行为，因此在一开始的2000多步里，个体只能完成少数几个完整的Episode，但随着个体找到一条链接起点到终点的路径，其快速优化策略的能力就显现的很明显了，因为它不需要走完一个Episode才能更新行为价值，而是每走一步就根据下一个状态能够得到的最好价值来更新当前状态的价值。</p><h3 id="SARSA-λ"><a href="#SARSA-λ" class="headerlink" title="SARSA(λ)"></a>SARSA(λ)</h3><h4 id="n-步SARSA"><a href="#n-步SARSA" class="headerlink" title="n-步SARSA"></a>n-步SARSA</h4><p>在之前，我们学习了n-步收获（见TD(λ)），这里类似的引出一个n-步Sarsa的概念。观察下面一些列的式子：</p><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_7.png" class="lazyload"><p>定义<strong>n-步Q收获（Q-return）：</strong></p><script type="math/tex; mode=display">q _ { t } ^ { ( n ) } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { n - 1 } R _ { t + n } + \gamma ^ { n } Q \left( S _ { t + n } \right)</script><p>则可以把n-步Sarsa用n-步Q收获来表示，如下式：</p><script type="math/tex; mode=display">Q \left( S _ { t } , A _ { t } \right) \leftarrow Q \left( S _ { t } , A _ { t } \right) + \alpha \left( q _ { t } ^ { ( n ) } - Q \left( S _ { t } , A _ { t } \right) \right)</script><p>假如我们给n-步Q收获的每一个收获分配一个权重，如下引入参数λ分配权重，并按权重对每一步Q收获求和，那么将得到  $q^{\lambda}$ 收获，它结合了所有n-步Q收获：</p><script type="math/tex; mode=display">q _ { t } ^ { \lambda } = ( 1 - \lambda ) \sum _ { n = 1 } ^ { \infty } \lambda ^ { n - 1 } q _ { t } ^ { ( n ) }</script><h4 id="两个方向理解SARSA-λ"><a href="#两个方向理解SARSA-λ" class="headerlink" title="两个方向理解SARSA(λ)"></a>两个方向理解SARSA(λ)</h4><h5 id="Sarsa-λ-前向认识"><a href="#Sarsa-λ-前向认识" class="headerlink" title="Sarsa(λ)前向认识"></a>Sarsa(λ)前向认识</h5><p>如果用某一状态的 $q^{\lambda}$ 收获来更新状态行为对的Q值，那么可以表示称如下的形式：</p><script type="math/tex; mode=display">Q \left( S _ { t } , A _ { t } \right) \leftarrow Q \left( S _ { t } , A _ { t } \right) + \alpha \left( q _ { t } ^ { \lambda } - Q \left( S _ { t } , A _ { t } \right) \right)</script><p>这就是前向认识Sarsa(λ)，使用它更新Q价值需要遍历完整的Episode，我们同样可以反向理解Sarsa(λ).</p><h5 id="Sarsa-λ-反向认识"><a href="#Sarsa-λ-反向认识" class="headerlink" title="Sarsa(λ)反向认识"></a>Sarsa(λ)反向认识</h5><p>与上一节对于TD(λ)的反向认识一样，引入效用追踪（Eligibility Trace）概念，不同的是这次的E值针对的不是一个状态，而是一个状态行为对：</p><script type="math/tex; mode=display">\begin{array} { c } { E _ { 0 } ( s , a ) = 0 } \\ { E _ { t } ( s , a ) = \gamma \lambda E _ { t - 1 } ( s , a ) + 1 \left( S _ { t } = s , A _ { t } = a \right) } \end{array}</script><p>它体现的是一个结果与某一个状态行为对的因果关系，与得到结果最近的状态行为对，以及那些在此之前频繁发生的状态行为对对得到这个结果的影响最大。</p><p>下式是引入ET概念的SARSA(λ)之后的Q值更新描述：</p><script type="math/tex; mode=display">\begin{aligned} E _ { t } ( s , a ) &= \gamma \lambda E _ { t - 1 } ( s , a ) + 1 \left( S _ { t } = s , A _ { t } = a \right)\\\delta _ { t } = & R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , A _ { t + 1 } \right) - Q \left( S _ { t } , A _ { t } \right) \\ & Q ( s , a ) \leftarrow Q ( s , a ) + \alpha \delta _ { t } E _ { t } ( s , a ) \end{aligned}</script><p>引入ET概念，同时使用SARSA(λ)将可以更有效的在线学习，因为不必要学习完整的Episode，数据用完即可丢弃。ET通常也是更多应用在在线学习算法中(online algorithm)。</p><h4 id="具体的SARSA-λ-算法"><a href="#具体的SARSA-λ-算法" class="headerlink" title="具体的SARSA(λ)算法"></a>具体的SARSA(λ)算法</h4><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_8.png" class="lazyload"><p>这里要提及一下的是$E(s,a)$在每浏览完一个Episode后需要重新置0，这体现了ET仅在一个Episode中发挥作用；其次要提及的是算法更新Q和E的时候针对的不是某个Episode里的Q或E，而是针对个体掌握的整个状态空间和行为空间产生的Q和E。</p><p>实际如果是基于查表的方式实现该算法，其速度明显比Sarsa要慢。毕竟带E的算法主要应用于在线更新。</p><h3 id="例——SARSA与SARSA-λ-的区别"><a href="#例——SARSA与SARSA-λ-的区别" class="headerlink" title="例——SARSA与SARSA(λ)的区别"></a>例——SARSA与SARSA(λ)的区别</h3><p>假定图描述的路线是个体采取两种算法中的一个得到的一个完整Episode的路径。为了下文更方便描述、解释两个算法之间的区别，先做几个合理的小约定：</p><ol><li><p>认定每一步的即时奖励为0，直到终点处即时奖励为1；</p></li><li><p>根据算法，除了终点以外的任何状态行为对的Q值可以是任意的，但我们设定所有的Q值均为0；</p></li><li><p>该路线是第一次找到终点的路线。</p><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_9.png" class="lazyload"></li></ol><div class="table-container"><table><thead><tr><th></th><th>Sarsa(0)</th><th>Sarsa(λ)</th></tr></thead><tbody><tr><td>第一次</td><td>Q表中的值均为0<br>依据当前策略 $A$ 右移动一步，到达$S’$位置<br>依据当前策略产生一个行为$A’$，在Q表中查找$Q(S’,A’)$得到$Q’$<br><br>依据更新公式更新表$Q(S,A)$值（在每到终点前都是0）<br>个体到达终点，获得奖励1，（从$S_H$向上走$A_{up}$到达的终点）<br>更新Q表中的$Q(S_H,A_{up})$<br>完成一个Episode（只有一次有意义的行为价值函数更新）</td><td>E表中的值均为0<br>依据当前策略 $A$ 右移动一步，到达$S’$位置<br>E(S,A)加1（表明经历了这个事件）<br>计算TD误差，估计这个事件对整个问题的价值（结果是0，说明这个举动没有说明积极帮助）<br>更新Q表中的$Q(S,A)$值<br>个体到达终点，获得奖励1，，（从$S_H$向上走$A_{up}$到达的终点）<br>计算TD误差为1,<br>更新<strong>整个过程经历过</strong>的$Q(s,a)$，其中与$<s_h,a_{up}>$越近发生的状态（体现在E值），价值提升越明显（个体在这个Episode中经历的所有状态行为对的Q值都将得到一个非0的更新）</s_h,a_{up}></td></tr><tr><td>第二次</td><td>一开始与首次一样都是盲目随机的<br>直到其进入终点位置下方的位置 $S_H$，在这个位置，个体更新的策略要求其选择向上的行为直接进入终点位置$S_G$</td><td>起点处向右走的价值不再是0<br>过程同上</td></tr><tr><td>第n次</td><td>如果采用greedy策略更新，个体最终将得到一条到达终点的路径，不过这条路径的倒数第二步永远是在终点位置的下方。<br>如果采用Ɛ-greedy策略更新，那么个体还会尝试到终点位置的左上右等其它方向的相邻位置价值也比较大，此时个体每次完成的路径可能都不一样。</td><td>如果采用greedy策略更新，个体将根据上次经验得到的新策略直接选择右走，并且一直按照原路找到终点。<br>如果采用Ɛ-greedy策略更新，那么个体还会尝试新的路线。</td></tr></tbody></table></div><h2 id="离线策略学习-Off-Policy-Learning"><a href="#离线策略学习-Off-Policy-Learning" class="headerlink" title="离线策略学习 Off-Policy Learning"></a><strong>离线策略学习 Off-Policy Learning</strong></h2><p>离线策略学习（Off-Policy Learning）则指的是在遵循一个策略 $\mu(a|s)$ 的同时评估另一个策略 $\pi(a|s)$，也就是计算确定这另一个策略下的状态价值函数 $v_{\pi}(s)$ 或状态行为价值函数 $q_{\pi}(s,a)$。</p><p>这样可以较容易的从人类经验或其他个体的经验中学习，也可以从一些旧的策略中学习，可以比较两个策略的优劣。可以遵循一个探索式策略的基础上优化现有的策略。</p><p>根据是否经历完整的Episode可以将其分为基于蒙特卡洛的和基于TD的。</p><h2 id="离线策略蒙特卡洛控制"><a href="#离线策略蒙特卡洛控制" class="headerlink" title="离线策略蒙特卡洛控制"></a>离线策略蒙特卡洛控制</h2><p>基于蒙特卡洛的离线策略学习仅有理论上的研究价值，在实际中毫无用处。</p><h2 id="离线策略时序差分控制-Off-Policy-Temporal-Difference-Control"><a href="#离线策略时序差分控制-Off-Policy-Temporal-Difference-Control" class="headerlink" title="离线策略时序差分控制 Off-Policy Temporal-Difference Control"></a>离线策略时序差分控制 Off-Policy Temporal-Difference Control</h2><p>离线策略TD学习的任务就是使用TD方法在遵循一个策略 $\mu(a|s)$ 的同时评估另一个策略 $\pi(a|s)$ 。具体数学表示为：</p><script type="math/tex; mode=display">V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( \frac { \pi \left( A _ { t } | S _ { t } \right) } { \mu \left( A _ { t } | S _ { t } \right) } \left( R _ { t + 1 } + \gamma V \left( S _ { t + 1 } \right) \right) - V \left( S _ { t } \right) \right)</script><p>这个公式可以这样解释：</p><p>个体处在状态 $S_t$中，基于策略 $\mu$ 产生了一个行为 $A_t$ ，执行该行为后进入新的状态 $S_{t+1}$ ，那么在当前策略下如何根据新状态的价值调整原来状态的价值呢？离线策略的方法就是，在状态时 $S_t$ 比较分别依据另一个策略 $\pi$和当前遵循的策略 $\mu$ 产生行为 $A_t$ 的概率大小。</p><ul><li><p>如果策略 $\pi$ 得到的概率值与遵循当前策略 $\mu$ 得到的概率值接近，说明根据状态 $S_{t+1}$ 价值来更新 $S_t$ 的价值同时得到两个策略的支持，这一更新操作比较有说服力。同时也说明在 $S_t$ 状态时，两个策略有接近的概率选择行为 $A_t$ 。</p></li><li><p>如果这一概率比值很小，则表明如果依照被评估的策略，选择 $A_t$ 的机会很小，这时候我们在更新  $S_t$ 价值的时候就不能过多的考虑基于当前策略得到的状态 $S_{t+1}$ 的价值。同样概率比值大于1时的道理也类似。这就相当于借鉴被评估策略的经验来更新我们自己的策略。</p></li></ul><h3 id="Q学习"><a href="#Q学习" class="headerlink" title="Q学习"></a>Q学习</h3><p>应用上述思想最好的方法是基于TD(0)的Q-学习（Q-learning）。它的要点在于，更新一个状态-行为的Q价值时，遵循策略 $\pi $ 得到的下一个状态行为对的Q价值。公式如下：</p><script type="math/tex; mode=display">Q \left( S _ { t } , A _ { t } \right) \leftarrow Q \left( S _ { t } , A _ { t } \right) + \alpha \left( R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , A ^ { \prime } \right) - Q \left( S _ { t } , A _ { t } \right) \right)</script><p>式中，$R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , A ^ { \prime } \right)$ TD目标是基于自己的策略(待评估) $\pi$ 产生的行为 $A’$ 得到的$Q$价值。</p><p><strong>Q学习最主要的表现形式是</strong>：个体遵循的策略 $\mu$ 是基于当前状态行为价值函数 $Q(s,a)$ 的一个Ɛ-贪婪策略，而目标策略 $\pi$ 是基于当前状态行为价值函数 $Q(s,a)$ 不包含Ɛ的单纯贪婪策略：</p><script type="math/tex; mode=display">\pi \left( S _ { t + 1 } \right) = \underset { a ^ { \prime } } { \operatorname { argmax } } Q \left( S _ { t + 1 } , a ^ { \prime } \right)</script><p>这样Q学习的TD目标值可以被大幅简化：</p><script type="math/tex; mode=display">\begin{aligned} & R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , A ^ { \prime } \right) \\ = & R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , \underset { a ^ { \prime } } { \operatorname { argmax } } Q \left( S _ { t + 1 } , a ^ { \prime } \right) \right) \\ = & R _ { t + 1 } + \max _ { x ^ { \prime } } \gamma Q \left( S _ { t + 1 } , a ^ { \prime } \right) \end{aligned}</script><p><strong>定理：</strong>Q学习控制将收敛至最优状态行为价值函数：$Q(s,a) \rightarrow q_*(s,a)$</p><p>下图是Q学习具体的更新公式和图解：</p><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_10.png" class="lazyload"><script type="math/tex; mode=display">Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma \max _ { a ^ { \prime } } Q \left( S ^ { \prime } , a ^ { \prime } \right) - Q ( S , A ) \right)</script><p>下图是Q学习的算法流程：</p><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_11.png" class="lazyload"><h2 id="总结DP与TD关系"><a href="#总结DP与TD关系" class="headerlink" title="总结DP与TD关系"></a><strong>总结DP与TD关系</strong></h2><p>下面两张图概括了各种DP算法和各种TD算法，同时也揭示了各种不同算法之间的区别和联系。总的来说TD是采样+有数据引导(bootstrap)，DP是全宽度+实际数据。如果从Bellman期望方程角度看：聚焦于状态本身价值的是迭代法策略评估（DP）和TD学习，聚焦于状态行为对价值函数的则是Q-策略迭代（DP）和SARSA；如果从针对状态行为价值函数的Bellman优化方程角度看，则是Q-价值迭代（DP）和Q学习。</p><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_12.png" class="lazyload"><img title="SARSA" data-src="/2019/12/02/强化学习基础4——基于无模型的强化学习方法/2_13.png" class="lazyload">]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习基础3——基于模型的动态规划方法</title>
      <link href="/2019/11/30/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%803%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95/"/>
      <url>/2019/11/30/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%803%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>上一章我们将强化学习的问题纳入了马尔可夫决策过程的框架下解决。马尔可夫决策过程可以利用元组$<s,a,p,r,\gamma>$来描述，而根据转移概率P是否已知，可以分为基于模型的动态规划方法和基于无模型的强化学习方法。这两类都包括策略迭代算法、值迭代算法、策略搜索算法。</s,a,p,r,\gamma></p><p>本讲着重讲解了利用动态规划来进行强化学习，具体是进行强化学习中的“规划”问题，也就是在已知模型的基础上判断一个策略的价值函数，并在此基础上寻找到最优的策略和最优价值函数，或者直接寻找最优策略和最优价值函数。本讲是整个强化学习课程核心内容的引子。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><strong>动态规划算法</strong>是解决复杂问题的一个方法，算法通过把复杂问题分解为子问题，通过求解子问题进而得到整个问题的解。在解决子问题的时候，其结果通常需要存储起来被用来解决后续复杂问题。当问题具有下列特性时，通常可以考虑使用动态规划来求解：第一个特性是一个复杂问题的最优解由数个小问题的最优解构成，可以通过寻找子问题的最优解来得到复杂问题的最优解；子问题在复杂问题内重复出现，使得子问题的解可以被存储起来重复利用。</p><p>马尔科夫决定过程（MDP）具有上述两个属性：Bellman方程把问题递归为求解子问题，价值函数就相当于存储了一些子问题的解，可以复用。因此可以使用动态规划来求解MDP。</p><p>我们用动态规划算法来求解一类称为“规划”的问题。“规划”指的是在了解整个MDP的基础上求解最优策略，也就是清楚模型结构的基础上：包括状态行为空间、转换矩阵、奖励等。这类问题不是典型的强化学习问题，我们可以用<strong>规划</strong>来进行<strong>预测</strong>和<strong>控制</strong>。</p><p>具体的数学描述是这样：</p><p><strong>预测：</strong>给定一个MDP $<s,a,p,r,\gamma>$ 和策略 $\pi$  ，或者给定一个MRP$<s,p^{\pi},r^{\pi},\gamma>$  ，要求输出基于当前策略π的价值函数 $V_{\pi}$ </s,p^{\pi},r^{\pi},\gamma></s,a,p,r,\gamma></p><p><strong>控制：</strong>给定一个MDP$<s,a,p,r,\gamma>$  ，要求确定最优价值函数 $V_{<em>}$ 和最优策略 $\pi_{</em>}$ </s,a,p,r,\gamma></p><h1 id="策略迭代-Policy-Iteration"><a href="#策略迭代-Policy-Iteration" class="headerlink" title="策略迭代 Policy Iteration"></a>策略迭代 Policy Iteration</h1><p>在讨论策略迭代算法前，先讨论两个问题：策略评估和策略改善</p><h2 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h2><h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><p><strong>问题：</strong>评估一个给定的策略π，也就是解决“预测”问题。</p><p><strong>解决方案：</strong>反向迭代应用Bellman期望方程</p><p><strong>具体方法：</strong></p><ul><li><p><strong>同步反向迭代</strong>，即在每次迭代过程中，对于第$k+1$ 次迭代，所有的状态s的价值用$v_k(s’)$ 计算并更新该状态第$k+1$  次迭代中使用的价值$v_k(S)$ ，其中$s’$是$s$的后继状态。此种方法通过反复迭代最终将收敛至$V_{\pi}$  。</p><script type="math/tex; mode=display">v _ { k + 1 } ( s ) = \sum _ { a \in A } \pi ( a | s ) \left( R _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in S } P _ { s s ^ { \prime } } ^ { a } v _ { k } \left( s ^ { \prime } \right) \right)</script><p>即：一次迭代内，状态s的价值等于前一次迭代该状态的即时奖励与所有s的下一个可能状态s’ 的价值与其概率乘积的和，如图示：</p><img title="This is an example image" data-src="/2019/11/30/强化学习基础3——基于模型的动态规划方法/policy_iteration1.png" class="lazyload"><p>公式的矩阵形式是：</p><script type="math/tex; mode=display">\mathbf { v } ^ { k + 1 } = \mathcal { R } ^ { \pi } + \gamma \mathcal { P } ^ { \pi } \mathbf { v } ^ { k }</script></li><li><p><strong>异步反向迭代</strong>，即在第k次迭代使用<strong>当次</strong>迭代的状态价值来更新状态价值。</p></li></ul><h3 id="示例——方格世界"><a href="#示例——方格世界" class="headerlink" title="示例——方格世界"></a>示例——方格世界</h3><img title="This is an example image" data-src="/2019/11/30/强化学习基础3——基于模型的动态规划方法/policy_iteration_example1.png" class="lazyload"><p><strong>已知：</strong></p><ul><li><strong>状态空间S</strong>：如图。S1 - S14非终止状态，ST终止状态，下图灰色方格所示两个位置；</li><li><strong>行为空间A</strong>：$\{n, e, s, w\} $对于任何非终止状态可以有东南西北移动四个行为；</li><li><strong>转移概率P：</strong>任何试图离开方格世界的动作其位置将不会发生改变，其余条件下将100%地转移到动作指向的状态；</li><li><strong>即时奖励R：</strong>任何在非终止状态间的转移得到的即时奖励均为-1，进入终止状态即时奖励为0；</li><li><strong>衰减系数γ：</strong>1；</li><li><strong>当前策略π：</strong>Agent采用随机行动策略，在任何一个非终止状态下有均等的几率采取任一移动方向这个行为，即$π(n|\cdot) = π(e|\cdot) = π(s|\cdot) = π(w|\cdot) = 1/4$。</li></ul><p><strong>问题：</strong>评估在这个方格世界里给定的策略。<br>   该问题等同于：求解该方格世界在给定策略下的（状态）价值函数，也就是求解在给定策略下，该方格世界里每一个状态的价值。</p><p><strong>求解：迭代法进行策略评估</strong><br><img title="This is an example image" data-src="/2019/11/30/强化学习基础3——基于模型的动态规划方法/policy_iteration_example2.png" class="lazyload"><br>状态价值在第153次迭代后收敛</p><h2 id="策略改善"><a href="#策略改善" class="headerlink" title="策略改善"></a>策略改善</h2><h3 id="理论-1"><a href="#理论-1" class="headerlink" title="理论"></a>理论</h3><p>通过方格世界的例子，我们得到了一个优化策略的办法，分为两步：首先我们在一个给定的策略下迭代更新价值函数：</p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \mathbb { E } \left[ R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots | S _ { t } = s \right]</script><p>随后，在当前策略基础上，贪婪地选取行为，使得后继状态价值增加最多：</p><script type="math/tex; mode=display">\pi' = greedy(v_{\pi})</script><p>在刚才的格子世界中，基于给定策略的价值迭代最终收敛得到的策略就是最优策略，但通过一个回合的迭代计算价值联合策略改善就能找到最优策略不是普遍现象。通常，还需在改善的策略上继续评估，反复多次。不过这种方法总能收敛至最优策略$\pi^*$ 。</p><h3 id="理论证明"><a href="#理论证明" class="headerlink" title="理论证明"></a>理论证明</h3><ol><li><p>考虑一个确定的策略（$a=\pi(s)$）： </p><p>为什么证明这个要考虑确定的策略？因为策略改善使用的贪婪策略，是将一个普通策略（在这个状态下做哪个行动的概率是<em>*</em>）变成了贪婪策略（在这个状态下就做这个行动，这个行动是所有行动里值函数最大的那个）。即为确定策略。</p><p>对于确定策略，由于$\pi(s)=a$（即在这个情况下一定选择这个策略），值函数公式 $v _ { \pi } ( s ) = \sum _ { a \in A } \pi ( a | s ) q _ { \pi } ( s , a )$等于$q _ { \pi } ( s , a )$ 等于$R_s^a + \gamma\sum _ { s’ \in S } P_{ss’}^a v _ { \pi } ( s’ )$（上一章的知识点）</p></li><li><p>通过贪婪计算优化策略（贪的行为价值函数）：</p><script type="math/tex; mode=display">\pi'(s) = \underset { a \in \mathcal { A } } { \operatorname { argmax } } q _ { \pi } ( s , a )</script></li><li><p>接着推导</p><script type="math/tex; mode=display">q_{\pi}(s,\pi'(s)) = \underset { a \in \mathcal { A } } { \operatorname { max } } q _ { \pi } ( s , a ) \geq q _ { \pi } ( s , \pi(s) ) = v_{\pi}(s)</script><p>由于策略是贪婪q值得到的，则这个更新策略后的q值一定不小于更新前的q值，而根据1所述，$q _ { \pi } ( s , \pi(s) ) = v_{\pi}(s)$。</p></li></ol><ol><li><p>而根据上一章的Bellman最优方程，我们找最优策略的目标是最大化v值，现在的贪婪策略在最大化比v值还大的值，等同于在最大化v值，即v值在每一次迭代都会更大。当$q_{\pi}(s,\pi’(s)) = \underset { a \in \mathcal { A } } { \operatorname { max } } q _ { \pi } ( s , a ) = q _ { \pi } ( s , \pi(s) ) = v_{\pi}(s)$时就可以结束迭代了，因为满足Bellman最优方程，说明当前策略下的状态价值就是最优状态价值。因而此时的策略就是最优策略。</p><p>（$v_{\pi’}(s)\geq v_{\pi}(s)$证明见David Silver视频PPT）</p></li></ol><h2 id="策略迭代算法-策略评估-策略改善"><a href="#策略迭代算法-策略评估-策略改善" class="headerlink" title="策略迭代算法=策略评估+策略改善"></a><strong>策略迭代算法=策略评估+策略改善</strong></h2><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>在当前策略上迭代计算$v$值，再根据$v$值贪婪地更新策略，如此反复多次，最终得到最优策略$\pi^<em>$ 和最优状态价值函数$V^</em>$  </p><p><strong>贪婪</strong> 指的是仅采取那个（些）使得状态价值得到最大的行为。</p><img title="This is an example image" data-src="/2019/11/30/强化学习基础3——基于模型的动态规划方法/policy_iteration2.png" class="lazyload"><img title="This is an example image" data-src="/2019/11/30/强化学习基础3——基于模型的动态规划方法/policy_iteration_algorithm.png" class="lazyload"><h3 id="示例——连锁汽车租赁"><a href="#示例——连锁汽车租赁" class="headerlink" title="示例——连锁汽车租赁"></a>示例——连锁汽车租赁</h3><p>举了一个汽车租赁的例子，说明如何在给定策略下得到基于该策略的价值函数，并根据更新的价值函数来调整策略，直至得到最优策略和最优价值函数。</p><p>一个连锁汽车租赁公司有两个地点提供汽车租赁，由于不同的店车辆租赁的市场条件不一样，为了能够实现利润最大化，该公司需要在每天下班后在两个租赁点转移车辆，以便第二天能最大限度的满足两处汽车租赁服务。</p><p><strong>已知</strong></p><p><strong>状态空间：</strong>2个地点，每个地点最多20辆车供租赁</p><p><strong>行为空间：</strong>每天下班后最多转移5辆车从一处到另一处；</p><p><strong>即时奖励：</strong>每租出1辆车奖励10元，必须是有车可租的情况；不考虑在两地转移车辆的支出。</p><p><strong>转移概率：</strong>求租和归还是随机的，但是满足泊松分布  。第一处租赁点平均每天租车请求3次，归还3次；第二处租赁点平均每天租车4次，归还2次。</p><p><strong>衰减系数</strong> $\gamma$：0.9；</p><p><strong>问题：</strong>怎样的策略是最优策略？</p><p><strong>求解方法：</strong>从一个确定的策略出发进行迭代，该策略可以是较为随意的，比如选择这样的策略：不管两地租赁业务市场需求，不移动车辆。以此作为给定策略进行价值迭代，当迭代收敛至一定程度后，改善策略，随后再次迭代，如此反复，直至最终收敛。</p><p>在这个问题中，状态用两个地点的汽车存量来描述，比如分别用c1,c2表示租赁点1,2两处的可租汽车数量，可租汽车数量同时参与决定夜间可转移汽车的最大数量。</p><p>解决该问题的核心就是依据泊松分布确定状态<c1,c2>的即时奖励，进而确定每一个状态的价值。</c1,c2></p><img title="This is an example image" data-src="/2019/11/30/强化学习基础3——基于模型的动态规划方法/policy_iteration_example3.png" class="lazyload"><h2 id="改进策略迭代"><a href="#改进策略迭代" class="headerlink" title="改进策略迭代"></a>改进策略迭代</h2><p>有时候不需要持续迭代至最有价值函数，可以设置一些条件提前终止迭代</p><ul><li>比如设定一个$ \epsilon $，比较两次迭代的价值函数平方差</li><li>直接设置迭代次数</li></ul><h1 id="价值迭代-Value-Iteration"><a href="#价值迭代-Value-Iteration" class="headerlink" title="价值迭代 Value Iteration"></a>价值迭代 Value Iteration</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><h3 id="优化原则-Principle-of-Optimality"><a href="#优化原则-Principle-of-Optimality" class="headerlink" title="优化原则 Principle of Optimality"></a>优化原则 Principle of Optimality</h3><p>一个最优策略可以被分解为两部分</p><ol><li>从状态 $s$ 到下一个状态 $s’$ 采取了最优行为$A_*$  </li><li>在状态 $s’$ 时遵循一个最优策略</li></ol><blockquote><p><strong>定理</strong></p><p>一个策略能够使得状态s获得最优价值，当且仅当：对于从状态$s$可以到达的任何状态$s’$，该策略能够使得状态s’的价值是最优价值：</p></blockquote><h3 id="确定性的价值迭代"><a href="#确定性的价值迭代" class="headerlink" title="确定性的价值迭代"></a>确定性的价值迭代</h3><p>如果我们知道了子问题$v_<em>(s’)$，则$v_</em>(s)$只需要通过一步就能得到</p><script type="math/tex; mode=display">v _ { * } ( s ) \leftarrow \max _ { a \in \mathcal { A } } \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)</script><p><strong>示例——最短路径</strong></p><p><strong>问题：</strong>如何在一个4*4的方格世界中，找到任一一个方格到最左上角方格的最短路径</p><p>如果我们清楚地知道我们期望的最终（goal）状态的位置以及反推需要明确的状态间关系，那么可以认为是一个<strong>确定性的价值迭代</strong>。此时，我们可以把问题分解成一些列的子问题，从最终目标状态开始分析，逐渐往回推，直至推至所有状态。</p><p>简要思路：在已知左上角为最终目标的情况下，我们可以从与左上角相邻的两个方格开始计算，因为这两个方格是可以仅通过1步就到达目标状态的状态，或者说目标状态是这两个状态的后继状态。最短路径可以量化为：每移动一步获得一个-1的即时奖励。为此我们可以更新与目标方格相邻的这两个方格的状态价值为-1。如此依次向右下角倒推，直至所有状态找到最短路径。</p><h2 id="价值迭代算法-value-iteration"><a href="#价值迭代算法-value-iteration" class="headerlink" title="价值迭代算法 value iteration"></a>价值迭代算法 value iteration</h2><p><strong>问题：</strong>寻找最优策略π</p><p><strong>解决方案：</strong>采用Bellman最优方程，从初始状态价值开始同步迭代计算，最终收敛，整个过程中没有遵循任何策略</p><script type="math/tex; mode=display">V _ { k + 1 } ( s ) = \max _ { a } \sum _ { s ^ { \prime } , r } P \left( s ^ { \prime } , r | s , a \right) \left( r + \gamma V _ { k } \left( s ^ { \prime } \right) \right)</script><img title="This is an example image" data-src="/2019/11/30/强化学习基础3——基于模型的动态规划方法/value_iteration_algorithm.png" class="lazyload"><p>对每一个当前状态 $s$ ,对每个可能的动作 $a$ 都计算一下采取这个动作后到达的下一个状态的期望价值。看看哪个动作可以到达的状态的期望价值函数最大，就将这个最大的期望价值函数作为当前状态的价值函数 $V(s)$ 循环执行这个步骤，直到价值函数收敛。</p><h2 id="价值迭代算法与策略迭代算法的区别"><a href="#价值迭代算法与策略迭代算法的区别" class="headerlink" title="价值迭代算法与策略迭代算法的区别"></a>价值迭代算法与策略迭代算法的区别</h2><ul><li>策略迭代有一个策略直接作用于value空间（即不会有value值来构建策略，策略再构建value值的过程）；而价值迭代过程其间得到的价值函数，不对应任何策略</li><li>值迭代是根据状态期望值选择动作，而策略迭代是先估计状态值然后修改策略 </li></ul><h1 id="同步动态规划问题总结"><a href="#同步动态规划问题总结" class="headerlink" title="同步动态规划问题总结"></a>同步动态规划问题总结</h1><img title="This is an example image" data-src="/2019/11/30/强化学习基础3——基于模型的动态规划方法/conclusion.png" class="lazyload"><ul><li><p><strong>预测问题</strong>：在给定策略下迭代计算价值函数。</p></li><li><p><strong>控制问题</strong>：策略迭代寻找最优策略问题则先在给定或随机策略下计算状态价值函数，根据状态函数贪婪更新策略，多次反复找到最优策略；单纯使用价值迭代，全程没有策略参与也可以获得最优策略，但需要知道状态转移矩阵，即状态s在行为a后到达的所有后续状态及概率。</p><p> 使用状态价值函数或行为价值函数两种价值迭代的算法时间复杂度都较大，为 $O(mn^2)$ 或$O(m^2n^2)$  。一种改进方案是使用异步动态规划，其他的方法即放弃使用动态规划，随后的几讲中将详细讲解其他方法。</p></li></ul><h1 id="动态规划的一些扩展"><a href="#动态规划的一些扩展" class="headerlink" title="动态规划的一些扩展"></a>动态规划的一些扩展</h1><h2 id="异步动态规划-Asynchronous-Dynamic-Programming"><a href="#异步动态规划-Asynchronous-Dynamic-Programming" class="headerlink" title="异步动态规划 Asynchronous Dynamic Programming"></a>异步动态规划 Asynchronous Dynamic Programming</h2><p>几个可能改进的点子</p><h3 id="原位动态规划-In-place-dynamic-programming"><a href="#原位动态规划-In-place-dynamic-programming" class="headerlink" title="原位动态规划(In-place dynamic programming)"></a>原位动态规划(In-place dynamic programming)</h3><p>直接原地更新下一个状态的v值，而不像同步迭代那样需要额外存储新的v值。在这种情况下，<strong>按何种次序更新状态价值有时候会比较有意义</strong>。</p><script type="math/tex; mode=display">v ( s ) \leftarrow \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v \left( s ^ { \prime } \right) \right)</script><h3 id="重要状态优先更新-Priortised-Sweeping"><a href="#重要状态优先更新-Priortised-Sweeping" class="headerlink" title="重要状态优先更新(Priortised Sweeping)"></a>重要状态优先更新(Priortised Sweeping)</h3><p>对那些重要的状态优先更新。</p><p>使用Bellman error：</p><script type="math/tex; mode=display">\max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v \left( s ^ { \prime } \right) \right) - v ( s )</script><p>来确定哪些状态是比较重要的。Bellman error 反映的是当前的状态价值与更新后的状态价值差的绝对值。Bellman error越大，越有必要优先更新。对那些Bellman error较大的状态进行备份。这种算法使用优先级队列能够较得到有效的实现。</p><h3 id="实时动态规划-Real-time-dynamic-programming"><a href="#实时动态规划-Real-time-dynamic-programming" class="headerlink" title="实时动态规划(Real-time dynamic programming)"></a>实时动态规划(Real-time dynamic programming)</h3><p>更新那些仅与个体关系密切的状态，同时使用个体的经验来知道更新状态的选择。有些状态虽然理论上存在，但在现实中几乎不会出现。利用已有现实经验。</p><script type="math/tex; mode=display">v \left( S _ { t } \right) \leftarrow \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { S _ { t } } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { S _ { t s ^ { \prime } } } ^ { a } v \left( s ^ { \prime } \right) \right)</script><p>St是实际与Agent相关或者说Agent经历的状态，可以省去关于那些仅存在理论上的状态的计算。</p><h2 id="采样更新-Sample-Backups"><a href="#采样更新-Sample-Backups" class="headerlink" title="采样更新 Sample Backups"></a>采样更新 Sample Backups</h2><p>动态规划使用full-width backups。意味着使用DP算法，对于每一次状态更新，都要考虑到其所有后继状态及所有可能的行为，同时还要使用MDP中的状态转移矩阵、奖励函数（信息）。DP解决MDP问题的这一特点决定了其对中等规模（百万级别的状态数）的问题较为有效，对于更大规模的问题，会带来Bellman维度灾难。</p><p>因此在面对大规模MDP问题是，需要寻找更加实际可操作的算法，主要的思想是Sample Backups，后续会详细介绍。这类算法的优点是不需要完整掌握MDP的条件（例如奖励机制、状态转移矩阵等），通过Sampling（举样）可以打破维度灾难，反向更新状态函数的开销是常数级别的，与状态数无关。</p><h2 id="近似动态规划-Approximate-Dynamic-Programming"><a href="#近似动态规划-Approximate-Dynamic-Programming" class="headerlink" title="近似动态规划 Approximate Dynamic Programming"></a>近似动态规划 Approximate Dynamic Programming</h2><p>使用其他技术手段（例如神经网络）建立一个参数较少，消耗计算资源较少、同时虽然不完全精确但却够用的近似价值函数：</p><script type="math/tex; mode=display">\bar { v } _ { k } ( s ) = \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v \left( s ^ { \prime } , w _ { k } \right) \right)</script><p>注：本讲的内容主要还是在于理解强化学习的基本概念，各种Bellman方程，在实际应用中，很少使用动态规划来解决大规模强化学习问题。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习基础2——强化学习问题描述</title>
      <link href="/2019/11/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%802%E2%80%94%E2%80%94%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0/"/>
      <url>/2019/11/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%802%E2%80%94%E2%80%94%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<p>在强化学习中，马尔科夫决策过程（Markov decision process, MDP）是对环境进行描述的工具。几乎所有的强化学习问题都可以转化为MDP。 </p><p>本章的脉络为</p><ol><li>马尔科夫性（介绍状态的马尔科夫性和状态转移概率）</li><li>马尔科夫过程（由有限具有马尔科夫性的状态和状态转移概率组成的马尔科夫链）</li><li>马尔科夫奖励过程（在马尔科夫过程的基础上加入奖励和衰减系数）</li><li>马尔科夫决策过程（在马尔科夫奖励过程的基础上加入动作）</li></ol><h1 id="马尔可夫性（Markov-Property）"><a href="#马尔可夫性（Markov-Property）" class="headerlink" title="马尔可夫性（Markov Property）"></a>马尔可夫性（Markov Property）</h1><p>未来只依赖于最近给定的状态，则认为该状态具有<strong>马尔科夫性</strong>。即：</p><script type="math/tex; mode=display">\mathrm{P}\left[S_{\mathrm{r}+1} | \mathrm{S}_{t} \right] = \mathrm{P}\left[S_{\mathrm{}+1} | \mathrm{S}_{1}, \ldots, \mathrm{S}_{\mathrm{t}}\right]</script><p><strong>状态转移概率公式</strong>可以用来描述马尔科夫性：</p><script type="math/tex; mode=display">\mathcal{P}_{s s^{\prime}}=\mathrm{P}\left[S_{t+1}=s^{\prime} | S_{t}=s\right]</script><p><strong>状态转移矩阵</strong> $\mathcal{P}$ 用来表示当前任意状态转移到其他状态的概率。每一行代表，当前处于这种状态时，下一个状态出现的概率</p><script type="math/tex; mode=display">\text{to} \\\mathcal{P}=\text { from }\left[\begin{array}{ccc}{\mathcal{P}_{11}} & {\cdots} & {\mathcal{P}_{1 n}} \\ {\vdots} & {} & {} \\ {\mathcal{P}_{n 1}} & {\cdots} & {\mathcal{P}_{n n}}\end{array}\right]</script><p> 式中n为状态数量，矩阵中每一行元素之和为1. </p><h1 id="马尔可夫过程（Markov-Process）"><a href="#马尔可夫过程（Markov-Process）" class="headerlink" title="马尔可夫过程（Markov Process）"></a>马尔可夫过程（Markov Process）</h1><p> <strong>马尔科夫过程</strong> 又叫马尔科夫链(Markov Chain)，是一个无记忆的随机过程</p><h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>可以用一个元组$<s, \mathcal{p}>$表示</s,></p><ul><li>$S$ 是具有马尔科夫性的有限随机状态集$\{\mathrm{S}_1，\mathrm{S}_2，\cdots\}$</li><li>$\mathcal{P}$是状态之间的转移概率矩阵</li></ul><h2 id="例"><a href="#例" class="headerlink" title="例"></a>例</h2><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/student_markov_chain.png" class="lazyload"><p>圆圈表示学生所处的状态，方块Sleep表示马尔科夫链的最终状态或者可以描述成自循环的状态，也就是Sleep状态的下一个状态100%的几率还是自己。箭头表示状态之间的转移，箭头上的数字表示当前转移的概率。 右边是这个马尔科夫链对应的状态转移矩阵。</p><p> 当学生处在第一节课（Class1）时，他/她有50%的几率会参加第2节课（Class2）；同时在也有50%的几率不在认真听课，进入到浏览facebook这个状态中。在浏览facebook这个状态时，他有90%的几率在下一时刻继续浏览，也有10%的几率返回到课堂内容上来。当学生进入到第二节课（Class2）时，会有80%的几率继续参加第三节课（Class3），也有20%的几率觉得课程较难而退出（Sleep）。当学生处于第三节课这个状态时，他有60%的几率通过考试，继而100%的退出该课程，也有40%的可能性去酒吧，又分别有20%、40%、40%的几率返回值第一、二、三节课重新继续学习。 </p><p>一个学生从状态Class1开始，最终结束于Sleep，其间的过程根据状态转化图可以有很多种可能性，这些都称为<strong>样本集（Sample Episodes）</strong>。一个<strong>采样</strong>是从初始状态到结束状态的一个状态序列，以下采样都是可能的：</p><ul><li>C1 - C2 - C3 - Pass - Sleep</li><li>C1 - FB - FB - C1 - C2 - Sleep</li><li>C1 - C2 - C3 - Pub - C2 - C3 - Pass - Sleep</li></ul><h1 id="马尔科夫奖励过程（-Markov-Reward-Process）"><a href="#马尔科夫奖励过程（-Markov-Reward-Process）" class="headerlink" title="马尔科夫奖励过程（ Markov Reward Process）"></a>马尔科夫奖励过程（ Markov Reward Process）</h1><p>马尔科夫奖励过程可以看做带有value判断的马尔科夫过程，这个value标志着这个状态的好坏。它在马尔科夫过程的基础上增加了<strong>奖励$R$</strong>和<strong>衰减系数$\gamma$</strong>。</p><h2 id="描述-概念"><a href="#描述-概念" class="headerlink" title="描述 / 概念"></a>描述 / 概念</h2><p>可以用四元组$<s, p, r, \gamma>$表示，分别是状态，转移概率，即时奖励，衰减系数。</s,></p><ul><li><p><strong>即时奖励（Reward）</strong> $R$</p><p>某一时刻（$t$）处在状态$s$下，在下 一个时刻（$t+1$）能获得的<strong>奖励期望</strong>，即为状态$s$下的奖励，与下一刻去哪个状态没有关系：</p><script type="math/tex; mode=display">R_{s}=E\left[R_{t+1} | S_{t}=s\right]</script><p>例：马尔科夫奖励过程图示。在“马尔科夫过程”基础上增加了针对每一个状态的奖励</p><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/Student_MRP.png" class="lazyload"><p>当学生处于 Class1 状态时，若他选择参加 Class2 获得的 Reward 是 -2；若他选择去刷 Facebook  获得的 Reward 是 -2。即他从 Class1 状态离开就可获得立即奖励，这个即时奖励和他去哪没关系。</p></li></ul><ul><li><p><strong>收益（ Return）</strong> $G_t$</p><p>由于即时奖励与具体的下一状体没关系，引入收益，来量化一个片段（Episode）的奖励和</p><p>收益为在一个马尔科夫奖励链上从 $t$ 时刻开始往后所有有衰减的奖励和。公式如下：</p><script type="math/tex; mode=display">G_{t}=R_{t+1}+\gamma R_{t+2}+\ldots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}</script><p>其中<strong>衰减系数 （Discount Factor）</strong> $\gamma \in [0,1]$ 体现了未来的奖励在当前时刻的价值比例，在 $k+1$ 时刻获得的奖励 $R$ 在t时刻的体现出的价值是  $\gamma^{k} R$，$\gamma$ 接近于0，表明趋向于“近视”性评估（贪婪）；γ接近1则表明偏重考虑远期的收益。</p><p><em>这里是从$R_{t+1}$开始算起的，因为描述上通常是智能体在$t$时刻做了一个操作，环境接收操作后更新时间，所以这个奖励通常描述为$R_{t+1}$</em></p><blockquote><p>衰减系数的引入有很多理由，其中有数学表达的方便，避免陷入无限循环，远期利益具有一定的不确定性，符合人类对于眼前利益的追求，符合金融学上获得的利益能够产生新的利益因而更有价值等等。</p></blockquote></li></ul><p>  收益值是针对一次片段的结果，存在很大的样本偏差。$G_t$ 是从 $t$ 时刻的状态到终止状态的一条状态转移序列的收益值，但从 $t$ 时刻的状态到终止状态可能有多条路径，就如上述状态转移图所示（而且可能还有很多序列没有观测到）。故要想精确的计算出 $G_t$ 是不可能的，因为无法穷举所有序列。</p><ul><li><p><strong>价值函数 Value Function</strong></p><p>引入价值函数是为了解决每个取样都对应一个$G_t$的问题。价值函数是一个数值包含了多条路径。</p><p>价值函数给出了某一状态或某一行为的长期价值，用来衡量一个状态的好坏。</p><p>一个马尔科夫奖励过程中某一状态的价值函数，为从该状态开始的马尔可夫链收获的期望：</p><script type="math/tex; mode=display">v(s)=E\left[G_{t} | S_{t}=s\right]</script><p><em>注：价值可以仅描述状态，也可以描述某一状态下的某个行为，在一些特殊情况下还可以仅描述某个行为</em></p></li></ul><h2 id="例-1"><a href="#例-1" class="headerlink" title="例"></a>例</h2><p>为方便计算，把“学生马尔科夫奖励过程”示例图表示成下表的形式。表中第二行对应各状态的即时奖励值，蓝色区域数字为状态转移概率，表示为从所在行状态转移到所在列状态的概率：</p><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MRP_example1.png" class="lazyload"><p><strong>收益计算例</strong></p><p>在$t=1$时刻（$S_{1}=C_{1}$）时状态 $S_1$ 的收益公式：            </p><script type="math/tex; mode=display">G_{1}=R_{2}+\gamma R_{3}+\ldots +\gamma^{r-2} R_{r}</script><p>如下4个马尔科夫链取样，现计算当$\gamma= 1/2$时的收益值。</p><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MRP_example2.png" class="lazyload"><p>从上表也可以理解到，收益是针对一个马尔科夫链中的<strong>某一个状态</strong>来说的。且收益是不确定的，随机的（因为取样是随机的）。但是价值函数不是随机的，价值函数是收益的期望</p><p><strong>价值计算例</strong></p><p>对于上述状态转移图如果仅仅观测到以上 4 条序列，那么在状态 Class1 处的学生的值函数就是上述 4 个 值除以 4 即可。</p><script type="math/tex; mode=display">v(Class1) = ( (-2.25) + (-3.125) + (-3.41) + (-3.21) )  ÷ 4 =  2.996</script><p><strong>不同衰减系数下</strong></p><p>当$\gamma = 0$时，上表描述的MRP中，各状态的即时奖励就与该状态的价值相同。当$\gamma \neq 0$时，各状态的价值需要通过计算得到，这里先给出$ \gamma $分别为0，0.9，和1三种情况下各状态的价值，如下图所示。</p><p>各状态圈内的数字表示该状态的价值，圈外的$R=-2$等表示的是该状态的即时奖励。</p><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MRP_example3.png" class="lazyload"><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MRP_example4.png" class="lazyload"><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MRP_example5.png" class="lazyload"><blockquote><p><strong>各状态价值的确定是很重要的，RL的许多问题可以归结为求状态的价值问题</strong>。假设你已经算出来上面的各个状态下的值函数，那么对于强化学习而言，假设是找到一条最优路径，起点状态是 Class1 ，那么很明显最优状态链是class1-&gt;class2-&gt;class3-&gt;pass-&gt;sleep，也就是说<strong>这条路径的累计回报</strong>最大，也就是说值函数求出来了，其实强化学习问题就解决了，对应的就是后续的值迭代法、Q-learning等思路了。因此如何求解各状态的价值，也就是寻找一个价值函数（从状态到价值的映射）就变得很重要了。</p></blockquote><h2 id="价值函数的推导"><a href="#价值函数的推导" class="headerlink" title="价值函数的推导"></a>价值函数的推导</h2><p>状态值函数的引入解决了不同路径收益不同问题。但状态值函数不好算，因为在计算某个状态的价值时，需要使用到将来所有状态的 $G_t$，这明显是不科学的。所以需要进一步推导价值函数<strong>使价值函数容易求解</strong>。</p><h3 id="按价值定义公式推导"><a href="#按价值定义公式推导" class="headerlink" title="按价值定义公式推导"></a>按价值定义公式推导</h3><script type="math/tex; mode=display">\begin{aligned} v(s) &=\mathbb{E}\left[G_{t} | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} | S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) | S_{t}=s\right] \end{aligned}</script><p>在导出最后一行时，将$G_{t+1}$ 变成了 $v\left(S_{t+1}\right)$。其理由是收获的期望等于收获的期望的期望。</p><h3 id="Bellman方程"><a href="#Bellman方程" class="headerlink" title="Bellman方程"></a>Bellman方程</h3><p>先看看贝尔曼方程是什么</p><blockquote><p>贝尔曼方程，又叫动态规划方程，是以Richard Bellman命名的，表示动态规划问题中相邻状态关系的方程。某些决策问题可以按照时间或空间分成多个阶段，每个阶段做出决策从而使整个过程取得效果最优的多阶段决策问题，可以用动态规划方法求解。某一阶段最优决策的问题，通过贝尔曼方程转化为下一阶段最优决策的子问题，从而初始状态的最优决策可以由终状态的最优决策(一般易解)问题逐步迭代求解。存在某种形式的贝尔曼方程，是动态规划方法能得到最优解的必要条件。 </p></blockquote><p>（哦！动态规划的递推式，推出了这个就可以用递归/递推来求价值函数了）</p><p>根据上面对价值函数的定义公式的推导，最后我们得到了针对MRP的Bellman方程： </p><script type="math/tex; mode=display">v(s)=\mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) | S_{t}=s\right]</script><p>通过方程可以看出 $v(s)$ 由两部分组成</p><ul><li><p>$s$ 状态的即时奖励期望。即时奖励期望等于即时奖励，因为根据即时奖励的定义，它与下一个状态无关</p></li><li><p>下一时刻状态的价值期望的衰减值，可以根据下一时刻状态的概率分布得到其期望。</p></li></ul><p>如果用 $s’$ 表示 $s$ 状态下一时刻任一可能的状态，那么Bellman方程可以写成：</p><script type="math/tex; mode=display">v(s)=\mathcal{R}_{s}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}} v\left(s^{\prime}\right)</script><p>即计算状态 s 的值函数方法 = 该状态的立即奖励 + 遍历该状态的各个后继状态，对于每一个后继状态：$\gamma$  × 状态转移概率  × 后继状态的值函数 求和。 </p><p><strong>例</strong><br>下图已经给出了$\gamma=1$时各状态的价值，状态$C_3$  的价值可以通过状态Pub和Pass的价值以及他们之间的状态转移概率来计算：</p><script type="math/tex; mode=display">4.3=-2+1.0 *(0.6 * 10+0.4 * 0.8)</script><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MRP_Bellman_example.png" class="lazyload"><h3 id="Bellman方程的矩阵形式和求解"><a href="#Bellman方程的矩阵形式和求解" class="headerlink" title="Bellman方程的矩阵形式和求解"></a>Bellman方程的矩阵形式和求解</h3><script type="math/tex; mode=display">v=\mathcal{R}+\gamma \mathcal{P} v</script><p>结合矩阵的具体表达形式还是比较好理解的：</p><script type="math/tex; mode=display">\left[\begin{array}{c}{v(1)} \\ {\vdots} \\ {v(n)}\end{array}\right]=\left[\begin{array}{c}{\mathcal{R}_{1}} \\ {\vdots} \\ {\mathcal{R}_{n}}\end{array}\right]+\gamma\left[\begin{array}{ccc}{\mathcal{P}_{11}} & {\cdots} & {\mathcal{P}_{1 n}} \\ {\vdots} & {} & {} \\ {\mathcal{P}_{11}} & {\cdots} & {\mathcal{P}_{n n}}\end{array}\right]\left[\begin{array}{c}{v(1)} \\ {\vdots} \\ {v(n)}\end{array}\right]</script><p>Bellman方程是一个线性方程组，因此理论上解可以直接求解：</p><script type="math/tex; mode=display">\begin{aligned} v & = \mathcal { R } + \gamma \mathcal { P } v \\ ( I - \gamma \mathcal { P } ) v & = \mathcal { R } \\ v & = ( l - \gamma \mathcal { P } ) ^ { - 1 } \mathcal { R } \end{aligned}</script><p>实际上，计算复杂度是$O(n^3)$ ， $n$是状态数量。因此直接求解仅适用于小规模的MRPs。大规模MRP的求解通常使用迭代法。常用的迭代方法有：动态规划Dynamic Programming、蒙特卡洛评估Monte-Carlo evaluation、时序差分学习Temporal-Difference。</p><h1 id="马尔科夫决策过程（Markov-Decision-Process）"><a href="#马尔科夫决策过程（Markov-Decision-Process）" class="headerlink" title="马尔科夫决策过程（Markov Decision Process）"></a>马尔科夫决策过程（Markov Decision Process）</h1><p>相较于马尔科夫奖励过程，马尔科夫决策过程多了一个行为集合$A$</p><h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><p>马尔科夫决策过程(MDP)与MP、MRP的区别</p><ul><li>在MP和MRP中，我们都是作为观察者，去观察其中的状态转移现象，去计算回报值</li><li>对于一个RL问题，我们更希望去改变状态转移的流程，去最大化回报值</li><li>通过MRP中引入决策即得到了马尔科夫决策过程</li></ul><h2 id="描述-概念-1"><a href="#描述-概念-1" class="headerlink" title="描述 / 概念"></a>描述 / 概念</h2><p>它是这样的一个五元组$M=(S,A,P,R,\gamma)$描述</p><ul><li><p>$ S $：表示状态集(states)，有$s \in S$，$s_i$ 表示第 $i$ 步的状态。</p></li><li><p>$A$：表示一组动作(actions)，有$a \in A$，$a_i$ 表示第 $i$ 步的动作。</p></li><li><p>$P$：表示状态转移概率。表示的是在当前$s \in S$状态下，经过$a \in A$作用后，会转移到的其他状态的概率分布情况。比如，在状态$s$下执行动作$a$，转移到$s’$的概率可以表示为$p(s’|s,a)$，具体的数学表达式如下：</p><script type="math/tex; mode=display">\mathcal{P}_{s s^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} | S_{t}=s, A_{t}=a\right]</script><p>（个人理解：马尔科夫奖励过程是在当前状态下选择到下一状态，而马尔可夫决策过程是在当前状态下先选择一个动作，这个动作会有不同的概率导向不同的新状态，智能体只能做这个动作，不能控制这个动作导向的状态，这个动作导向的结果由环境决定，所以马尔可夫决策过程很适合描述强化学习场景。这里的状态转移概率矩阵是三维的，第一维是当前状态，第二维是所做动作，第三维是下一状态，下面的回报函数同理）</p></li><li><p>$R$：回报函数(reward function)。$R(s, a)$ 描述了在状态 $s$做动作 $a$的奖励。与MRP的奖励与状态对应不同，<strong>MDP的奖励是与动作对应的</strong>，具体的数学表达式如下：</p><script type="math/tex; mode=display">\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} | S_{t}=s, A_{t}=a\right]</script></li><li><p>$\gamma$ ：衰减系数</p></li></ul><p>注意：$P$和$R$看起来很类似马尔科夫奖励过程，但这里的它们都与具体的<strong>行为</strong> $a$ 对应，而不像马尔科夫奖励过程那样仅对应于某个状态</p><p>在马尔可夫决策过程中能够随意愿改变的变成了动作。我们关心的是在什么情况先该做什么动作</p><p><strong>例</strong></p><p>下图给出了一个可能的MDP的状态转化图。图中红色的文字表示的是采取的行为，而不是先前的状态名。对比之前的学生MRP示例可以发现，即时奖励与行为对应了，同一个状态下采取不同的行为得到的即时奖励是不一样的。由于引入了Action，容易与状态名混淆，因此此图没有给出各状态的名称；此图还把Pass和Sleep状态合并成一个终止状态；另外当选择”去酒吧”这个动作时，<strong>主动</strong>进入了一个临时状态（ 这样的状态不属于MDP中考虑的状态 ，图中用黑色小实点表示），随后<strong>被动的</strong>被环境按照其动力学分配到另外三个状态（正式的），也就是说Agent做出一个动作后没有权利选择决定去哪一个状态。</p><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_example.png" class="lazyload"><p> 注，图中除了在 Class4 状态上执行 去酒吧 动作外，其他的所有状态跳转都是确定性，我们通过在不同的状态上执行不同的动作，实现状态跳转。 </p><h3 id="策略Policy"><a href="#策略Policy" class="headerlink" title="策略Policy"></a>策略Policy</h3><p>策略 $\pi$ 是概率的集合或分布，其元素 $\pi(a|s)$  在某一状态$s$采取可能的行为$a$的概率。用$\pi(a|s)$  表示</p><script type="math/tex; mode=display">\pi ( a | s ) = \mathbb { P } \left[ A _ { t } = a | S _ { t } = s \right]</script><p>策略π有两个含义</p><ol><li>当为确定性策略的时候，$\pi$就等于$a$表示一个动作；</li><li>当策略不确定的时候，就表示一个动作分布。假设动作是有限的，我们就会写成一个向量，向量上的值对应相应动作的概率，该向量求和为1</li></ol><p>注意：</p><ul><li><p>策略是对智能体行为的全部描述，若策略给定，所有动作将被确定。</p></li><li><p>在MDPs中的策略是基于马尔科夫状态的（而不是基于历史的）</p></li><li><p>策略是时间稳定的，只和$s$有关，与时间$t$无关。</p></li><li><p>策略是RL问题的终极目标</p></li><li><p>如果策略的概率分布输出的是独立的，那么称为确定性策略，否则为随机策略。</p></li></ul><p><strong>MDP与MRP的关系</strong></p><p>当给定一个MDP: $M=(S,A,P,R,\gamma)$ 和一个策略$\pi$，那么状态序列$S_1,S_2,\cdots$  是一个马尔科夫过程$<s,p^{\pi}>$ ；同样，状态和奖励序列$S_1,R_2,S_2,R_3,S_3\cdots$   是一个马尔科夫奖励过程$<s,p^{\pi},r^{\pi},\gamma>$  ，并且在这个奖励函数和转移概率如下中满足下面两个方程：</s,p^{\pi},r^{\pi},\gamma></s,p^{\pi}></p><script type="math/tex; mode=display">\mathcal { P } _ { s , s ^ { \prime } } ^ { \pi } = \sum _ { a \in \mathcal { A } } \pi ( a | s ) \mathcal { P } _ { s s ^ { \prime } } ^ { a }</script><script type="math/tex; mode=display">\mathcal { R } _ { s } ^ { \pi } = \sum _ { a \in \mathcal { A } } \pi ( a | s ) \mathcal { R } _ { s } ^ { a }</script><p>在执行策略 $\pi$  时，状态从 $s$ 转移至 $s’$ 的概率等于一系列概率的和。因为到状态$s’$一定要做一个动作同时又跳转到了$s’$，所以这整个事件发生的概率就是在$s$状态执行了$a$动作的概率 乘 执行这个动作$a$正好跳转到了$s’$的概率 之和。奖励函数同理</p><p>策略在MDP中的作用相当于agent可以在某一个状态时做出选择，进而有形成各种马尔科夫过程的可能，而且基于策略产生的每一个马尔科夫过程是一个马尔科夫奖励过程，各过程之间的差别是不同的选择产生了不同的后续状态以及对应的不同的奖励。</p><h3 id="基于策略π的价值函数"><a href="#基于策略π的价值函数" class="headerlink" title="基于策略π的价值函数"></a>基于策略π的价值函数</h3><ul><li><strong>状态价值函数$v_\pi(s)$</strong>  ，表示从状态$s$开始，<strong>遵循当前策略</strong>时所获得的收获的期望；或者说在执行当前策略 $\pi$ 时，衡量个体处在状态 $s$ 时的价值大小。数学表示如下：</li></ul><script type="math/tex; mode=display">v _ { \pi } ( s ) = \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s \right]</script><ul><li><strong>行为价值函数$q_\pi(s,a)$</strong>，表示在执行策略 $\pi$ 时，对当前状态 $s$ 执行某一具体行为 $a$ 所能的到的收获的期望；或者说在遵循当前策略π时，衡量对当前状态执行行为a的价值大小。行为价值函数一般都是与某一特定的状态相对应的，更精细的描述是<strong>状态行为对</strong>价值函数。行为价值函数的公式描述如下：<script type="math/tex; mode=display">q _ { \pi } ( s , a ) = \mathbb { E } _ { \pi } \left[ G _ { t } | S _ { t } = s , A _ { t } = a \right]</script><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_example2.png" class="lazyload"></li></ul><p><strong>注:MDPs 中，任何不说明策略π 的情况下，讨论值函数都是在耍流氓！</strong></p><ul><li><p><strong>$v _ { \pi } ( s ) $和的$q _ { \pi } ( s , a ) $关系</strong></p><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_relation.png" class="lazyload"><p>上图中，空心较大圆圈表示状态，黑色实心小圆表示的是动作本身，连接状态和动作的线条仅仅把该状态以及该状态下可以采取的行为关联起来。可以看出，在遵循策略π时，状态s的价值体现为在该状态下遵循某一策略而采取所有可能行为的价值按行为发生概率的乘积求和。</p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \sum _ { a \in A } \pi ( a | s ) q _ { \pi } ( s , a )</script></li></ul>  <img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_relation2.png" class="lazyload"><p>  类似的，一个行为价值函数也可以表示成状态价值函数的形式：</p><script type="math/tex; mode=display">q _ { \pi } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { \pi } \left( s ^ { \prime } \right)</script><p>  它表明，一个某一个状态下采取一个行为的价值，可以分为两部分：其一是离开这个状态的价值，其二是所有进入新的状态的价值于其转移概率乘积的和。</p><p>  如果组合起来，可以得到下面的结果：</p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \sum _ { a \in \mathcal { A } } \pi ( a | s ) \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { \pi } \left( s ^ { \prime } \right) \right)</script>  <img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_relation3.png" class="lazyload"><p>  也可以得到下面的结果：</p><script type="math/tex; mode=display">q _ { \pi } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } \sum _ { a ^ { \prime } \in \mathcal { A } } \pi \left( a ^ { \prime } | s ^ { \prime } \right) q _ { \pi } \left( s ^ { \prime } , a ^ { \prime } \right)</script>  <img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_relation4.png" class="lazyload"><p>  例：<br>  下图解释了红色空心圆圈状态的状态价值是如何计算的，遵循的策略随机策略，即所有可能的行为有相同的几率被选择执行。</p>  <img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_relation5.png" class="lazyload"><h2 id="价值函数的推导-1"><a href="#价值函数的推导-1" class="headerlink" title="价值函数的推导"></a>价值函数的推导</h2><h3 id="Bellman期望方程"><a href="#Bellman期望方程" class="headerlink" title="Bellman期望方程"></a>Bellman期望方程</h3><p>和 MRP 相似，MDPs 中的值函数也能分解成瞬时奖励和后继状态的值函数两部分：</p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \mathbb { E } _ { \pi } \left[ R _ { t + 1 } + \gamma v _ { \pi } \left( S _ { t + 1 } \right) | S _ { t } = s \right]</script><script type="math/tex; mode=display">q _ { \pi } ( s , a ) = \mathbb { E } _ { \pi } \left[ R _ { t + 1 } + \gamma q _ { \pi } \left( S _ { t + 1 } , A _ { t + 1 } \right) | S _ { t } = s , A _ { t } = a \right]</script><h3 id="Bellman期望方程矩阵形式"><a href="#Bellman期望方程矩阵形式" class="headerlink" title="Bellman期望方程矩阵形式"></a>Bellman期望方程矩阵形式</h3><script type="math/tex; mode=display">v _ { \pi } = \mathcal { R } ^ { \pi } + \gamma \mathcal { P } ^ { \pi } v _ { \pi }</script><script type="math/tex; mode=display">v _ { \pi } = \left( l - \gamma \mathcal { P } ^ { \pi } \right) ^ { - 1 } \mathcal { R } ^ { \pi }</script><h2 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h2><p>之前值函数，以及贝尔曼期望方程针对的都是给定策略 $\pi$  的情况，是一个评价的问题（评价策略 $\pi$ 的好坏）。<br>现在来讨论强化学习中的优化问题，即找出最好的策略。</p><h3 id="最优价值函数"><a href="#最优价值函数" class="headerlink" title="最优价值函数"></a>最优价值函数</h3><p>最优值函数指的是在所有策略中的值函数最大值，其中包括最优 V 函 数和最优 Q 函数</p><ul><li><p>最优状态价值函数 $v_{*}(s)$  指的是在从所有策略产生的状态价值函数中，选取使状态 $s$ 价值最大的函数：</p><script type="math/tex; mode=display">{ v _ { * } ( s ) = \max _ { \pi } v _ { \pi } ( s ) }</script></li><li><p>最优行为价值函数 $q_*(s,a)$ 指的是从所有策略产生的行为价值函数中，选取是状态行为对  $<s,a>$  价值最大的函数：</s,a></p><script type="math/tex; mode=display">{ q _ { * } ( s , a ) = \max q _ { \pi } ( s , a ) }</script><p>最优价值函数明确了MDP的最优可能表现，当我们知道了最优价值函数，也就知道了每个状态的最优价值，这时便认为这个MDP获得了解决。</p></li><li><p>例：</p><p>学生MDP问题的最优状态价值</p><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_max1.png" class="lazyload"><p>学生MDP问题的最优行为价值</p><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_max2.png" class="lazyload"><p>注：youtube留言认为Pub行为对应的价值是+9.4而不是+8.4</p></li></ul><h3 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h3><ul><li><p><strong>定义</strong></p><p>当对于任何状态 $s$，遵循策略π的价值不小于遵循策略 $\pi’$ 下的价值，则策略 $\pi$ 优于策略 $\pi’$：</p><script type="math/tex; mode=display">\pi \geq \pi ^ { \prime } \text { if } v _ { \pi } ( s ) \geq v _ { \pi ^ { \prime } } ( s ) , \forall s</script><blockquote><p><strong>定理</strong><br>对于任何MDP，下面几点成立：<br>1.存在一个最优策略，比任何其他策略更好或至少相等；<br>2.所有的最优策略有相同的状态价值函数；<br>3.所有的最优策略具有相同的行为价值函数。</p></blockquote></li><li><p><strong>寻找最优策略</strong></p><p>可以通过最大化最优行为价值函数来找到最优策略：</p><script type="math/tex; mode=display">\pi _ { * } ( a | s ) = \left\{ \begin{array} { l l } { 1 } & { \text { if } a = \underset { a \in \mathcal { A } } { \operatorname { argmax } } q _ { * } ( s , a ) } \\ { 0 } & { \text { otherwise } } \end{array} \right.</script><p>对于任何MDP问题，总存在一个确定性的最优策略；同时如果我们知道最优行为价值函数，则表明我们找到了最优策略。</p></li><li><p><strong>学生MDP最优策略示例</strong></p><p>红色箭头表示的行为表示最优策略</p><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_max3.png" class="lazyload"></li></ul><h3 id="Bellman最优方程"><a href="#Bellman最优方程" class="headerlink" title="Bellman最优方程"></a>Bellman最优方程</h3><ul><li><p>定义</p><p>针对 $v_*$ ，一个状态的最优价值等于从该状态出发采取的所有行为产生的行为价值中最大的那个行为价值：</p><script type="math/tex; mode=display">v _ { * } ( s ) = \max _ { a } q _ { * } ( s , a )</script><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_max4.png" class="lazyload"><p>针对 $q _ { * }$ ，在某个状态$s$下，采取某个行为的最优价值由2部分组成，一部分是离开状态 s 的即刻奖励，另一部分则是所有能到达的状态 s’ 的最优状态价值按出现概率求和：</p><script type="math/tex; mode=display">q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)</script><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_max5.png" class="lazyload"></li></ul><p>  组合起来，针对 $v _ { * }$ ，有：</p><script type="math/tex; mode=display">  v _ { * } ( s ) = \max _ { a } \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)</script>  <img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_max6.png" class="lazyload"><p>  针对 $q _ { * }$ ，有：</p><script type="math/tex; mode=display">  q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } \max _ { a ^ { \prime } } q _ { * } \left( s ^ { \prime } , a ^ { \prime } \right)</script>  <img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_max7.png" class="lazyload"><ul><li><p><strong>Bellman最优方程学生MDP示例</strong></p><img title="This is an example image" data-src="/2019/11/22/强化学习基础2——强化学习问题描述/MDP_max8.png" class="lazyload"></li><li><p><strong>求解Bellman最优方程</strong></p><p>对于Bellman期望方程，可以使用矩阵求逆来求解</p><p>但Bellman最优方程是非线性的，没有固定的解决方案，通过一些迭代方法来解决：价值迭代、策略迭代、Q学习、Sarsa等。后续会逐步讲解展开。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>有任务依赖的卸载问题研究</title>
      <link href="/2019/11/21/%E6%9C%89%E4%BB%BB%E5%8A%A1%E4%BE%9D%E8%B5%96%E7%9A%84%E5%8D%B8%E8%BD%BD%E9%97%AE%E9%A2%98%E7%A0%94%E7%A9%B6/"/>
      <url>/2019/11/21/%E6%9C%89%E4%BB%BB%E5%8A%A1%E4%BE%9D%E8%B5%96%E7%9A%84%E5%8D%B8%E8%BD%BD%E9%97%AE%E9%A2%98%E7%A0%94%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="Computation-Offloading-in-Multi-Access-Edge-Computing-Using-a-Deep-Sequential-Model-Based-on-Reinforcement-Learning"><a href="#Computation-Offloading-in-Multi-Access-Edge-Computing-Using-a-Deep-Sequential-Model-Based-on-Reinforcement-Learning" class="headerlink" title="Computation Offloading in Multi-Access Edge Computing Using a Deep Sequential Model Based on Reinforcement Learning"></a>Computation Offloading in Multi-Access Edge Computing Using a Deep Sequential Model Based on Reinforcement Learning</h1><h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>任务模型：DAG表示任务依赖</p><p>可执行任务角色：MEC / 本地设备</p><p>执行调度角色：设备</p><p>任务执行方式：MEC服务器一个一个执行</p><p>假设： 所有子任务都能卸载</p><img title="This is an example image" data-src="/2019/11/21/有任务依赖的卸载问题研究/framework.png" class="lazyload"><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>最小化代价（例如：延迟（文中），能耗）</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>基于深度强化学习的方法</p><p>使用recurrent neural network(RNN)来编码任务信息，使用sequence-to-sequence neural network来为卸载策略建模，输入是已编码任务信息，使用Proximal Policy Optimization(PPO)方法来训练</p><p>强化学习三要素的定义：</p><ul><li>状态： $s=\left(\mathrm{G}, A_{1: i}\right)$， $G$为DAG图，$A_{1: i}$为前 $i$ 个卸载决策</li><li>动作集：$\mathcal{A}=\{1,0\}$，1为卸载，0为不卸载</li><li>奖励函数：当做一个决策后（是否卸载），延迟的负增长估计值</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ol><li><p>对比算法</p><p>穷举（Optimal）</p><p>贪婪（HEFT-based）——还引了一篇文章</p><p>轮询（Round-robin）</p></li><li><p>环境及参数设置</p><p>环境：tensorflow</p><p>任务传输数据大小：5KB到50KB</p><p>任务所需的CPU周期：10到100megacycles</p><p>任务集大小：100个DAG，每个DAG有10个任务</p><p>传输速率：1.2/7/20/30/60Mbps   7Mbps</p><p>计算能力：1GHz(设备)，10GHz(MEC)</p></li><li><p>实验过程</p><p>用参数产生DAG任务构成任务集，在不同传输速率下对比不同算法的延迟（柱状图）</p><p>用参数产生DAG任务构成任务集，任务集大小以5个步长递增，在不同任务集大小下对比不同算法的延迟（表格）</p></li></ol><h2 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h2><ol><li><p>使用马尔可夫决策过程来构建卸载问题</p><p>状态：任务的卸载决策和DAG图</p><p>动作：每个子任务的卸载决策</p><p>奖励函数：当做一个决策后（是否卸载），延迟的负增长估计值</p></li><li><p>提出了一个基于S2S的卸载算法框架</p><img title="This is an example image" data-src="/2019/11/21/有任务依赖的卸载问题研究/S2S.png" class="lazyload"></li></ol><h1 id="Knowledge-Driven-Service-Offloading-Decision-for-Vehicular-Edge-Computing-A-Deep-Reinforcement-Learning-Approach"><a href="#Knowledge-Driven-Service-Offloading-Decision-for-Vehicular-Edge-Computing-A-Deep-Reinforcement-Learning-Approach" class="headerlink" title="Knowledge-Driven Service Offloading Decision for Vehicular Edge Computing: A Deep Reinforcement Learning Approach"></a>Knowledge-Driven Service Offloading Decision for Vehicular Edge Computing: A Deep Reinforcement Learning Approach</h1><p>这篇论文提的概念是服务卸载，一个服务内有多个任务，任务之间有依赖性，而边缘结点只提供其中几个服务</p><h2 id="场景-1"><a href="#场景-1" class="headerlink" title="场景"></a>场景</h2><ul><li><p>可提供服务的角色：云（BS） / MEC（AP） / Ad-hoc对等车辆 （三层结构！）</p><p>BS具有大规模覆盖范围和高性能计算，通过3G / 4G接入对应的较远节点网络，影响QoS</p><p>AP覆盖范围有限，计算性能中等，如果太多的车辆访问同一无线同时网络，稀缺的带宽可能会影响应用程序的QoS</p></li><li><p>执行调度角色：设备</p><img title="This is an example image" data-src="/2019/11/21/有任务依赖的卸载问题研究/2_environment.png" class="lazyload"></li><li><p>任务模型：DAG（并行的任务算最长的那个任务的时间）</p></li><li><p>对卸载的描述：</p></li></ul><blockquote><p>The heterogeneous resources provided by vehicular edge computing nodes are abstracted to several containers with specific functions and parameters. The multi-task in a complicated application is modeled as a specific dataflow graph and denoted by DAG, depicted in Fig. 2. Due to the differences existing in the edge computing nodes, including several cloudlets, BS that contains computing and storage capability and the neighboring vehicles on the road, each task in a service has multiple offloading destinations.</p></blockquote>  <img title="This is an example image" data-src="/2019/11/21/有任务依赖的卸载问题研究/2_service.png" class="lazyload"><h2 id="目标-1"><a href="#目标-1" class="headerlink" title="目标"></a>目标</h2><p>最小化延迟</p><h2 id="架构-模型"><a href="#架构-模型" class="headerlink" title="架构 / 模型"></a>架构 / 模型</h2><h3 id="KD-service-offloading-decision-framework"><a href="#KD-service-offloading-decision-framework" class="headerlink" title="KD service offloading decision framework"></a>KD service offloading decision framework</h3><ul><li><p>决策模块(decision model)：DRL算法</p></li><li><p>观察函数(observation function)：根据计算能力排序这三类的所有可接入结点，作为候选卸载目标</p></li></ul><h3 id="构建的模型"><a href="#构建的模型" class="headerlink" title="构建的模型"></a>构建的模型</h3><h4 id="任务延迟"><a href="#任务延迟" class="headerlink" title="任务延迟"></a>任务延迟</h4><script type="math/tex; mode=display">D_{i}=\frac{f_{i}^{t}}{f_{c_{i}}}+\frac{d_{i}^{u}}   {b_{c_{i}}}+\frac{d_{i-1, i}}{b_{c_{i-1}, c_{i}}}</script><p>计算时间 + 任务$i-1$传输结果到$i$所需的时间 + 互动数据的传输时间</p><p>注：这里的计算量$f_{i}^{t}$是任务$i$的指令条数</p><h4 id="通信模型"><a href="#通信模型" class="headerlink" title="通信模型"></a>通信模型</h4><ul><li><p>BS（使用4G通信）</p><script type="math/tex; mode=display">b_{c_{i}}=w \log _{2} \frac{1+q_{v} g_{v, c_{i}}}{\varpi_{0}+\sum_{u \in U_{c_{i}}, u \neq v} q_{u} g_{u, c_{i}}}, c_{i} \in B S</script></li><li><p>AP（使用WLAN通信）</p><script type="math/tex; mode=display">b_{c_{i}}=\frac{h p_{t} L}{\left(1-p_{t}\right)\left(1+\tau_{s}\right)+\left[\left(1-p_{t}\right)^{1-h}- \left(1-p_{t}-h p_{t}\right)\right] p_{t}}, c_{i} \in AP</script><script type="math/tex; mode=display">p_{t}=\frac{2\left(1-2 p_{c}\right)}{\left(1-2 p_{c}\right)\left(W_{m i n}+1\right)+p_{c} W_{m i n}\left[1-\left(2 p_{c}\right)^{m_{b}}\right]}</script><p>$p_{t}$是设备在一个时隙传输一个包的概率</p></li></ul><h4 id="移动带来的性能开销"><a href="#移动带来的性能开销" class="headerlink" title="移动带来的性能开销"></a>移动带来的性能开销</h4><ul><li><p>BS/AP结点——切换时间</p><p>BS和AP可以根据车辆的移动，将卸载的任务迁移到新节点，这个时间叫<strong>切换时间</strong>（$D_h$）</p><p><strong>切换的次数</strong>由任务执行时间（假设为指数分布），在范围的滞留时间（符合一个概率分布$f_{res}(x)$），公式如下：</p><script type="math/tex; mode=display">N _ { c _ { i } } ^ { h } = \sum _ { k = 0 } ^ { \infty } k P ( k ) = \sum _ { k = 1 } ^ { \infty } \frac { k \mu _ { i } } { \eta _ { c _ { i } } } \left[ 1 - f _ { r e s } ^ { * } \left( \mu _ { i } \right) \right] ^ { 2 } \left[ f _ { r e s } ^ { * } \mu _ { i } \right] ^ { k - 1 }=\frac{\eta_{c_i}}{\mu_i}</script><p>一个任务的总切换时间为$D_hN_{c_i}^{h}$</p></li><li><p>车结点——任务重启时间</p><ol><li><p>使用了离散时间有限状态马尔可夫链为车距建模</p><p>状态：$X=\{x_1,x_2,x_3,\cdots,x\}$其中$x_i \in [z_{min},z_{max}]$是任务执行期间每个时间步长上，车与邻居结点的距离</p><p>状态转移概率：$X_j$在下一时间戳可转化为前一个状态($q _ { j }$)、当前状态($l_j$)、后一个状态($p _ { j }$)，转移概率公式为</p><script type="math/tex; mode=display">\begin{array} { l } { p _ { j } = p \left[ 1 - \beta \left( 1 - \frac { z _ { \min } + j z } { z _ { \max } } \right) \right] } \\ { q _ { j } = q \left[ 1 - \beta \left( 1 - \frac { \left( z _ { \min } + j z \right.} { z _ { \max } } \right) \right] } \\ { l _ { j } = 1 - p _ { j } - q _ { j } , 0 \leq p , q , \beta \leq 1 } \end{array}</script><p>其中 $p，q，\beta$ 根据车辆密度设定。直观的看如果$\beta$为0，则状态转移概率依赖于当前状态</p><p>单步状态转移概率矩阵：</p><script type="math/tex; mode=display">Q = \left( \begin{array} { c c c c c c } { l _ { 0 } } & { p _ { 0 } } & { 0 } & { \cdots } & { \cdots } & { 0 } \\ { q _ { 0 } } & { l _ { 1 } } & { p _ { 1 } } & { \cdots } & { \cdots } & { 0 } \\ { \cdots } & { \cdots } & { \cdots } & { \cdots } & { \cdots } & { 0 } \\ { 0 } & { 0 } & { 0 } & { \cdots } & { l _ { z ^ { * } } } & { 0 } \\ { 0 } & { 0 } & { 0 } & { \cdots } & { \cdots } & { 0 } \end{array} \right)</script><p>状态转移概率矩阵：</p><p>使用 $P_j^I$ 来表示状态$X_j$在第$I$步的状态转移概率。则状态转移概率为</p><script type="math/tex; mode=display">\pi(\zeta) = \left( P _ { 0 } ^ { \zeta } , P _ { 1 } ^ { \zeta } , P _ { 2 } ^ { \zeta } , \ldots , P _ { X _ { z } } ^ { \zeta } , \ldots , P _ { X _ { \max } } ^ { \zeta } \right)</script><p>$\zeta$ 为时间步数，$0 &lt; \zeta \leq \mathbb { 1 } / k \mu _ { i }$<br>递推公式为</p><script type="math/tex; mode=display">\pi ( \zeta ) = \pi ( 0 ) Q ^ { \zeta } = \pi ( 0 ) \times \underbrace { ( Q \times \cdots \times Q ) } _ { \zeta }</script></li><li><p>建模完成后，可得到车结点$c_i$对任务$i$的可用性为（是个概率）</p><script type="math/tex; mode=display">R _ { c _ { i } } = \sum _ { j = 0 } ^ { z ^ { * } } P _ { j } ^ { \zeta }</script><p>当车离开范围时，任务需重新执行，时间为$R _ { c _ { i } }\left( \frac { f _ { i } ^ { t } } { f ^ { c _ { i } = l o } } + \frac { d _ { i - 1 , i } } { b _ { i - 1 , c _ { i } = l o } } \right)$</p><h4 id="实际任务执行延迟"><a href="#实际任务执行延迟" class="headerlink" title="实际任务执行延迟"></a>实际任务执行延迟</h4></li></ol></li></ul><script type="math/tex; mode=display">D _ { i } ^ { \prime } = \left\{ \begin{array} { l l } { D _ { i } + D _ { h } N _ { c _ { i } } ^ { h } , } & { c _ { i } \in B S \cup A P } \\ { D _ { i } + R _ { c _ { i } } \left( \frac { f _ { i } ^ { t } } { f ^ { c _ { i } = l o } } + \frac { d _ { i - 1 , i } } { b _ { i - 1 , c _ { i } = l o } } \right) , } & { c _ { i } \in V N } \end{array} \right.</script><p>即在原$D_i$的基础上考虑了车辆移动所带来的影响：对于BS\AP，会有切换迁移时间；对于车结点，会有任务失败重新执行的时间</p><h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>使用A3C算法</p><blockquote><p>异步的优势行动者评论家算法（Asynchronous Advantage Actor-Critic，A3C）是Mnih等人根据异步强化学习（Asynchronous Reinforcement Learning， ARL） 的思想，提出的一种轻量级的 DRL 框架，该框架可以使用异步的梯度下降法来优化网络控制器的参数。基于AC框架，该框架集成了值函数估计算法和策略搜索算法。</p></blockquote><p>强化学习三要素的定义：</p><ul><li><p>状态：$S=(T,C,v)$</p><p>$T$：任务信息向量。$T=(f_i^t,d_i^u,d_{i-1,i})$，分别为计算量，互动信息大小，输入数据大小</p><p>$C$：目标卸载点信息。$C=(c_{type},f_{c_i},b{c_i},N_{c_i}^h,R_{c_i})$，分别为结点类型，计算速度，传输速率，切换时间，结点可用性</p><p>$v$：车辆行驶速度</p></li><li><p>动作集：$A=(a_{local},a_1,\cdots,a_m,\cdots)$，$a=1$表示卸载</p></li><li><p>奖励函数：真实世界的执行延迟（例如：$D_i’$）</p></li></ul><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><ol><li>对比算法</li></ol><ol><li><p>环境及参数设置</p><ul><li><p>结点信息：</p><p>|                                 | BS      | AP      | vehicle                      |<br>| ———————————————- | ———- | ———- | —————————————— |<br>| 结点个数                        | 2       | 2       | 6                            |<br>| 计算能力（标准差为5的正态分布） | 560,676 | 526,430 | 124, 120, 177, 144, 165, 130 |</p></li><li><p>带宽：</p><p>BS-&gt;BS/AP AP-&gt;AP/车：100M</p><p>BS-&gt;车：50M</p><p>车-&gt;车：300M</p></li><li><p>任务信息：</p><p>在docker中跑了对象检测，特征学习，图像自动注释，图像分割和基于位置的推荐 等应用</p><p>根据这些服务生成数据集，包括每个服务的任务信息。数据包含任务所需的CPU周期，传输数据量和相关性。此外，在实验中，两个或三个任务合并为一个任务，将它们部署在一个节点上。因此，实验中任务个数会从5到30</p></li><li><p>车辆移动数据</p><p>根据前面说的移动模型产生？？？？？</p></li></ul></li><li><p>实验过程</p><ul><li><p>车辆移动对卸载的影响（测试切换时间和重启时间）</p><p>滞留时间对切换时间的影响</p><p>任务执行时间对切换时间的影响</p><p>不同$q，\beta$（前面建模公式的参数） 对可用性的影响</p><p>最大通信方位对可用性的影响</p></li><li><p>DEL模型分析</p><p>收敛性</p><p>各种参数下的算法性能</p></li><li><p>服务延迟对比</p><p>对比算法：贪婪</p><p>任务大小对延迟的影响</p><p>任务个数对延迟的影响</p></li></ul></li></ol><h2 id="创新点-1"><a href="#创新点-1" class="headerlink" title="创新点"></a>创新点</h2><ol><li>提出了一种基于DRL的卸载决策模型，模型考虑了资源需求，接入网络，用户移动性和任务依赖性</li><li>构建了车辆移动性模型用于卸载决策，通过所访问的车辆边缘计算节点，来构建车辆移动对任务延迟的影响程度</li><li>使用A3C算法，实现移动车辆在线优化卸载决策</li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 任务卸载 </tag>
            
            <tag> 论文笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习基础1——强化学习问题提出</title>
      <link href="/2019/11/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%801%E2%80%94%E2%80%94%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E6%8F%90%E5%87%BA/"/>
      <url>/2019/11/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%801%E2%80%94%E2%80%94%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E6%8F%90%E5%87%BA/</url>
      
        <content type="html"><![CDATA[<p>强化学习问题可以从智能体和环境两方面来描述。</p><img title="This is an example image" data-src="/2019/11/21/强化学习基础1——强化学习问题提出/1.png" class="lazyload"><p>在 $t$ 时刻，智能体和环境分别可以：</p><div class="table-container"><table><thead><tr><th>智能体</th><th>环境</th></tr></thead><tbody><tr><td>有一个对于环境的观察评估$O_t$<br>做出一个行为$A_t$<br><br>从环境得到一个奖励信号 $R_{t+1}$</td><td><br>接收个体的动作 $A_t$，更新环境信息<br>给个体一个奖励信号 $R_{t+1}$<br><br>同时使得个体可以得到下一个观测$O_{t+1}$<br></td></tr></tbody></table></div><h1 id="序列决策问题-Sequential-Decision-Making"><a href="#序列决策问题-Sequential-Decision-Making" class="headerlink" title="序列决策问题 Sequential Decision Making"></a>序列决策问题 Sequential Decision Making</h1><p>强化学习解决的是序列决策问题，即环境状态-&gt;行动-&gt;新的环境状态-&gt;下一步行动的决策</p><p>决策的目标：选择一定的行为系列以<strong>最大化未来的总体奖励</strong>，这些行为可能是一个长期的序列，奖励可能而且通常是延迟的，有时候宁愿牺牲即时（短期）的奖励以获取更多的长期奖励</p><p>比如：下棋时，没有到最后一刻都不知道棋局结果，而要决策的是一系列下棋的位置</p><h1 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h1><h2 id="环境-Environment"><a href="#环境-Environment" class="headerlink" title="环境 Environment"></a>环境 Environment</h2><h3 id="奖励-Reward"><a href="#奖励-Reward" class="headerlink" title="奖励 Reward"></a>奖励 Reward</h3><p>$R_t$ 是信号的反馈，是一个标量，它反映智能体在 $t$ 时刻做得怎么样。个体的工作就是最大化累计奖励。<br>比如：Flappy bird可把奖励定义为当前做完该行动的得分</p><blockquote><p>强化学习主要基于这样的”奖励假设”：所有问题解决的目标都可以被描述成最大化累积奖励。</p></blockquote><h2 id="环境与智能体交互"><a href="#环境与智能体交互" class="headerlink" title="环境与智能体交互"></a>环境与智能体交互</h2><h3 id="历史-History"><a href="#历史-History" class="headerlink" title="历史 History"></a>历史 History</h3><p>历史是观测、行为、奖励的序列： $H_{t}=O_{1}, A_{1}, R_{1}, \ldots, O_{t-1}, A_{t-1}, R_{t-1}, O_{t}, A_{t}, R_{t}$</p><h3 id="状态-State（重要）"><a href="#状态-State（重要）" class="headerlink" title="状态 State（重要）"></a>状态 State（重要）</h3><p>状态是所有决定将来行为的已有的信息，是对历史信息的总结，通常我们研究的是State而不是HIstory， 是关于历史的一个函数 $S_{t}=f\left(H_{t}\right)$</p><p>状态只有经过最后一次观测才能是合理有效的</p><p>状态的三种定义：</p><ol><li><p>环境状态</p><p>是环境的私有呈现，真实环境抽象的某些数字集合，理解真实环境。包括环境用来决定下一个观测/奖励的所有数据，通常对个体并不完全可见，也就是个体有时候并不知道环境状态的所有细节。即使有时候环境状态 $S_{t}^{e}$ 对个体可以是完全可见的，这些信息也可能包含着一些无关信息。</p></li><li><p>智能体状态</p><p>是智能体的内部呈现，包括智能体可以使用的、决定未来动作的所有信息。智能体状态是强化学习算法可以利用的信息，它可以是历史的一个函数：$S_{t}^{a}=f\left(H_{t}\right)$</p></li><li><p>信息状态（用数学语言对状态进行定义）</p><p>包括历史上所有<strong>有用</strong>的信息，又称Markov状态</p><p>在我们使用状态表示法的时候，这些状态包含history的全部有用信息，我们能做的就是定义一个状态表示，使状态具有马尔可夫性</p></li></ol><blockquote><p>马尔可夫性</p><p>指系统的下一个状态$s_{t+1}$仅与当前状态$s_t$有关，与过去没多大关系）。$状态s_t是马尔科夫的\Leftrightarrow P[s_{t+1} | s_t]=P[s_{t+1}|s_1,…,s2]$</p><p>即$H_{1:t}$</p></blockquote><p>也就是说，如果信息状态是可知的，那么所有历史信息都可以丢掉，仅需要$t$时刻的信息状态就可以了。例如：环境状态是Markov的，因为环境状态是环境包含了环境决定下一个观测/奖励的所有信息；同样，（完整的）历史$H_t$也是马尔可夫的，即</p><script type="math/tex; mode=display">H_t \to S_t \to H_{t+1: \infty}</script><p>例：</p><p>有如下三个针对老鼠的事件序列，其中前两个最后的事件分别是老鼠遭电击和获得一块奶酪，现在请分析比较这三个事件序列的特点，分析第第三个事件序列中，老鼠是获得电击还是奶酪？</p><img title="This is an example image" data-src="/2019/11/21/强化学习基础1——强化学习问题提出/例.png" class="lazyload"><ul><li>假如个体状态 = 序列中的后三个事件（不包括电击、获得奶酪，下同），事件序列3的结果会是什么？（电击）</li><li>假如个体状态 = 亮灯、响铃和拉电闸各自事件发生的次数，那么事件序列3的结果又是什么？（奶酪）</li><li>假如个体状态 = 完整的事件序列，那结果又是什么？（未知）</li></ul><p>状态表示某种程度上决定了未来会发生什么。我们可以使用各种各样不同的方式来表征我们的状态</p><ul><li><strong>完全可观测的环境 Fully Observable Environments</strong></li></ul><p>智能体能够直接观测到环境状态。在这种条件下:</p><script type="math/tex; mode=display">智能体对环境的观测O_t = 个体状态S_t^a = 环境状态S_t^e</script><p>这种问题是一个<strong>马尔可夫决策过程</strong>（Markov Decision Process， MDP）</p><ul><li><strong>部分可观测的环境 Partially Observable Environments</strong></li></ul><p>智能体间接观测环境。几个例子：</p><ol><li>一个可拍照的机器人个体对于其周围环境的观测并不能说明其绝度位置，它必须自己去估计自己的绝对位置，而绝对位置则是非常重要的环境状态特征之一；</li><li>一个交易员只能看到当前的交易价格；</li><li>一个扑克牌玩家只能看到自己的牌和其他已经出过的牌，而不知道整个环境（包括对手的牌）状态。</li></ol><p>在这种条件下： </p><script type="math/tex; mode=display">智能体状态 ≠ 环境状态</script><p>这种问题是一个<strong>部分可观测马儿可夫决策过程</strong>。智能体必须构建它自己的状态呈现形式。</p><p>比如：记住完整的历史，整个动作序列是一个状态：$S_t^a = H_t$ 。这种方法比较原始、幼稚。</p><p>还有其他办法，例如 ：</p><ol><li><p>Beliefs of environment state：此时虽然智能体不知道环境状态到底是什么样，但智能体可以利用已有经验（数据），用各种个体已知状态的概率分布作为当前时刻的智能体状态的呈现：</p><script type="math/tex; mode=display">S_{t}^{a}=\left(\mathbb{P}\left[S_{t}^{e}=s^{1}\right], \ldots, \mathbb{P}\left[S_{t}^{e}=s^{n}\right]\right)</script></li><li><p>Recurrent neural network：不需要知道概率，只需要将最近的智能体状态与最近的观测结合起来，送入循环神经网络(RNN)中得到一个当前智能体状态的呈现：</p></li></ol><script type="math/tex; mode=display">S_{t}^{a}=\sigma\left(S_{t-1}^{a} W_{s}+O_{t} W_{o}\right)</script><h2 id="智能体-Agent"><a href="#智能体-Agent" class="headerlink" title="智能体 Agent"></a>智能体 Agent</h2><p>智能体可以由以下三个组成部分中的一个或多个组成：</p><h3 id="策略-Policy"><a href="#策略-Policy" class="headerlink" title="策略 Policy"></a>策略 Policy</h3><p>策略是决定智能体行为的机制。是从状态到行为的一个映射，可以是确定性的$a = \pi(s)$，也可以是不确定性的$\pi(a|s)=P[A=a|S=s]$。</p><p>比如：在看电视这个状态，做作业的概率是0.3，出去玩的概率是0.7。就是一个策略</p><h3 id="价值函数-Value-Function"><a href="#价值函数-Value-Function" class="headerlink" title="价值函数 Value Function"></a>价值函数 Value Function</h3><p>是一个未来奖励的预测，用来评价当前状态的好坏程度。当面对两个不同的状态时，智能体可以用一个Value值来评估这两个状态可能获得的最终奖励区别，继而指导选择不同的行为，即制定不同的策略。同时，一个价值函数是基于某一个特定策略的，不同的策略下同一状态的价值并不相同。</p><h3 id="模型-Model"><a href="#模型-Model" class="headerlink" title="模型 Model"></a>模型 Model</h3><p>智能体对环境的一个建模，它体现了个体是如何思考环境运行机制的（如何理解环境，模拟环境），个体希望模型能模拟环境与个体的交互机制。</p><p>模型至少要解决两个问题：</p><p>一是状态转化概率，即预测下一个可能状态发生的概率</p><script type="math/tex; mode=display">\mathcal{P}_{s s^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} | S_{t}=s, A_{t}=a\right]</script><p>即状态是$s$且执行了$a$操作的条件下下一个状态是$s^{\prime}$的概率</p><p>另一项工作是预测可能获得的即时奖励</p><script type="math/tex; mode=display">\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} | S_{t}=s, A_{t}=a\right]</script><p>即到目前为止，出现了状态是$s$后执行了$a$操作的情况，所获得的平均奖励</p><blockquote><p>注意： </p><ol><li>模型并不是构建一个个体所必需的，很多强化学习算法中个体并不依赖模型。</li><li>模型仅针对个体而言，环境实际运行机制不称为模型，而称为环境动力学(dynamics of environment)，它能够明确确定个体下一个状态和所得的即时奖励。</li></ol></blockquote><p>例：<br>在 Flappy bird 这个游戏中，需要简单的点击操作来控制小鸟，躲过各种水管，飞的越远越好，因为飞的越远就能获得更高的积分奖励。</p><p>这就是一个典型的强化学习场景：</p><ul><li>小鸟角色——智能体</li><li>水管排布——环境</li><li>控制小鸟飞的更远——决策目标</li><li>$t$时刻水管的排布——环境的观察评估$O_t$</li><li>$t$小鸟飞——行动$A_t$</li><li>$t$时刻是否撞柱子——奖励信号 $R_t$</li></ul><blockquote><p>根据智能体的三个组成部分分类</p><p>是否有策略/价值函数</p><ul><li>Value Based<br>没有策略，靠价值函数选择，存储的是价值函数</li><li>policay Based<br>没有价值函数，靠策略选择，存储的是策略</li><li>Actor Critic<br>前面两者的结合，既有策略，也有价值函数</li></ul><p>是否有模型</p><ul><li>Model Free：没有模型，有策略或/和价值函数</li><li>Model Based：有模型，有策略或/和价值函数</li></ul></blockquote><h2 id="学习和规划-Learning-amp-Planning"><a href="#学习和规划-Learning-amp-Planning" class="headerlink" title="学习和规划 Learning &amp; Planning"></a>学习和规划 Learning &amp; Planning</h2><ul><li>学习：环境初始时是未知的，个体不知道环境如何工作，个体通过与环境进行交互，逐渐改善其行为策略。</li><li>规划: 环境如何工作对于个体是已知或近似已知的，个体并不与环境发生实际的交互，而是利用其构建的模型进行计算，在此基础上改善其行为策略。</li></ul><p>一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。</p><h2 id="探索和利用-Exploration-amp-Exploitation"><a href="#探索和利用-Exploration-amp-Exploitation" class="headerlink" title="探索和利用 Exploration &amp; Exploitation"></a>探索和利用 Exploration &amp; Exploitation</h2><p>强化学习类似于一个试错的学习，个体需要从其与环境的交互中发现一个好的策略，同时又不至于在试错的过程中丢失太多的奖励。探索和利用是个体进行决策时需要平衡的两个方面。</p><p>一个形象的比方是，当你去一个餐馆吃饭，“探索”意味着你对尝试新餐厅感兴趣，很可能会去一家以前没有去过的新餐厅体验，“利用”则意味着你就在以往吃过的餐厅中挑一家比较喜欢的，而不去尝试以前没去过的餐厅。这两种做法通常是一对矛盾，但对解决强化学习问题又都非常重要。</p><p>其它一些例子，在线广告推广时，显示最受欢迎的广告和显示一个新的广告；油气开采时选择一个已知的最好的地点同在未知地点进行开采；玩游戏时选择一个你认为最好的方法同实验性的采取一个新的方法。</p><h2 id="预测和控制-Prediction-amp-Control"><a href="#预测和控制-Prediction-amp-Control" class="headerlink" title="预测和控制 Prediction &amp; Control"></a>预测和控制 Prediction &amp; Control</h2><p>在强化学习里，我们经常需要先解决关于预测（prediction）的问题，而后在此基础上解决关于控制（Control）的问题。</p><ul><li>预测：给定一个策略，评价未来。可以看成是求解在给定策略下的价值函数（value function）的过程。How well will I(an agent) do if I(the agent) follow a specific policy?</li><li>控制：找到一个好的策略来最大化未来的奖励。</li></ul><p>举了一个例子来说明预测和控制的区别。</p><p>预测：现在给出了从A到A’的奖励以及从B到B’的奖励，在“随机选择4个方向进行移动”的策略下，如何得知每一个位置的价值。</p><p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-d5bc46627629fa140851d1ffa2373c15_hd.png" class="lazyload"></p><p>控制：同样的条件，在所有可能的策略下最优的价值函数是什么？最优策略是什么？</p><p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-14eb771da4f44787cf3ab3174163187c_hd.png" class="lazyload"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>强化学习解决的序列决策问题，智能体可由三个部分组成：策略、价值函数、模型。环境会响应智能体的行为，给予奖励。智能体和环境的交互过程中会产生由【观察，行为，奖励】构成的序列称为历史，智能体将历史概括为状态。当环境完全可观测时，观察值=个体状态=环境状态，为一个马尔可夫决策过程；当环境部分可观测时，观察值≠环境状态，需要构建自己状态的呈现方式（通过RNN等方式构建），为部分可观测马尔可夫决策过程</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习导论</title>
      <link href="/2019/11/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/"/>
      <url>/2019/11/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习是什么"><a href="#强化学习是什么" class="headerlink" title="强化学习是什么"></a>强化学习是什么</h1><p>强化学习(Reinforcement Learning, RL)是机器学习的一个重要分支：监督学习、无监督学习、强化学习，又称为增强学习，模拟的是人类的一种学习方式，在执行某个动作或决策后根据执行效果来获得奖励，通过不断与环境的交互进行学习，最终达到目标。即强调如何基于环境而行动，以取得最大化的预期利益。</p><p>其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。</p><p>主要包含四个元素，<strong>智能体（agent），状态，行动，奖励</strong>， 强化学习的目标就是获得最多的累计奖励 </p><blockquote><p>强化学习与监督学习、无监督学习的区别</p><p>强化学习不同于 <em>监督学习</em>。 监督学习是目前机器学习领域中研究最多的一种学习方式，它从知识渊博的教练所提供的有标记的训练集中学习。 每一个样例都是一种情况的描述，都带有标签，标签描述的是系统在该情况下的应该采取的正确动作，每一个样例用来识别这种情况应该属于哪一类。 这种学习的目的是让系统推断概括它应有的反馈机制，使它可以对未知样本作出正确回应。 这是一种重要的学习方式，但单凭它并不足以从交互中学习。 在交互问题中，找到期待的既正确又典型的样例通常都是不切实际的。 在一个未知的领域，若要使收益最大化，个体必须能够从自己的经验中学习。</p><p>强化学习也与机器学习研究人员所谓的 <em>无监督学习</em> 不同，后者通常是用于寻找隐藏在未标记数据集合中的结构。 监督学习和无监督学习这两个术语似乎对机器学习范式进行了详尽的分类，但事实却并非如此。 尽管人们可能会试图将强化学习视为一种无监督学习，因为它不依赖于正确行为的样例，但强化学习试图最大化奖励信号而不是试图找到隐藏的结构。 在个体的经验数据中揭示结构确实对强化学习特别有用，但是它本身并没有解决最大化奖励信号的强化学习问题。 因此，我们认为强化学习是除监督学习和无监督学习之外的第三种机器学习范式，也许还有其他范式。</p></blockquote><h1 id="强化学习的特点"><a href="#强化学习的特点" class="headerlink" title="强化学习的特点"></a>强化学习的特点</h1><p>强化学习是一种学习如何将状态映射到动作，以获得最大奖励的学习机制。 学习者不会被告知要采取哪些动作，而是必须通过尝试来发现哪些动作会产生最大的回报。 动作不仅可以影响直接奖励，还可以影响下一个状态，并通过下一个状态，影响到随后而来的奖励。 这两个特征 - <strong>试错法和延迟奖励</strong> - 是强化学习的两个最重要的可区别特征。 </p><ol><li>没有监督数据、只有奖励信号</li><li>奖励信号不一定是实时的，而很可能是延后的，有时甚至延后很多。</li><li>时间（序列）是一个重要因素</li><li>当前的行为影响后续接收到的数据</li></ol><h1 id="发展历程："><a href="#发展历程：" class="headerlink" title="发展历程："></a>发展历程：</h1><ul><li>1956年 Bellman提出了动态规划方法。</li><li>1977年 Werbos提出只适应动态规划算法。</li><li>1988年 sutton提出时间差分算法。</li><li>1992年 Watkins 提出Q-learning 算法。</li><li>1994年 rummery 提出Saras算法。</li><li>1996年 Bersekas提出解决随机过程中优化控制的神经动态规划方法。</li><li>2006年 Kocsis提出了置信上限树算法。</li><li>2009年 kewis提出反馈控制只适应动态规划算法。</li><li>2014年 silver提出确定性策略梯度（Policy Gradients）算法。</li><li>2015年 Google-deepmind 提出Deep-Q-Network算法。</li></ul><h1 id="强化学习的应用"><a href="#强化学习的应用" class="headerlink" title="强化学习的应用"></a>强化学习的应用</h1><p>直升机特技飞行、经典游戏、投资管理、发电站控制、让机器人模仿人类行走等</p><h1 id="强化学习解决什么问题"><a href="#强化学习解决什么问题" class="headerlink" title="强化学习解决什么问题"></a>强化学习解决什么问题</h1><p>强化学习试图解决决策优化的问题。即面对特定状态（State,S），采取什么行动方案（Action,A）， 通过选择一定的行动系列，使得未来的总体奖励最大（Reward,R）。比如：下棋、投资、课程安排、驾车</p><p>例：在 Flappy bird 这个游戏中，我们需要简单的点击操作来控制小鸟，躲过各种水管，飞的越远越好，因为飞的越远就能获得更高的积分奖励。面对特定的水管排布选择“飞”这个动作，通过选择一系列动作（如：飞|不飞|飞|飞）来使小鸟飞得更远获得更多的分</p><blockquote><p>强化学习的本质是在大空间中寻找最优解，是介于穷举和贪婪之间的一种探索机制，这种探索机制是学习得来的。<br>“介于穷举和贪婪之间”：不用穷举可以减少搜索量，提升速度；不用贪婪可以尽量避免局部最优解。因此可以说是在计算时间与全局最优之间的权衡。</p></blockquote><h1 id="强化学习算法分类"><a href="#强化学习算法分类" class="headerlink" title="强化学习算法分类"></a>强化学习算法分类</h1><h2 id="强化学习的几个元素的角度划分"><a href="#强化学习的几个元素的角度划分" class="headerlink" title="强化学习的几个元素的角度划分"></a>强化学习的几个元素的角度划分</h2><ul><li>Policy based, 关注点是找到最优策略</li><li>Value based, 关注点是找到最优奖励总和</li><li>Action based, 关注点是每一步的最优行动</li></ul><h2 id="其他角度划分"><a href="#其他角度划分" class="headerlink" title="其他角度划分"></a>其他角度划分</h2><ol><li>基于理不理解所处环境来进行分类：</li></ol><ul><li><p><strong>Model-free</strong>：环境给了我们什么就是什么. 我们就把这种方法叫做 model-free, 这里的 model 就是用模型来表示环境</p></li><li><p><strong>Model-based</strong>：那理解了环境也就是学会了用一个模型来代表环境, 所以这种就是 model-based 方法</p><img title="This is an example image" data-src="/2019/11/08/强化学习导论/Model-Free-RL.png" class="lazyload"><img title="This is an example image" data-src="/2019/11/08/强化学习导论/Model-Based-RL.png" class="lazyload"></li></ul><ol><li>基于是否直接输出各个动作概率进行分类：</li></ol><ul><li><p><strong>Policy based</strong>：通过感官分析所处的环境, 直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动。</p></li><li><p><strong>Value based</strong>：输出的是所有动作的价值, 根据最高价值来选动作，这类方法不能选取连续的动作。</p><img title="This is an example image" data-src="/2019/11/08/强化学习导论/Policy-Based-RL.png" class="lazyload"><img title="This is an example image" data-src="/2019/11/08/强化学习导论/Value-Based-RL.png" class="lazyload"></li></ul><ol><li>是否单步更新分类：</li></ol><ul><li><p><strong>Monte-carlo update</strong>：游戏开始后, 要等待游戏结束, 然后再总结这一回合中的所有转折点, 再更新行为准则。</p></li><li><p><strong>Temporal-difference update</strong>：在游戏进行中每一步都在更新, 不用等待游戏的结束, 这样就能边玩边学习了。</p> <img title="This is an example image" data-src="/2019/11/08/强化学习导论/Monte-Carlo-update.png" class="lazyload"><img title="This is an example image" data-src="/2019/11/08/强化学习导论/Temporal-Difference-update.png" class="lazyload"></li></ul><ol><li>基于是否在线学习分类：</li></ol><ul><li><p><strong>On-policy</strong>：必须本人在场, 并且一定是本人边玩边学习。</p></li><li><p><strong>Off-policy</strong>：可以选择自己玩, 也可以选择看着别人玩, 通过看别人玩来学习别人的行为准则。</p><img title="This is an example image" data-src="/2019/11/08/强化学习导论/On-Policy.png" class="lazyload"><img title="This is an example image" data-src="/2019/11/08/强化学习导论/Off-Policy.png" class="lazyload"></li></ul><h1 id="代表性算法"><a href="#代表性算法" class="headerlink" title="代表性算法"></a>代表性算法</h1><h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>Q-learning定义动作效用函数（action-utility function），用于判断在特定状态下采取某个动作的优劣，可以将之理解为智能体（Agent）的大脑。</p><p>Q-learning 只利用了下一步信息, 让系统按照策略指引进行探索，在探索每一步都使用更新公式进行状态价值的更新</p><h2 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h2><p>同Q-learning只是更新公式不同</p><h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><p>DQN（Deep Q Network）是一种融合了神经网络和Q learning的方法.</p><p>有些问题太复杂，Q表无法存储，即使可以存储，搜索也很麻烦。故而，将Q表用神经网络进行替代。</p><h2 id="Policy-Gradients"><a href="#Policy-Gradients" class="headerlink" title="Policy Gradients"></a>Policy Gradients</h2><h2 id="Actor-critic"><a href="#Actor-critic" class="headerlink" title="Actor-critic"></a>Actor-critic</h2><p> 算法分为两个部分：Actor 和 Critic。Actor 更新概率， Critic 更新价值。Critic 就可以用之前介绍的 SARSA 或者 Q Learning 算法。 是基于价值和基于概率的结合体</p><h1 id="如何用强化算法解决问题"><a href="#如何用强化算法解决问题" class="headerlink" title="如何用强化算法解决问题"></a>如何用强化算法解决问题</h1><ol><li><p>将实际问题建模成马尔可夫决策过程，抽象出五元组（状态集、动作集、状态转移概率、奖励函数、折扣因子），  其中奖励与实际目标相关联</p></li><li><p>根据动作是否连续选择对应的算法 </p><p>​    动作离散：DQN   </p><p>​    动作连续：Policy Gradients，Actor-Critic，DDPG</p></li><li><p>根据算法写代码</p></li></ol><h1 id="强化学习资料"><a href="#强化学习资料" class="headerlink" title="强化学习资料"></a>强化学习资料</h1><p>强化学习课程</p><blockquote><p>【推荐】David Silver 的 UCL 强化学习课程：<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a><br>DeepMind 和 UCL 的深度学习和强化学习课程：<a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs</a><br>Sergey Levine 教授的深度强化学习课程：<a href="http://rail.eecs.berkeley.edu/deeprlcourse/" target="_blank" rel="noopener">http://rail.eecs.berkeley.edu/deeprlcourse/</a><br>OpenAI 的 Spinning Up in Deep RL：<a href="https://blog.openai.com/spinning-up-in-deep-rl/" target="_blank" rel="noopener">https://blog.openai.com/spinning-up-in-deep-rl/</a></p></blockquote><p>书籍</p><blockquote><p>Sutton 和 Barto 的强化学习著作《Reinforcement learning: an introduction》：<a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">http://incompleteideas.net/book/the-book-2nd.html</a><br>一本有关深度强化学习的书籍草稿：<a href="https://arxiv.org/abs/1810.06339" target="_blank" rel="noopener">https://arxiv.org/abs/1810.06339</a><br>Richard Sutton 和 Andrew Barto的《强化学习：简介（第二版）》：<a href="http://incompleteideas.net/book/RLbook2018.pdf" target="_blank" rel="noopener">http://incompleteideas.net/book/RLbook2018.pdf</a><br> 郭宪博士2017年写的《深入浅出强化学习：原理入门》，入门级别的书，语言通俗易懂</p></blockquote><p>学习路线</p><p> <a href="https://github.com/dennybritz/reinforcement-learning" target="_blank" rel="noopener">https://github.com/dennybritz/reinforcement-learning</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo常用命令</title>
      <link href="/2019/10/04/Hexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2019/10/04/Hexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>详细使用文档参见 <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a> 官网</p><p>[TOC]</p><h1 id="新建网站-init"><a href="#新建网站-init" class="headerlink" title="新建网站 init"></a>新建网站 init</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init [路径]</span><br></pre></td></tr></table></figure><p>新建一个网站。如果没有设置 <code>folder</code> ，Hexo 默认在当前文件夹建立网站。</p><h1 id="新建文章-new"><a href="#新建文章-new" class="headerlink" title="新建文章 new"></a>新建文章 new</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new [布局][参数] &quot;文章名字&quot;</span><br></pre></td></tr></table></figure><p>新建一篇文章。如果没有设置 <code>layout</code> 的话，默认使用 <a href="https://hexo.io/zh-cn/docs/configuration" target="_blank" rel="noopener">_config.yml</a> 中的 <code>default_layout</code> 参数代替</p><h1 id="生成静态网站-generate"><a href="#生成静态网站-generate" class="headerlink" title="生成静态网站 generate"></a>生成静态网站 generate</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>生成静态文件。</p><div class="table-container"><table><thead><tr><th style="text-align:left">选项</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left"><code>-d</code>, <code>--deploy</code></td><td style="text-align:left">文件生成后立即部署网站</td></tr><tr><td style="text-align:left"><code>-w</code>, <code>--watch</code></td><td style="text-align:left">监视文件变动</td></tr><tr><td style="text-align:left"><code>-b</code>, <code>--bail</code></td><td style="text-align:left">生成过程中如果发生任何未处理的异常则抛出异常</td></tr><tr><td style="text-align:left"><code>-f</code>, <code>--force</code></td><td style="text-align:left">强制重新生成文件 Hexo 引入了差分机制，如果 <code>public</code> 目录存在，那么 <code>hexo g</code> 只会重新生成改动的文件。 使用该参数的效果接近 <code>hexo clean &amp;&amp; hexo generate</code></td></tr></tbody></table></div><p>该命令可以简写为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure><h1 id="发布草稿publish"><a href="#发布草稿publish" class="headerlink" title="发布草稿publish"></a>发布草稿publish</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo publish [layout] &lt;filename&gt;</span><br></pre></td></tr></table></figure><p>发表草稿。</p><h1 id="启动服务器-server"><a href="#启动服务器-server" class="headerlink" title="启动服务器 server"></a>启动服务器 server</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>启动服务器。默认情况下，访问网址为： <code>http://localhost:4000/</code>。</p><div class="table-container"><table><thead><tr><th style="text-align:left">选项</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left"><code>-p</code>, <code>--port</code></td><td style="text-align:left">重设端口</td></tr><tr><td style="text-align:left"><code>-s</code>, <code>--static</code></td><td style="text-align:left">只使用静态文件</td></tr><tr><td style="text-align:left"><code>-l</code>, <code>--log</code></td><td style="text-align:left">启动日记记录，使用覆盖记录格式</td></tr></tbody></table></div><h1 id="部署网站deploy"><a href="#部署网站deploy" class="headerlink" title="部署网站deploy"></a>部署网站deploy</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>部署网站。</p><div class="table-container"><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left"><code>-g</code>, <code>--generate</code></td><td style="text-align:left">部署之前预先生成静态文件</td></tr></tbody></table></div><p>该命令可以简写为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><h1 id="清除缓存-clean"><a href="#清除缓存-clean" class="headerlink" title="清除缓存 clean"></a>清除缓存 clean</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure><p>清除缓存文件 (<code>db.json</code>) 和已生成的静态文件 (<code>public</code>)。</p><p>在某些情况（尤其是更换主题后），如果发现对站点的更改无论如何也不生效，可能需要运行该命令。</p><h1 id="查看版本-version"><a href="#查看版本-version" class="headerlink" title="查看版本 version"></a>查看版本 version</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo version</span><br></pre></td></tr></table></figure><p>显示 Hexo 版本。</p><h1 id="各种模式"><a href="#各种模式" class="headerlink" title="各种模式"></a>各种模式</h1><h2 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo --safe</span><br></pre></td></tr></table></figure><p>在安全模式下，不会载入插件和脚本。当您在安装新插件遭遇问题时，可以尝试以安全模式重新执行。</p><h2 id="调试模式"><a href="#调试模式" class="headerlink" title="调试模式"></a>调试模式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo --debug</span><br><span class="line">$ hexo s --debug  #以debug模式运行</span><br></pre></td></tr></table></figure><p>在终端中显示调试信息并记录到 <code>debug.log</code>。当您碰到问题时，可以尝试用调试模式重新执行一次</p><h2 id="简洁模式"><a href="#简洁模式" class="headerlink" title="简洁模式"></a>简洁模式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo --silent</span><br></pre></td></tr></table></figure><p>隐藏终端信息。</p><h1 id="显示草稿"><a href="#显示草稿" class="headerlink" title="显示草稿"></a>显示草稿</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo --draft</span><br></pre></td></tr></table></figure><p>显示 <code>source/_drafts</code> 文件夹中的草稿文章。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo文章操作详解</title>
      <link href="/2019/10/04/Hexo%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3/"/>
      <url>/2019/10/04/Hexo%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="网站配置"><a href="#网站配置" class="headerlink" title="网站配置"></a>网站配置</h1><p>网站根目录下的 <code>_config.yml</code> 中修改大部分的配置</p><h2 id="网站"><a href="#网站" class="headerlink" title="网站"></a>网站</h2><div class="table-container"><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left"><code>title</code></td><td style="text-align:left">网站标题</td></tr><tr><td style="text-align:left"><code>subtitle</code></td><td style="text-align:left">网站副标题</td></tr><tr><td style="text-align:left"><code>description</code></td><td style="text-align:left">网站描述</td></tr><tr><td style="text-align:left"><code>keywords</code></td><td style="text-align:left">网站的关键词。使用半角逗号 <code>,</code> 分隔多个关键词。</td></tr><tr><td style="text-align:left"><code>author</code></td><td style="text-align:left">您的名字</td></tr><tr><td style="text-align:left"><code>language</code></td><td style="text-align:left">网站使用的语言</td></tr><tr><td style="text-align:left"><code>timezone</code></td><td style="text-align:left">网站时区。Hexo 默认使用您电脑的时区。<a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones" target="_blank" rel="noopener">时区列表</a>。比如说：<code>America/New_York</code>, <code>Japan</code>, 和 <code>UTC</code> 。</td></tr></tbody></table></div><p>其中，<code>description</code>主要用于SEO，告诉搜索引擎一个关于您站点的简单描述，通常建议在其中包含您网站的关键词。<code>author</code>参数用于主题显示文章的作者。</p><h2 id="网址"><a href="#网址" class="headerlink" title="网址"></a>网址</h2><div class="table-container"><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">描述</th><th style="text-align:left">默认值</th></tr></thead><tbody><tr><td style="text-align:left"><code>url</code></td><td style="text-align:left">网址</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>root</code></td><td style="text-align:left">网站根目录</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>permalink</code></td><td style="text-align:left">文章的 <a href="https://hexo.io/zh-cn/docs/permalinks" target="_blank" rel="noopener">永久链接</a> 格式</td><td style="text-align:left"><code>:year/:month/:day/:title/</code></td></tr><tr><td style="text-align:left"><code>permalink_defaults</code></td><td style="text-align:left">永久链接中各部分的默认值</td></tr></tbody></table></div><p>如果你的网站存放在子目录中，例如 <code>http://yoursite.com/blog</code>，则请将您的 <code>url</code> 设为 <code>http://yoursite.com/blog</code> 并把 <code>root</code> 设为 <code>/blog/</code></p><h2 id="文章"><a href="#文章" class="headerlink" title="文章"></a>文章</h2><div class="table-container"><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">描述</th><th style="text-align:left">默认值</th></tr></thead><tbody><tr><td style="text-align:left"><code>new_post_name</code></td><td style="text-align:left">新文章的文件名称</td><td style="text-align:left">:title.md</td></tr><tr><td style="text-align:left"><code>default_layout</code></td><td style="text-align:left">预设布局</td><td style="text-align:left">post</td></tr><tr><td style="text-align:left"><code>auto_spacing</code></td><td style="text-align:left">在中文和英文之间加入空格</td><td style="text-align:left">false</td></tr><tr><td style="text-align:left"><code>titlecase</code></td><td style="text-align:left">把标题转换为 title case</td><td style="text-align:left">false</td></tr><tr><td style="text-align:left"><code>external_link</code></td><td style="text-align:left">在新标签中打开链接</td><td style="text-align:left">true</td></tr><tr><td style="text-align:left"><code>filename_case</code></td><td style="text-align:left">把文件名称转换为 (1) 小写或 (2) 大写</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left"><code>render_drafts</code></td><td style="text-align:left">显示草稿</td><td style="text-align:left">false</td></tr><tr><td style="text-align:left"><code>post_asset_folder</code></td><td style="text-align:left">启动 Asset 文件夹</td><td style="text-align:left">false</td></tr><tr><td style="text-align:left"><code>relative_link</code></td><td style="text-align:left">把链接改为与根目录的相对位址</td><td style="text-align:left">false</td></tr><tr><td style="text-align:left"><code>future</code></td><td style="text-align:left">显示未来的文章</td><td style="text-align:left">true</td></tr><tr><td style="text-align:left"><code>highlight</code></td><td style="text-align:left">代码块的设置</td></tr></tbody></table></div><h2 id="分类-amp-标签"><a href="#分类-amp-标签" class="headerlink" title="分类 &amp; 标签"></a>分类 &amp; 标签</h2><div class="table-container"><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">描述</th><th style="text-align:left">默认值</th></tr></thead><tbody><tr><td style="text-align:left"><code>default_category</code></td><td style="text-align:left">默认分类</td><td style="text-align:left"><code>uncategorized</code></td></tr><tr><td style="text-align:left"><code>category_map</code></td><td style="text-align:left">分类别名</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>tag_map</code></td><td style="text-align:left">标签别名</td></tr></tbody></table></div><h2 id="日期-时间格式"><a href="#日期-时间格式" class="headerlink" title="日期 / 时间格式"></a>日期 / 时间格式</h2><p>Hexo 使用 <a href="http://momentjs.com/" target="_blank" rel="noopener">Moment.js</a> 来解析和显示时间。</p><div class="table-container"><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">描述</th><th style="text-align:left">默认值</th></tr></thead><tbody><tr><td style="text-align:left"><code>date_format</code></td><td style="text-align:left">日期格式</td><td style="text-align:left"><code>YYYY-MM-DD</code></td></tr><tr><td style="text-align:left"><code>time_format</code></td><td style="text-align:left">时间格式</td><td style="text-align:left"><code>HH:mm:ss</code></td></tr></tbody></table></div><h2 id="分页"><a href="#分页" class="headerlink" title="分页"></a>分页</h2><div class="table-container"><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">描述</th><th style="text-align:left">默认值</th></tr></thead><tbody><tr><td style="text-align:left"><code>per_page</code></td><td style="text-align:left">每页显示的文章量 (0 = 关闭分页功能)</td><td style="text-align:left"><code>10</code></td></tr><tr><td style="text-align:left"><code>pagination_dir</code></td><td style="text-align:left">分页目录</td><td style="text-align:left"><code>page</code></td></tr></tbody></table></div><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><div class="table-container"><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left"><code>theme</code></td><td style="text-align:left">当前主题名称。值为<code>false</code>时禁用主题</td></tr><tr><td style="text-align:left"><code>theme_config</code></td><td style="text-align:left">主题的配置文件。在这里放置的配置会覆盖主题目录下的 _config.yml 中的配置</td></tr><tr><td style="text-align:left"><code>deploy</code></td><td style="text-align:left">部署部分的设置</td></tr><tr><td style="text-align:left"><code>meta_generator</code></td><td style="text-align:left"><a href="https://developer.mozilla.org/zh-CN/docs/Web/HTML/Element/meta#属性" target="_blank" rel="noopener">Meta generator</a> 标签。 值为 <code>false</code> 时 Hexo 不会在头部插入该标签</td></tr></tbody></table></div><h1 id="主题配置"><a href="#主题配置" class="headerlink" title="主题配置"></a>主题配置</h1><h1 id="文章文件"><a href="#文章文件" class="headerlink" title="文章文件"></a>文章文件</h1><h2 id="创建文章"><a href="#创建文章" class="headerlink" title="创建文章"></a>创建文章</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new [文章布局][参数] "文章标题"  #文章布局可省略，默认是post</span><br></pre></td></tr></table></figure><p>执行完成后可以在<code>/source/_posts</code> 下看到一个“<code>文章标题.md</code>”的文章文件。是 <code>Markdown</code> 格式的文件，头部为布局文件中的形式。最好不要直接在<code>/source/_posts</code>下直接创建<code>.md</code>文件。如果要直接创建则要加上头部信息(Front-matter)</p><p>参数：</p><div class="table-container"><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left"><code>-p</code>, <code>--path</code></td><td style="text-align:left">自定义新文章的路径</td></tr><tr><td style="text-align:left"><code>-r</code>, <code>--replace</code></td><td style="text-align:left">如果存在同名文章，将其替换</td></tr><tr><td style="text-align:left"><code>-s</code>, <code>--slug</code></td><td style="text-align:left">文章的 Slug，作为新文章的文件名和发布后的 URL</td></tr></tbody></table></div><p>默认情况下，Hexo 会使用文章布局来决定文章文件的路径。对于page布局来说，Hexo 会创建一个以标题为名字的目录，并在目录中放置一个 <code>index.md</code> 文件。可以使用 <code>--path</code> 参数来覆盖上述行为、自行决定文件的目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page --path about/me &quot;About me&quot;</span><br></pre></td></tr></table></figure><p>以上命令会创建一个 <code>source/about/me.md</code> 文件，同时 Front Matter 中的 title 为 <code>&quot;About me&quot;</code></p><h2 id="文章布局（Layout）"><a href="#文章布局（Layout）" class="headerlink" title="文章布局（Layout）"></a>文章布局（Layout）</h2><p>在新建文章时，Hexo 会根据 <code>scaffolds</code> 文件夹内相对应的文章布局文件来建立<code>.md</code>文件。文章的布局（layout），默认为 <code>post</code>（对应<code>scaffolds</code>中的<code>post.md</code>文件），可以通过修改 <code>_config.yml</code> 中的 <code>default_layout</code> 参数来指定默认布局</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new photo &quot;My Gallery&quot;</span><br></pre></td></tr></table></figure><p>在执行这行指令时，Hexo 会尝试在 <code>scaffolds</code> 文件夹中寻找 <code>photo.md</code>，并根据其内容建立<code>&quot;My Gallery.md&quot;</code>文章</p><h3 id="默认布局：post、page、draft"><a href="#默认布局：post、page、draft" class="headerlink" title="默认布局：post、page、draft"></a>默认布局：<code>post</code>、<code>page</code>、<code>draft</code></h3><p>在创建三种不同类型的文件时，它们将会被保存到不同的路径；自定义的其他布局和 <code>post</code> 相同，都将储存到 <code>source/_posts</code> 文件夹。</p><div class="table-container"><table><thead><tr><th style="text-align:left">布局</th><th style="text-align:left">路径</th></tr></thead><tbody><tr><td style="text-align:left"><code>post</code></td><td style="text-align:left"><code>source/_posts</code></td></tr><tr><td style="text-align:left"><code>page</code></td><td style="text-align:left"><code>source</code></td></tr><tr><td style="text-align:left"><code>draft</code></td><td style="text-align:left"><code>source/_drafts</code></td></tr></tbody></table></div><p>草稿：</p><p>draft是Hexo 的一种特殊布局，这种布局在建立时会被保存到 <code>source/_drafts</code> 文件夹，默认不渲染，可通过 <code>publish</code> 命令将草稿移动到 <code>source/_posts</code> 文件夹进行渲染，该命令的使用方式与 <code>new</code> 类似</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo publish [文章布局] &quot;文章标题&quot;  #文章布局可省略，默认是post</span><br></pre></td></tr></table></figure><p>草稿默认不会显示在页面中，可在执行时加上 <code>--draft</code> 参数，或是把 <code>render_drafts</code> 参数设为 <code>true</code> 来预览草稿。</p><h3 id="自定义布局"><a href="#自定义布局" class="headerlink" title="自定义布局"></a>自定义布局</h3><p>支持自定义布局，可模仿<code>scaffolds/post.md</code>文件书写，详细见 Front-matter</p><h2 id="文件名称"><a href="#文件名称" class="headerlink" title="文件名称"></a>文件名称</h2><p>Hexo 默认以标题做为文件名称，可编辑站点配置文件的 <code>new_post_name</code> 参数来改变默认的文件名称，推荐默认设置 <code>:year-:month-:day-:title.md</code> 可更方便的通过日期来管理文章。</p><div class="table-container"><table><thead><tr><th style="text-align:left">变量</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left"><code>:title</code></td><td style="text-align:left">标题（小写，空格将会被替换为短杠）</td></tr><tr><td style="text-align:left"><code>:year</code></td><td style="text-align:left">建立的年份，比如， <code>2015</code></td></tr><tr><td style="text-align:left"><code>:month</code></td><td style="text-align:left">建立的月份（有前导零），比如， <code>04</code></td></tr><tr><td style="text-align:left"><code>:i_month</code></td><td style="text-align:left">建立的月份（无前导零），比如， <code>4</code></td></tr><tr><td style="text-align:left"><code>:day</code></td><td style="text-align:left">建立的日期（有前导零），比如， <code>07</code></td></tr><tr><td style="text-align:left"><code>:i_day</code></td><td style="text-align:left">建立的日期（无前导零），比如， <code>7</code></td></tr></tbody></table></div><h2 id="文章头部（Front-matter）"><a href="#文章头部（Front-matter）" class="headerlink" title="文章头部（Front-matter）"></a>文章头部（Front-matter）</h2><p>Front-matter 是文件最上方以 <code>---</code> 分隔的区域，用于指定个别文件的变量，举例来说：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: Hello World</span><br><span class="line">date: 2013/7/13 20:46:25</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>一般Front-matter：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">title: your title</span><br><span class="line">date: year-month-day</span><br><span class="line">tags:</span><br><span class="line"><span class="bullet">   - </span>your tag1</span><br><span class="line"><span class="bullet">   - </span>your tag2</span><br><span class="line">categories:</span><br><span class="line"><span class="bullet">   - </span>your category1</span><br><span class="line"><span class="bullet">   - </span>your category2</span><br></pre></td></tr></table></figure><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>以下是预先定义的参数，可在模板中使用这些参数值并加以利用。</p><div class="table-container"><table><thead><tr><th style="text-align:left">参数</th><th style="text-align:left">描述</th><th style="text-align:left">默认值</th></tr></thead><tbody><tr><td style="text-align:left"><code>layout</code></td><td style="text-align:left">布局</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>title</code></td><td style="text-align:left">标题</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>date</code></td><td style="text-align:left">建立日期</td><td style="text-align:left">文件建立日期</td></tr><tr><td style="text-align:left"><code>updated</code></td><td style="text-align:left">更新日期</td><td style="text-align:left">文件更新日期</td></tr><tr><td style="text-align:left"><code>comments</code></td><td style="text-align:left">开启文章的评论功能</td><td style="text-align:left">true</td></tr><tr><td style="text-align:left"><code>tags</code></td><td style="text-align:left">标签（不适用于分页）</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>categories</code></td><td style="text-align:left">分类（不适用于分页）</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>permalink</code></td><td style="text-align:left">覆盖文章网址</td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><code>keywords</code></td><td style="text-align:left">仅用于 meta 标签和 Open Graph 的关键词（不推荐使用）</td></tr></tbody></table></div><h3 id="分类和标签"><a href="#分类和标签" class="headerlink" title="分类和标签"></a>分类和标签</h3><p>只有文章支持分类和标签，可在 Front-matter 中设置。分类具有顺序性和层次性，也就是说 <code>Foo, Bar</code> 不等于 <code>Bar, Foo</code>；而标签没有顺序和层次。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">categories:</span><br><span class="line"> - Diary</span><br><span class="line">tags:</span><br><span class="line"> - PS3</span><br><span class="line"> - Games</span><br></pre></td></tr></table></figure><h1 id="文章资源"><a href="#文章资源" class="headerlink" title="文章资源"></a>文章资源</h1><p>资源（Asset）指<code>source</code> 文件夹中除了文章以外的所有文件，例如图片、CSS、JS 文件等。使用文章资源的方式有两种：使用Markdow语法或使用插件</p><h2 id="使用Markdown语法和相对路径来引用资源"><a href="#使用Markdown语法和相对路径来引用资源" class="headerlink" title="使用Markdown语法和相对路径来引用资源"></a>使用Markdown语法和相对路径来引用资源</h2><p>如果你的Hexo项目中只有少量图片，最简单的方法就是将它们放在 <code>source/images</code> 文件夹中。然后使用Markdown语法 <code>![](/images/image.jpg)</code> 的方法访问它们。</p><p>但通过常规的 markdown 语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确</p><h2 id="使用文章资源文件夹和相对路径引用的标签插件"><a href="#使用文章资源文件夹和相对路径引用的标签插件" class="headerlink" title="使用文章资源文件夹和相对路径引用的标签插件"></a>使用文章资源文件夹和相对路径引用的标签插件</h2><ol><li><p>打开文件资源文件夹</p><p>Hexo提供了更组织化的方式来管理资源。通过将 主站配置文件<code>_config.yml</code> 文件中的 <code>post_asset_folder</code> 选项设为 <code>true</code> 来打开。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_config.ymlpost_asset_folder: true</span><br></pre></td></tr></table></figure><p>当资源文件管理功能打开后，Hexo将会在你每一次通过 <code>hexo new [layout] &lt;title&gt;</code> 命令创建新文章时自动创建一个文件夹。这个资源文件夹将会有与这个文章文件一样的名字。将所有与你的文章有关的资源放在这个关联文件夹中之后，可以通过相对路径来引用它们，这样就得到了一个更简单而且方便得多的工作流。</p></li><li><p>使用相对路径引用的标签插件插入图片</p><p>Hexo 3 这种标签插件已被加入到了核心代码中。这使得可以更简单地在文章中引用你的资源，语句如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_path slug %&#125;</span><br><span class="line">&#123;% asset_img slug [title] %&#125;</span><br><span class="line">&#123;% asset_link slug [title] %&#125;</span><br></pre></td></tr></table></figure><p>例如：当你打开文章资源文件夹功能后，把一个 <code>example.jpg</code> 图片放在了对应的资源文件夹中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_img example.jpg This is an example image %&#125;</span><br></pre></td></tr></table></figure></li></ol><p>​    通过这种方式，图片将会同时出现在文章和主页以及归档页中。</p><h1 id="插件"><a href="#插件" class="headerlink" title="插件"></a>插件</h1>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 Github Page + Hexo 搭建自己的博客网站</title>
      <link href="/2019/10/03/Github-Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2019/10/03/Github-Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="注册Github账号、创建代码库"><a href="#注册Github账号、创建代码库" class="headerlink" title="注册Github账号、创建代码库"></a>注册Github账号、创建代码库</h1><ol><li><p>没有账号的先<a href="https://github.com/" target="_blank" rel="noopener">注册</a>账号</p><img title="This is an example image" data-src="/2019/10/03/Github-Hexo搭建博客/1.png" class="lazyload"></li><li><p>点击 Start project 或者下面的 new repository 创建一个新的仓库，注意仓库名要以  <strong>用户名.github.io</strong> 命名。且Github规定一个用户仅能使用一个同名仓库的代码托管一个静态站点</p><img title="This is an example image" data-src="/2019/10/03/Github-Hexo搭建博客/2.png" class="lazyload"></li><li><p>可访问 <a href="https://用户名.github.io" target="_blank" rel="noopener">https://用户名.github.io</a> 来访问你的静态站点（如果你在该仓库写一个index.html则会显示这个页面的内容）</p></li></ol><h1 id="安装各种工具"><a href="#安装各种工具" class="headerlink" title="安装各种工具"></a>安装各种工具</h1><h2 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h2><ol><li><p>下载<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git</a>，<a href="https://www.liaoxuefeng.com/wiki/896043488029600" target="_blank" rel="noopener">Git使用教程</a></p></li><li><p>傻瓜式安装，成功标志：在开始菜单里找到“Git”-&gt;“Git Bash”，，蹦出一个类似命令行窗口的东西</p></li><li><p>配置Git，没有账号的见注册步骤</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name "Your Name"</span><br><span class="line">git config --global user.email "email@example.com"</span><br></pre></td></tr></table></figure></li></ol><h2 id="NodeJS"><a href="#NodeJS" class="headerlink" title="NodeJS"></a>NodeJS</h2><p>Node.js 是一个基于 Chrome V8 引擎的 <a href="https://baike.baidu.com/item/JavaScript/321142" target="_blank" rel="noopener">JavaScript</a> 运行环境。 Node.js 使用了一个事件驱动、非阻塞式 I/O 的模型。Node 是一个让 JavaScript 运行在<a href="https://baike.baidu.com/item/服务端/6492316" target="_blank" rel="noopener">服务端</a>的开发平台，它让 JavaScript 成为与<a href="https://baike.baidu.com/item/PHP/9337" target="_blank" rel="noopener">PHP</a>、<a href="https://baike.baidu.com/item/Python/407313" target="_blank" rel="noopener">Python</a>、<a href="https://baike.baidu.com/item/Perl/851577" target="_blank" rel="noopener">Perl</a>、<a href="https://baike.baidu.com/item/Ruby/11419" target="_blank" rel="noopener">Ruby</a> 等服务端语言平起平坐的<a href="https://baike.baidu.com/item/脚本语言/1379708" target="_blank" rel="noopener">脚本语言</a>。</p><p>npm是NodeJS的包管理器</p><p>由于Hexo基于NodeJS，安装Hexo需先安装NodeJS</p><ol><li><p>下载<a href="https://nodejs.org/en/" target="_blank" rel="noopener">NodeJS</a></p></li><li><p>傻瓜式安装，成功标志：打开cmd输入node -v和npm -v显示版本。显示不是内部或外部命令的解决方法</p><ul><li>重启cmd</li><li>查看是否配置NodeJS环境变量(计算机-&gt;属性-&gt;高级系统属性-&gt;高级-&gt;环境变量-&gt;找到Path查看是否有NodeJS路径)</li></ul></li><li><p>验证前两步</p><p>打开git bash（Windowns）或者终端（Mac），在命令行中输入相应命令验证是否成功，如果成功会有相应的版本号</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git version</span><br><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure></li></ol><h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><p>以上环境准备好了就可以使用 npm 开始安装 Hexo 了，<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">官方Hexo文档</a></p><p>使用NodeJS的包管理器npm来安装Hexo，在命令行输入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><h1 id="使用Hexo生成本地博客"><a href="#使用Hexo生成本地博客" class="headerlink" title="使用Hexo生成本地博客"></a>使用Hexo生成本地博客</h1><h2 id="使用以下命令初始化一个博客项目"><a href="#使用以下命令初始化一个博客项目" class="headerlink" title="使用以下命令初始化一个博客项目"></a>使用以下命令初始化一个博客项目</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd &lt;博客放置的路径&gt;  #不执行此语句默认在用户目录下</span><br><span class="line">hexo init myBlog&lt;博客文件夹名&gt;</span><br><span class="line">cd myBlog&lt;博客文件夹名&gt;</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><p>创建完成后，博客文件夹目录如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml # 网站的配置信息，您可以在此配置大部分的参数。 </span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds # 模版文件夹,当您新建文章时，Hexo 会根据 scaffolds 来建立文件</span><br><span class="line">├── source  # 资源文件夹，除 _posts 文件，其他以下划线_开头的文件或者文件夹不会被编译打包到public文件夹</span><br><span class="line">|   └── _posts # 文章Markdowm文件 </span><br><span class="line">└── themes  # 主题文件夹，Hexo 会根据主题来生成静态页面</span><br></pre></td></tr></table></figure><h2 id="运行浏览博客"><a href="#运行浏览博客" class="headerlink" title="运行浏览博客"></a>运行浏览博客</h2><p>要能够在本地预览自己的博客，需要本地启动服务器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s   #浏览器访问 http://localhost:4000 浏览</span><br></pre></td></tr></table></figure><p>本地博客搭建成功，下面部署到 Github Page</p><h1 id="部署到Github-Page"><a href="#部署到Github-Page" class="headerlink" title="部署到Github Page"></a>部署到Github Page</h1><h2 id="配置SSH-key"><a href="#配置SSH-key" class="headerlink" title="配置SSH key"></a>配置SSH key</h2><p>要使用 git 工具首先要配置一下SSH key，为部署本地博客到 Github 做准备。</p><ol><li><p>查看是否有SSH key</p><p>打开命令行输入 cd ~/.ssh 如果没报错或者提示什么的说明就是以前生成过的，直接使用 cat ~/.ssh/id_rsa.pub 命令就是可以查看本机上的 SSH key 了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure></li><li><p>没有就创建</p><p>全局配置一下本地账户：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;用户名&quot;</span><br><span class="line">git config --global user.email &quot;邮箱地址&quot;</span><br></pre></td></tr></table></figure><p>生成密钥 SSH key</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &apos;上面的邮箱&apos;</span><br></pre></td></tr></table></figure><p>按照提示完成三次回车，即可生成 ssh key。通过查看 ~/.ssh/id_rsa.pub 文件内容，获取到你的 SSH key</p><img title="This is an example image" data-src="/2019/10/03/Github-Hexo搭建博客/3.png" class="lazyload"><p>首次使用还需要确认并添加主机到本机SSH可信列表。若返回 Hi xxx! You’ve successfully authenticated, but GitHub does not provide shell access. 内容，则证明添加成功。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure></li><li><p>登录 <a href="https://github.com/" target="_blank" rel="noopener">Github</a> 上Settings添加刚刚生成的SSH key</p><img title="This is an example image" data-src="/2019/10/03/Github-Hexo搭建博客/4.png" class="lazyload"><p>创建一个新的 SSH key, 标题随便取，key 填刚才生成的，这样在你的 SSH keys 列表里就有刚刚添加的密钥。</p></li></ol><h2 id="部署到Github"><a href="#部署到Github" class="headerlink" title="部署到Github"></a>部署到Github</h2><p>将本地博客库和Github连接起来</p><p>打开项目根目录下的 _config.yml 配置文件。Deploymen部分按如下配置（也可同时部署到多个仓库）：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">git</span></span><br><span class="line"><span class="attr">  repo:</span> </span><br><span class="line"><span class="attr">    github:</span> <span class="attr">https://github.com/&lt;github用户名&gt;/&lt;github用户名&gt;.github.io.git</span></span><br><span class="line"><span class="attr">  branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><p>安装部署插件 <a href="https://github.com/hexojs/hexo-deployer-git" target="_blank" rel="noopener">hexo-deployer-git</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>最后执行以下命令就可以部署上传啦，以下 g 是 generate 缩写，d 是 deploy 缩写：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g -d</span><br></pre></td></tr></table></figure><p>稍等一会，在浏览器访问网址： <a href="https://用户名.github.io" target="_blank" rel="noopener">https://用户名.github.io</a>  成功！！！具体配置和使用教程参见下一博客</p>]]></content>
      
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
