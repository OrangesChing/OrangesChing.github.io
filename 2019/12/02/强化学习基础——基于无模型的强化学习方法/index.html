<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>强化学习基础——基于无模型的强化学习方法 | 程子卿的博客</title><meta name="description" content="强化学习基础——基于无模型的强化学习方法"><meta name="keywords" content="机器学习,强化学习"><meta name="author" content="程子卿"><meta name="copyright" content="程子卿"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon2.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="强化学习基础——基于无模型的强化学习方法"><meta name="twitter:description" content="强化学习基础——基于无模型的强化学习方法"><meta name="twitter:image" content="https://orangesching.github.io/img/cover/cover2.gif"><meta property="og:type" content="article"><meta property="og:title" content="强化学习基础——基于无模型的强化学习方法"><meta property="og:url" content="https://orangesching.github.io/2019/12/02/强化学习基础——基于无模型的强化学习方法/"><meta property="og:site_name" content="程子卿的博客"><meta property="og:description" content="强化学习基础——基于无模型的强化学习方法"><meta property="og:image" content="https://orangesching.github.io/img/cover/cover2.gif"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://orangesching.github.io/2019/12/02/强化学习基础——基于无模型的强化学习方法/"><link rel="next" title="强化学习基础——基于模型的动态规划方法" href="https://orangesching.github.io/2019/11/30/强化学习基础——基于模型的动态规划方法/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: undefined,
  medium_zoom: 'false',
  Snackbar: undefined
  
}</script></head><body><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">程子卿的博客</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></span><span class="pull_right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="/img/my.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">10</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">5</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#简介-Introduction"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">简介 Introduction</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#策略评估"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">策略评估</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#蒙特卡洛强化学习-Monte-Carlo-Reinforcement-Learning"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">蒙特卡洛强化学习 Monte-Carlo Reinforcement Learning</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#启发（为什么会想到这个方法）"><span class="toc_mobile_items-number">2.1.1.</span> <span class="toc_mobile_items-text">启发（为什么会想到这个方法）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#定义"><span class="toc_mobile_items-number">2.1.2.</span> <span class="toc_mobile_items-text">定义</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#重复状态的收获计算"><span class="toc_mobile_items-number">2.1.2.1.</span> <span class="toc_mobile_items-text">重复状态的收获计算</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#首次访问蒙特卡洛策略评估-First-Visit-Monte-Carlo-Policy-Evaluation"><span class="toc_mobile_items-number">2.1.2.1.1.</span> <span class="toc_mobile_items-text">首次访问蒙特卡洛策略评估(First-Visit Monte-Carlo Policy Evaluation)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#每次访问蒙特卡洛策略评估"><span class="toc_mobile_items-number">2.1.2.1.2.</span> <span class="toc_mobile_items-text">每次访问蒙特卡洛策略评估</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#示例：二十一点游戏-Blackjack-Example"><span class="toc_mobile_items-number">2.1.2.2.</span> <span class="toc_mobile_items-text">　示例：二十一点游戏 Blackjack Example</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#蒙特卡洛累进更新"><span class="toc_mobile_items-number">2.1.3.</span> <span class="toc_mobile_items-text">蒙特卡洛累进更新</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#时序差分学习-Temporal-Difference-Learning"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">时序差分学习 Temporal-Difference Learning</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#TD-λ"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">TD(λ)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#小结"><span class="toc_mobile_items-number">2.4.</span> <span class="toc_mobile_items-text">小结</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#策略求解"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">策略求解</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#简介-Introduction"><span class="toc-number">1.</span> <span class="toc-text">简介 Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#策略评估"><span class="toc-number">2.</span> <span class="toc-text">策略评估</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#蒙特卡洛强化学习-Monte-Carlo-Reinforcement-Learning"><span class="toc-number">2.1.</span> <span class="toc-text">蒙特卡洛强化学习 Monte-Carlo Reinforcement Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#启发（为什么会想到这个方法）"><span class="toc-number">2.1.1.</span> <span class="toc-text">启发（为什么会想到这个方法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#定义"><span class="toc-number">2.1.2.</span> <span class="toc-text">定义</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#重复状态的收获计算"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">重复状态的收获计算</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#首次访问蒙特卡洛策略评估-First-Visit-Monte-Carlo-Policy-Evaluation"><span class="toc-number">2.1.2.1.1.</span> <span class="toc-text">首次访问蒙特卡洛策略评估(First-Visit Monte-Carlo Policy Evaluation)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#每次访问蒙特卡洛策略评估"><span class="toc-number">2.1.2.1.2.</span> <span class="toc-text">每次访问蒙特卡洛策略评估</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#示例：二十一点游戏-Blackjack-Example"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">　示例：二十一点游戏 Blackjack Example</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#蒙特卡洛累进更新"><span class="toc-number">2.1.3.</span> <span class="toc-text">蒙特卡洛累进更新</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#时序差分学习-Temporal-Difference-Learning"><span class="toc-number">2.2.</span> <span class="toc-text">时序差分学习 Temporal-Difference Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TD-λ"><span class="toc-number">2.3.</span> <span class="toc-text">TD(λ)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#小结"><span class="toc-number">2.4.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#策略求解"><span class="toc-number">3.</span> <span class="toc-text">策略求解</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/cover/cover2.gif)"><div id="post-info"><div id="post-title"><div class="posttitle">强化学习基础——基于无模型的强化学习方法</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-12-02<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2019-12-02</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/机器学习/">机器学习</a></span><div class="post-meta-wordcount"><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="简介-Introduction"><a href="#简介-Introduction" class="headerlink" title="简介 Introduction"></a>简介 Introduction</h1><p>马尔可夫决策过程可以利用元组$<s,a,p,r,\gamma>$来描述，而根据转移概率P是否已知，可以分为基于模型的动态规划方法和基于无模型的强化学习方法。这两类都包括策略迭代算法、值迭代算法、策略搜索算法。</s,a,p,r,\gamma></p>
<p>上一章学习了基于模型的动态规划方法，学习了如何从理论上解决一个<strong>已知</strong>的MDP：通过动态规划来评估一个给定的策略，并且得到最优价值函数，根据最优价值函数来确定最优策略；也可以直接进行不基于任何策略的状态价值迭代得到最优价值函数和最优策略。</p>
<p>这一章我们将讨论解决一个可以被认为是MDP、但却不掌握MDP具体细节（不知道环境是怎样的）的问题，也就是讲述如何直接从Agent与环境的交互来得得到一个估计的最优价值函数和最优策略。这部分内容分为两部分</p>
<ul>
<li><p>第一部分聚焦于策略评估，也就是预测，直白的说就是在给定的策略同时不清楚MDP细节的情况下，估计得到值函数。</p>
</li>
<li><p>第二部分将利用第一部分的策略评估的主要观念来进行控制进而找出最优策略，最大化Agent的奖励。</p>
</li>
</ul>
<h1 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h1><p>这部分内容分为三个小部分，分别是蒙特卡洛强化学习、时序差分强化学习和介于两者之间的λ时序差分强化学习</p>
<h2 id="蒙特卡洛强化学习-Monte-Carlo-Reinforcement-Learning"><a href="#蒙特卡洛强化学习-Monte-Carlo-Reinforcement-Learning" class="headerlink" title="蒙特卡洛强化学习 Monte-Carlo Reinforcement Learning"></a>蒙特卡洛强化学习 Monte-Carlo Reinforcement Learning</h2><h3 id="启发（为什么会想到这个方法）"><a href="#启发（为什么会想到这个方法）" class="headerlink" title="启发（为什么会想到这个方法）"></a>启发（为什么会想到这个方法）</h3><p>前面讲的主要内容是整个问题可以转换成一个马尔科夫决策过程(MDP)五元组，但是，在现实世界中，我们无法同时知道这个5元组。比如P，状态转移概率就很难知道，P不知道，我们就无法使用bellman方程来求解V和Q值。但是我们依然要去解决这个问题。怎么办？</p>
<p>一个想法是，虽然我不知道状态转移概率P，但是这个概率是真实存在的。我们可以直接去尝试，不断采样，然后会得到奖赏，通过奖赏来评估值函数。这个想法与蒙特卡罗方法的思想是一致的。我们可以尝试很多次，最后估计的V值就会很接近真实的V值了。</p>
<img title="This is an example image" data-src="/2019/12/02/强化学习基础——基于无模型的强化学习方法/monte_carlo.png" class="lazyload">
<p>比如上图，矩形的面积我们可以轻松得到，但是对于阴影部分的面积，我们积分是比较困难的。所以为了计算阴影部分的面积，我们可以在矩形上均匀地撒豆子，然后统计在阴影部分的豆子数占总的豆子数的比例，就可以估算出阴影部分的面积了</p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><strong>蒙特卡洛强化学习</strong>：又叫统计模拟方法，在不清楚MDP状态转移及即时奖励的情况下，直接从经历过的<strong>完整Episode</strong>来学习状态价值，通常情况下某状态的价值等于在多个Episode中以该状态算得到的所有收益的平均。</p>
<p><strong>目标：</strong>在给定策略下，从一系列的完整Episode经历中学习得到该策略下的状态价值函数。</p>
<p><strong>蒙特卡洛强化学习的特点</strong>：不基于模型本身，直接从经历过的Episode中学习，必须是<strong>完整的Episode</strong></p>
<p><strong>蒙特卡洛强化学习使用的思想</strong>就是<strong>使用平均收获值代替价值</strong>。理论上Episode越多，结果越准确。</p>
<blockquote>
<p><strong>Episode</strong></p>
<p>episode就是经历，每条episode就是一条从起始状态到结束状态的经历。例如在走迷宫，一条episode就是从你开始进入迷宫，到最后走出迷宫的路径。</p>
<p>首先我们要得到的是某一个状态$s$的平均收获。所以我们说的episode要经过状态$s$。所以下图中第二条路径没有经过状态s，对于s来说就不能使用它了。而且最后我们episode都是要求达到终点的，才能算是一个episode。</p>
<img title="This is an example image" data-src="/2019/12/02/强化学习基础——基于无模型的强化学习方法/monte_carlo.png" class="lazyload">
<p>Episode其包含的信息有：状态的转移、使用的行为序列、中间状态获得的即时奖励以及到达终止状态时获得的即时奖励。</p>
<p>完整的Episode 指必须从某一个状态开始，Agent与Environment交互直到终止状态，环境给出终止状态的即时收获为止。</p>
<p>完整的Episode不要求起始状态一定是某一个特定的状态，但是要求个体最终进入环境认可的某一个终止状态。</p>
</blockquote>
<p>数学描述如下：</p>
<ul>
<li>基于特定策略 $\pi$  的一个Episode信息可以表示为如下的一个序列：</li>
</ul>
<script type="math/tex; mode=display">
S _ { 1 } , A _ { 1 } , R _ { 2 } , S _ { 2 } , A _ { 2 } , \ldots , S _ { t } , A _ { t } , R _ { t + 1 } , \ldots , S _ { k } \sim \pi</script><ul>
<li>$t$时刻状态 $S_t$ 的收益：</li>
</ul>
<script type="math/tex; mode=display">
G _ { t } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { T - 1 } R _ { T }</script><p>其中 $T$ 为终止时刻。</p>
<ul>
<li>该策略下某一状态 $s$ 的价值：</li>
</ul>
<script type="math/tex; mode=display">
v _ { \pi } ( s ) = E _ { \pi } \left[ G _ { t } | S _ { t } = s \right]</script><h4 id="重复状态的收获计算"><a href="#重复状态的收获计算" class="headerlink" title="重复状态的收获计算"></a>重复状态的收获计算</h4><p>在状态转移过程中，可能发生一个状态经过一定的转移后又一次或多次返回该状态，此时在一个Episode里如何计算这个状态发生的次数和计算该Episode的收获呢？可以有如下两种方法：</p>
<h5 id="首次访问蒙特卡洛策略评估-First-Visit-Monte-Carlo-Policy-Evaluation"><a href="#首次访问蒙特卡洛策略评估-First-Visit-Monte-Carlo-Policy-Evaluation" class="headerlink" title="首次访问蒙特卡洛策略评估(First-Visit Monte-Carlo Policy Evaluation)"></a>首次访问蒙特卡洛策略评估(First-Visit Monte-Carlo Policy Evaluation)</h5><p>在给定一个策略，使用一系列完整Episode评估某一个状态$s$时，对于每一个Episode，仅当该状态<strong>第一次</strong>出现时列入计算</p>
<p>状态出现的次数加1： </p>
<script type="math/tex; mode=display">
N ( s ) \leftarrow N ( s ) + 1</script><p>总的收获值更新：</p>
<script type="math/tex; mode=display">
S ( s ) \leftarrow S ( s ) + G_t</script><p>状态s的价值：</p>
<script type="math/tex; mode=display">
V(s) = \frac {S(s)}{N(s)}</script><p>当$N ( s ) \rightarrow \infty $ 时，$V(s) \leftarrow v_{\pi}(s)$ </p>
<img title="This is an example image" data-src="/2019/12/02/强化学习基础——基于无模型的强化学习方法/First_visit_mc.jpg" class="lazyload">
<h5 id="每次访问蒙特卡洛策略评估"><a href="#每次访问蒙特卡洛策略评估" class="headerlink" title="每次访问蒙特卡洛策略评估"></a>每次访问蒙特卡洛策略评估</h5><p>在给定一个策略，使用一系列完整Episode评估某一个状态$s$时，对于每一个Episode，状态s<strong>每次</strong>出现在状态转移链时，计算的具体公式与上面的一样，但具体意义不一样。</p>
<p>状态出现的次数加1： </p>
<script type="math/tex; mode=display">
N ( s ) \leftarrow N ( s ) + 1</script><p>总的收获值更新：</p>
<script type="math/tex; mode=display">
S ( s ) \leftarrow S ( s ) + G_t</script><p>状态s的价值：</p>
<script type="math/tex; mode=display">
V(s) = \frac {S(s)}{N(s)}</script><p>当$N ( s ) \rightarrow \infty $ 时，$V(s) \leftarrow v_{\pi}(s)$ </p>
<h4 id="示例：二十一点游戏-Blackjack-Example"><a href="#示例：二十一点游戏-Blackjack-Example" class="headerlink" title="　示例：二十一点游戏 Blackjack Example"></a>　示例：二十一点游戏 Blackjack Example</h4><p>该示例解释了Model-Free下的策略评估问题和结果，没有说具体的学习过程。</p>
<p>游戏规则：你会得到一副手牌，一开始是两张，你需要得到尽量靠近21点但不能超过21点的手牌点数和。越接近21点越有可能打赢庄家，打赢庄家就算赢。庄家会亮一张牌给玩家看</p>
<p><strong>状态空间</strong>：（多达200种，根据对状态的定义可以有不同的状态空间，这里采用的定义是牌的分数，不包括牌型）</p>
<ul>
<li>当前牌的分数（12 - 21），低于12时，你可以安全的再叫牌，所以没意义。</li>
<li>庄家出示的牌（A - 10），庄家会显示一张牌面给玩家</li>
<li>我有“useable” ace吗？（是或否）A既可以当1点也可以当11点。</li>
</ul>
<p><strong>行为空间</strong>：</p>
<ul>
<li>停止要牌 stick</li>
<li>继续要牌 twist</li>
</ul>
<p><strong>奖励（停止要牌）</strong>：</p>
<ul>
<li>+1：如果你的牌分数大于庄家分数</li>
<li>0： 如果两者分数相同</li>
<li>-1：如果你的牌分数小于庄家分数</li>
</ul>
<p><strong>奖励（继续要牌）</strong>：</p>
<ul>
<li>-1：如果牌的分数&gt;21，并且进入终止状态</li>
<li>0：其它情况</li>
</ul>
<p><strong>状态转换（Transitions）</strong>：如果牌分小于12时，自动要牌</p>
<p><strong>当前策略</strong>：牌分只要小于20就继续要牌。</p>
<p><strong>问题：</strong>评估该策略的好坏。</p>
<p><strong>评估过程：</strong>使用庄家显示的牌面值、玩家当前牌面总分值来确定一个二维状态空间，区分手中有无A分别处理。统计每一牌局下决定状态的庄家和玩家牌面的状态数据，同时计算其最终收获。通过模拟多次牌局，计算每一个状态下的平均值，得到如下图示。</p>
<p><strong>最终结果：</strong>无论玩家手中是否有A牌，该策略在绝大多数情况下各状态价值都较低，只有在玩家拿到21分时状态价值有一个明显的提升。</p>
<img title="This is an example image" data-src="/2019/12/02/强化学习基础——基于无模型的强化学习方法/monte_carlo_example.png" class="lazyload">
<h3 id="蒙特卡洛累进更新"><a href="#蒙特卡洛累进更新" class="headerlink" title="蒙特卡洛累进更新"></a>蒙特卡洛累进更新</h3><p>在使用蒙特卡洛方法求解平均收获时，需要计算平均值。通常计算平均值要预先存储所有的数据，最后使用总和除以此次数。这里介绍了一种更简单实用的方法：</p>
<p><strong>累进更新平均值 Incremental Mean</strong></p>
<p>这里提到了在实际操作时常用的一个实时更新均值的办法，使得在计算平均收获时不需要存储所有既往收获，而是每得到一次收获，就计算其平均收获。</p>
<p>理论公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mu _ { k } & = \frac { 1 } { k } \sum _ { j = 1 } ^ { k } x _ { j } \\ & = \frac { 1 } { k } \left( x _ { k } + \sum _ { j = 1 } ^ { k - 1 } x _ { j } \right) \\ & = \frac { 1 } { k } \left( x _ { k } + ( k - 1 ) \mu _ { k - 1 } \right) \\ & = \mu _ { k - 1 } + \frac { 1 } { k } \left( x _ { k } - \mu _ { k - 1 } \right) \end{aligned}</script><p>把这个方法应用于蒙特卡洛策略评估，就得到蒙特卡洛累进更新。</p>
<p><strong>蒙特卡洛累进更新</strong></p>
<p>对于一系列Episodes中的每一个：</p>
<script type="math/tex; mode=display">
S _ { 1 } , A _ { 1 } , R _ { 2 } , S _ { 2 } , A _ { 2 } , \ldots , S _ { t } , A _ { t } , R _ { t + 1 } , \ldots , S _ { k }</script><p>对于Episode里的每一个状态 $S_t$ ，有一个收获 $G_t$ ，每碰到一次 $S_t$ ，使用下式计算状态的平均价值 $V(S_t)$ ：</p>
<script type="math/tex; mode=display">
V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \frac { 1 } { N \left( S _ { t } \right) } \left( G _ { t } - V \left( S _ { t } \right) \right)</script><p>其中：</p>
<script type="math/tex; mode=display">
N ( S_t ) \leftarrow N ( S_t ) + 1</script><p>在处理非静态问题时，使用这个方法跟踪一个实时更新的平均值是非常有用的，可以扔掉那些已经计算过的Episode信息。此时可以引入参数 $\alpha$  来更新状态价值：</p>
<script type="math/tex; mode=display">
V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left( G _ { t } - V \left( S _ { t } \right) \right)</script><p>以上就是蒙特卡洛学习方法的主要思想和描述，由于蒙特卡洛学习方法有许多缺点，因此实际应用并不多。接下来介绍实际常用的TD学习方法。</p>
<h2 id="时序差分学习-Temporal-Difference-Learning"><a href="#时序差分学习-Temporal-Difference-Learning" class="headerlink" title="时序差分学习 Temporal-Difference Learning"></a>时序差分学习 Temporal-Difference Learning</h2><p>时序差分学习简称TD学习，它的特点如下：和蒙特卡洛学习一样，它也从Episode学习，不需要了解模型本身；但是它可以学习<strong>不完整</strong>的Episode，通过自身的引导（bootstrapping），猜测Episode的结果，同时持续更新这个猜测。</p>
<p>我们已经学过，在Monte-Carlo学习中，使用实际的收获（return） <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=G_%7Bt%7D" class="lazyload"> 来更新价值（Value）：</p>
<p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=V%28S_%7Bt%7D%29+%5Cleftarrow+V%28S_%7Bt%7D%29+%2B+%5Calpha+%28G_%7Bt%7D+-+V%28S_%7Bt%7D%29%29" class="lazyload"></p>
<p>在TD学习中，算法在估计某一个状态的价值时，用的是离开该状态的即刻奖励 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D" class="lazyload"> 与下一状态 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D" class="lazyload"> 的预估状态价值乘以衰减系数(<img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%5Cgamma" class="lazyload">)组成，这符合Bellman方程的描述：</p>
<p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=V%28S_%7Bt%7D%29+%5Cleftarrow+V%28S_%7Bt%7D%29+%2B+%5Calpha+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+V%28S_%7Bt%2B1%7D%29+-+V%28S_%7Bt%7D%29%29" class="lazyload"></p>
<p><strong>式中：</strong></p>
<p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D+%2B+%5Cgamma+V%28S_%7Bt%2B1%7D%29+" class="lazyload"> 称为 TD目标值</p>
<p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%5Cdelta_%7Bt%7D+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+V%28S_%7Bt%2B1%7D%29+-+V%28S_%7Bt%7D%29" class="lazyload"> 称为TD误差</p>
<p><strong>BootStrapping</strong> 指的就是TD目标值 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D+%2B+%5Cgamma+V%28S_%7Bt%2B1%7D%29+" class="lazyload"> 代替收获 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=G_t" class="lazyload"> 的过程，暂时把它翻译成“<strong>引导”</strong>。</p>
<p>下面用一个例子直观解释蒙特卡洛策略评估和TD策略评估的差别。</p>
<ul>
<li>示例——驾车返家</li>
</ul>
<p>想象一下你下班后开车回家，需要预估整个行程花费的时间。假如一个人在驾车回家的路上突然碰到险情：对面迎来一辆车感觉要和你相撞，严重的话他可能面临死亡威胁，但是最后双方都采取了措施没有实际发生碰撞。如果使用蒙特卡洛学习，路上发生的这一险情可能引发的负向奖励不会被考虑进去，不会影响总的预测耗时；但是在TD学习时，碰到这样的险情，这个人会立即更新这个状态的价值，随后会发现这比之前的状态要糟糕，会立即考虑决策降低速度赢得时间，也就是说你不必像蒙特卡洛学习那样直到他死亡后才更新状态价值，那种情况下也无法更新状态价值。</p>
<p>TD算法相当于在整个返家的过程中（一个Episode），根据已经消耗的时间和预期还需要的时间来不断更新最终回家需要消耗的时间。</p>
<p><img alt="img" data-src="https://pic4.zhimg.com/80/v2-bdbd1f09610b6a393f1fe67c0710f3ff_hd.png" class="lazyload"></p>
<p>基于上表所示的数据，下图展示了蒙特卡洛学习和TD学习两种不同的学习策略来<strong>更新</strong>价值函数（各个状态的价值）。这里使用的是<strong>从某个状态预估的到家还需耗时</strong>来<strong>间接</strong>反映某状态的价值：某位置预估的到家时间越长，该位置价值越低，在优化决策时需要避免进入该状态。对于蒙特卡洛学习过程，驾驶员在路面上碰到各种情况时，他不会更新对于回家的预估时间，等他回到家得到了真实回家耗时后，他会重新估计在返家的路上着每一个主要节点状态到家的时间，在下一次返家的时候用新估计的时间来帮助决策；而对于TD学习，在一开始离开办公室的时候你可能会预估总耗时30分钟，但是当你取到车发现下雨的时候，你会立刻想到原来的预计过于乐观，因为既往的经验告诉你下雨会延长你的返家总时间，此时你会更新目前的状态价值估计，从原来的30分钟提高到40分钟。同样当你驾车离开高速公路时，会一路根据当前的状态（位置、路况等）对应的预估返家剩余时间，直到返回家门得到实际的返家总耗时。这一过程中，你会根据状态的变化实时更新该状态的价值。</p>
<p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-b22c26e56989df79a740dd5f8570733d_hd.png" class="lazyload"></p>
<p>通过这个例子，我们可以直观的了解到：</p>
<p><strong>MC对比 TD之一</strong></p>
<p>TD 在知道结果之前可以学习，MC必须等到最后结果才能学习；</p>
<p>TD 可以在没有结果时学习，可以在持续进行的环境里学习。</p>
<p><strong>MC对比 TD之二</strong></p>
<p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=G_%7Bt%7D" class="lazyload"> ：实际收获，是基于某一策略状态价值的<strong>无偏</strong>估计</p>
<p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-0569204d117959cbfbac2f68d32408ad_hd.png" class="lazyload"></p>
<p>TD target：TD目标值，是基于下一状态<strong>预估</strong>价值计算的当前预估收获，是当前状态实际价值的<strong>有偏</strong>估计</p>
<p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-0bbf74adc3e1e9f0c2ae0686ac7a667c_hd.png" class="lazyload"></p>
<p>True TD target： 真实TD目标值，是基于下一状态的<strong>实际</strong>价值对当前状态实际价值的无偏估计</p>
<p><img alt="img" data-src="https://pic4.zhimg.com/80/v2-4976a516e6b77d124f8c1959957ca333_hd.png" class="lazyload"></p>
<p>MC 没有偏倚（bias），但有着较高的变异性（Variance），且对初始值不敏感；</p>
<p>TD 低变异性variance, 但有一定程度的bias，对初始值较敏感，通常比 MC 更高效；</p>
<p>这里的偏倚指的是距离期望的距离，预估的平均值与实际平均值的偏离程度；变异性指的是方差，评估单次采样结果相对于与平均值变动的范围大小。基本就是统计学上均值与方差的概念。</p>
<p>对于MC和TD的区别，还可以用下面的例子来加深理解：</p>
<ul>
<li>示例——随机行走</li>
</ul>
<p><strong>状态空间</strong>：如下图：A、B、C、D、E为中间状态，C同时作为起始状态。灰色方格表示终止状态；</p>
<p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-9b8fc4a4eaba3fd7bf426cedbb9338b1_hd.png" class="lazyload"></p>
<p><strong>行为空间</strong>：除终止状态外，任一状态可以选择向左、向右两个行为之一；</p>
<p><strong>即时奖励：</strong>右侧的终止状态得到即时奖励为1，左侧终止状态得到的即时奖励为0，在其他状态间转化得到的即时奖励是0；</p>
<p><strong>状态转移</strong>：100%按行为进行状态转移，进入终止状态即终止；</p>
<p><strong>衰减系数：</strong>1；</p>
<p><strong>给定的策略</strong>：随机选择向左、向右两个行为。</p>
<p><strong>问题：</strong>对这个MDP问题进行预测，也就是评估随机行走这个策略的价值，也就是计算该策略下每个状态的价值，也就是确定该MDP问题的状态价值函数。</p>
<p><strong>求解：</strong>下图是使用TD算法得到的结果。横坐标显示的是状态，纵坐标是各状态的价值估计，一共5条折线，数字表明的是实际经历的Episode数量，true value所指的那根折线反映的是各状态的实际价值。第0次时，各状态的价值被初始化为0.5，经过1次、10次、100次后得到的价值函数越来越接近实际状态价值函数。</p>
<p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-a7bf900d61bb5862430ce4692c193a24_hd.png" class="lazyload"></p>
<p>下图比较了MC和TD算法的效率。横坐标是经历的Episode数量，纵坐标是计算得到的状态函数和实际状态函数下各状态价值的均方差。黑色是MC算法在不同step-size下的学习曲线，灰色的曲线使用TD算法。可以看出TD较MC更高效。此图还可以看出当step-size不是非常小的情况下，TD有可能得不到最终的实际价值，将会在某一区间震荡。</p>
<p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-8800b621c86c52d4e05874461d7c7278_hd.png" class="lazyload"></p>
<ul>
<li><strong>示例——AB</strong></li>
</ul>
<p><strong>已知：</strong>现有两个状态(A和B)，MDP未知，衰减系数为1，有如下表所示8个完整Episode的经验及对应的即时奖励，其中除了第1个Episode有状态转移外，其余7个均只有一个状态。</p>
<p><img alt="img" data-src="https://pic3.zhimg.com/80/v2-da98b709419947f9f28a6ad8fcb06486_hd.png" class="lazyload"></p>
<p><strong>问题：依据仅有的Episode，计算状态A，B的价值分别是多少，即</strong>V(A)=？， V(B)=？</p>
<p><strong>答案：</strong>V(B) = 6/8，V(A)根据不同算法结果不同，用MC算法结果为0，TD则得出6/8。</p>
<p><strong>解释：</strong>应用MC算法，由于需要完整的Episode,因此仅Episode1可以用来计算A的状态价值，很明显是0；同时B的价值是6/8。应用TD算法时，TD算法试图利用现有的Episode经验构建一个MDP（如下图），由于存在一个Episode使得状态A有后继状态B，因此状态A的价值是通过状态B的价值来计算的，同时经验表明A到B的转移概率是100%，且A状态的即时奖励是0，并且没有衰减，因此A的状态价值等于B的状态价值。</p>
<p><img alt="img" data-src="https://pic4.zhimg.com/80/v2-ad94d8c9a13f7b4e4280cd2b09c074eb_hd.png" class="lazyload"></p>
<p>MC算法试图收敛至一个能够最小化状态价值与实际收获的均方差的解决方案，这一均方差用公式表示为：</p>
<p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-9bb06379d7391f62f6b67da9fee6558c_hd.png" class="lazyload"></p>
<p>式中， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=k" class="lazyload"> 表示的是Episode序号， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=K" class="lazyload"> 为总的Episode数量， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=t" class="lazyload"> 为一个Episode内状态序号（第1,2,3…个状态等）， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=T_%7Bk%7D" class="lazyload"> 表示的是第 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=k" class="lazyload"> 个Episode总的状态数， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=G%5E%7Bk%7D_%7Bt%7D" class="lazyload"> 表示第 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=k" class="lazyload"> 个Episode里 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=t" class="lazyload"> 时刻状态 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" class="lazyload"> 获得的最终收获， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=V%28S%5E%7Bk%7D_%7Bt%7D%29" class="lazyload"> 表示的是第 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=k" class="lazyload"> 个Episode里算法估计的 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=t" class="lazyload"> 时刻状态 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=S_%7Bt%7D" class="lazyload"> 的价值。</p>
<p>TD算法则收敛至一个根据已有经验构建的最大可能的马儿可夫模型的状态价值，也就是说TD算法将首先根据已有经验估计状态间的转移概率：</p>
<p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-7be23c64954a02bc102cc88554b2d884_hd.png" class="lazyload"></p>
<p>同时估计某一个状态的即时奖励：</p>
<p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-b8e42986cb35b500c09d81272467bbcd_hd.png" class="lazyload"></p>
<p>最后计算该MDP的状态函数。</p>
<p><strong>MC对比 TD之三</strong></p>
<p>通过比较可以看出，TD算法使用了MDP问题的马儿可夫属性，在Markov 环境下更有效；但是MC算法并不利用马儿可夫属性，通常在非Markov环境下更有效。</p>
<ul>
<li>小结——三种强化学习算法</li>
</ul>
<p>Monte-Carlo, Temporal-Difference 和 Dynamic Programming 都是计算状态价值的一种方法，区别在于，前两种是在不知道Model的情况下的常用方法，这其中又以MC方法需要一个完整的Episode来更新状态价值，TD则不需要完整的Episode；DP方法则是基于Model（知道模型的运作方式）的计算状态价值的方法，它通过计算一个状态S所有可能的转移状态S’及其转移概率以及对应的即时奖励来计算这个状态S的价值。</p>
<p><strong>关于是否Bootstrap：</strong>MC 没有引导数据，只使用实际收获；DP和TD都有引导数据。</p>
<p><strong>关于是否用样本来计算:</strong> MC和TD都是应用样本来估计实际的价值函数；而DP则是利用模型直接计算得到实际价值函数，没有样本或采样之说。</p>
<p>下面的几张图直观地体现了三种算法的区别：</p>
<p><img alt="img" data-src="https://pic4.zhimg.com/80/v2-bf2dc5da52e18c9222553fc23a172b43_hd.png" class="lazyload"></p>
<p>MC: 采样，一次完整经历，用实际收获更新状态预估价值</p>
<p><img alt="img" data-src="https://pic4.zhimg.com/80/v2-07dbd91e923c03f6bc6d467f3eecb2ef_hd.png" class="lazyload"></p>
<p>TD：采样，经历可不完整，用喜爱状态的预估状态价值预估收获再更新预估价值</p>
<p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-c8c5d1ce8f68a3188002d1cd31060694_hd.png" class="lazyload"></p>
<p>DP：没有采样，根据完整模型，依靠预估数据更新状态价值</p>
<p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-80efa588b6886c9c48ab34bc39fd6169_hd.png" class="lazyload"></p>
<p>上图从两个维度解释了四种算法的差别，多了一个穷举法。这两个维度分别是：采样深度和广度。当使用单个采样，同时不走完整个Episode就是TD；当使用单个采样但走完整个Episode就是MC；当考虑全部样本可能性，但对每一个样本并不走完整个Episode时，就是DP；当既考虑所有Episode又把Episode从开始到终止遍历完，就变成了穷举法。</p>
<p>需要提及的是：DP利用的是整个MDP问题的模型，也就是状态转移概率，虽然它并不实际利用样本，但是它利用了整个模型的规律，因此认为是Full Width的。</p>
<h2 id="TD-λ"><a href="#TD-λ" class="headerlink" title="TD(λ)"></a><strong>TD(λ)</strong></h2><p>先前所介绍的TD算法实际上都是TD(0)算法，括号内的数字0表示的是在当前状态下往前多看1步，要是往前多看2步更新状态价值会怎样？这就引入了n-step的概念。</p>
<ul>
<li><strong>n-步预测 n-Step Prediction</strong></li>
</ul>
<p>在当前状态往前行动n步，计算n步的return，同样TD target 也由2部分组成，已走的步数使用确定的即时reward，剩下的使用估计的状态价值替代。</p>
<p><img alt="img" data-src="https://pic3.zhimg.com/80/v2-d4b66252355a47cf227042f678e792ba_hd.png" class="lazyload"></p>
<p>注：图中空心大圆圈表示状态，实心小圆圈表示行为</p>
<ul>
<li>n-步收获</li>
</ul>
<p>TD或TD(0)是基于1-步预测的，MC则是基于∞-步预测的：</p>
<p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-193c513a3b9be5b96c60e1d858d6313d_hd.png" class="lazyload"></p>
<p>注意：n=2时不写成TD(2)。</p>
<p>定义<strong>n-步收获</strong>：</p>
<p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-22e9b0f8fed11728d32b31cdf472b4b1_hd.png" class="lazyload"></p>
<p>那么，n步TD学习状态价值函数的更新公式为：</p>
<p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-1a7e16e1df80f61d16cd626165f90121_hd.png" class="lazyload"></p>
<p>既然存在n-步预测，那么n=？时效果最好呢，下面的例子试图回答这个问题：</p>
<ul>
<li><strong>示例——大规模随机行走</strong></li>
</ul>
<p>这个示例研究了使用多个不同步数预测联合不同步长(step-size，公式里的系数α）时，分别在在线和离线状态时状态函数均方差的差别。所有研究使用了10个Episode。离线与在线的区别在于，离线是在经历所有10个Episode后进行状态价值更新；而在线则至多经历一个Episode就更新依次状态价值。</p>
<p><img alt="img" data-src="https://pic4.zhimg.com/80/v2-5a9ca14701dfc2ba4ab870d4b739ae43_hd.png" class="lazyload"></p>
<p>结果如图表明，离线和在线之间曲线的形态差别不明显；从步数上来看，步数越长，越接近MC算法，均方差越大。对于这个大规模随机行走示例，在线计算比较好的步数是3-5步，离线计算比较好的是6-8步。但是不同的问题其对应的比较高效的步数不是一成不变的。因此选择多少步数作为一个较优的计算参数也是一个问题。</p>
<p>这里我们引入了一个新的参数：λ。通过引入这个新的参数，可以做到在不增加计算复杂度的情况下综合考虑所有步数的预测。这就是<strong>λ预测</strong>和<strong>λ收获。</strong></p>
<ul>
<li><strong>λ-收获</strong></li>
</ul>
<p>λ-收获 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=G_%7Bt%7D%5E%7B%5Clambda%7D+" class="lazyload"> 综合考虑了从 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=1" class="lazyload"> 到 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%5Cinfty" class="lazyload"> 的所有步收获，它给其中的任意一个 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=n-" class="lazyload"> 步收获施加一定的权重 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%281-%5Clambda%29%5Clambda%5E%7Bn-1%7D" class="lazyload"> 。通过这样的权重设计，得到如下的公式：</p>
<p><img alt="img" data-src="https://pic3.zhimg.com/80/v2-eae5a4f432e0b884afd593e92e82191e_hd.png" class="lazyload"></p>
<p>对应的λ-预测写成TD(λ):</p>
<p><img alt="img" data-src="https://pic3.zhimg.com/80/v2-c39faa426f8458e051eec8764222e4be_hd.png" class="lazyload"></p>
<p>下图是各步收获的权重分配图，图中最后一列λ的指数是 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=T-t-1" class="lazyload"> 。 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=T" class="lazyload"> 为终止状态的时刻步数， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=t" class="lazyload"> 为当前状态的时刻步数，所有的权重加起来为1。</p>
<p><img alt="img" data-src="https://pic3.zhimg.com/80/v2-c6c2dd5a7c1992bb3e31df0b63d9ce96_hd.png" class="lazyload"></p>
<ul>
<li><strong>TD(λ)对于权重分配的图解</strong></li>
</ul>
<p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-b56f0f8461ad39d8225ffb34a5fc8f98_hd.png" class="lazyload"></p>
<p>这张图还是比较好理解，例如对于n=3的3-步收获，赋予其在 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=%5Clambda" class="lazyload"> 收获中的权重如左侧阴影部分面积，对于终止状态的T-步收获，T以后的<strong>所有</strong>阴影部分面积。而所有节段面积之和为1。这种几何级数的设计也考虑了算法实现的计算方便性。</p>
<p>TD((λ)的设计使得Episode中，后一个状态的状态价值与之前所有状态的状态价值有关，同时也可以说成是一个状态价值参与决定了后续所有状态的状态价值。但是每个状态的价值对于后续状态价值的影响权重是不同的。我们可以从两个方向来理解TD(λ)：</p>
<p><strong>前向认识TD(λ)</strong></p>
<p>引入了λ之后，会发现要更新一个状态的状态价值，必须要走完整个Episode获得每一个状态的即时奖励以及最终状态获得的即时奖励。这和MC算法的要求一样，因此TD(λ)算法有着和MC方法一样的劣势。λ取值区间为[0,1]，当λ=1时对应的就是MC算法。这个实际计算带来了不便。</p>
<p><strong>反向认识TD(λ)</strong></p>
<p>TD(λ)从另一方面提供了一个单步更新的机制，通过下面的示例来说明。</p>
<ul>
<li>示例——被电击的原因</li>
</ul>
<p>这是之前见过的一个例子，老鼠在连续接受了3次响铃和1次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？</p>
<p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-a9a7d918998aae00799d8334ad343504_hd.png" class="lazyload"></p>
<p>两个概念：</p>
<p><strong>频率启发 Frequency heuristic：</strong>将原因归因于出现频率最高的状态</p>
<p><strong>就近启发 Recency heuristic：</strong>将原因归因于较近的几次状态</p>
<p>给每一个状态引入一个数值：<strong>效用追踪</strong>（<strong>Eligibility Traces, ES，也有翻译成“资质追踪”，这是同一个概念从两个不同的角度理解得到的不同翻译</strong>），可以结合上述两个启发。定义：</p>
<p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=E_%7B0%7D%28s%29+%3D+0" class="lazyload"></p>
<p><img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=E_%7Bt%7D%28s%29+%3D+%5Cgamma+%5Clambda+E_%7Bt-1%7D%28s%29+%2B+1%28S_%7Bt%7D+%3D+s%29" class="lazyload"></p>
<p>其中 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=1%28S_%7Bt%7D+%3D+s%29" class="lazyload"> 是一个条件判断表达式。</p>
<p>下图给出了 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=E_%7Bt%7D%28s%29" class="lazyload"> 对于 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=t" class="lazyload"> 的一个可能的曲线图：</p>
<p><img alt="img" data-src="https://pic3.zhimg.com/80/v2-12e398763ecfbc5fd8ebf178746a1b76_hd.png" class="lazyload"></p>
<p>该图横坐标是时间，横坐标下有竖线的位置代表当前进入了状态 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=s" class="lazyload"> ，纵坐标是效用追踪值 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=E" class="lazyload"> 。可以看出当某一状态连续出现，E值会在一定衰减的基础上有一个单位数值的提高，此时将增加该状态对于最终收获贡献的比重，因而在更新该状态价值的时候可以较多地考虑最终收获的影响。同时如果该状态距离最终状态较远，则其对最终收获的贡献越小，在更新该状态时也不需要太多的考虑最终收获。</p>
<p>特别的， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=E" class="lazyload"> 值并不需要等到完整的Episode结束才能计算出来，它可以每经过一个时刻就得到更新。 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=E" class="lazyload"> 值存在饱和现象，有一个瞬时最高上限： <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=E_%7Bmax%7D+%3D+1%2F%281-%5Cgamma%5Clambda%29+" class="lazyload"></p>
<p>把刚才的描述体现在公式里更新状态价值，是这样的：</p>
<p><img alt="img" data-src="https://pic1.zhimg.com/80/v2-02bab51822814b27feb4999ddfe65288_hd.png" class="lazyload"></p>
<p><img alt="img" data-src="https://pic3.zhimg.com/80/v2-f6e9ffeae24fa2068a47a18d9bbf9e6a_hd.png" class="lazyload"></p>
<p>注：每一个状态都有一个 <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=E" class="lazyload"> 值， <img alt="[公式]" data-src="https://www.zhihu.com/equation?tex=E" class="lazyload"> 值随时间而变化。</p>
<p>当λ=0时，只有当前状态得到更新，等同于TD(0)算法；</p>
<p>当λ=1时，TD(1)粗略看与每次访问的MC算法等同；在线更新时，状态价值差每一步都会有积累；离线更新时，TD(1)等同于MC算法。</p>
<p>注：ET是一个非常符合神经科学相关理论的、非常精巧的设计。把它看成是神经元的一个参数，它反映了神经元对某一刺激的敏感性和适应性。神经元在接受刺激时会有反馈，在持续刺激时反馈一般也比较强，当间歇一段时间不刺激时，神经元又逐渐趋于静息状态；同时不论如何增加刺激的频率，神经元有一个最大饱和反馈。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>下表给出了λ取各种值时，不同算法在不同情况下的关系。</p>
<p><img alt="img" data-src="https://pic2.zhimg.com/80/v2-4678c7d4ecc09488af4eb067e2908ae5_hd.png" class="lazyload"></p>
<p>相较于MC算法，TD算法应用更广，是一个非常有用的强化学习方法，在下一讲讲解控制相关的算法时会详细介绍TD算法的实现。</p>
<h1 id="策略求解"><a href="#策略求解" class="headerlink" title="策略求解"></a>策略求解</h1></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">程子卿</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://orangesching.github.io/2019/12/02/强化学习基础——基于无模型的强化学习方法/">https://orangesching.github.io/2019/12/02/强化学习基础——基于无模型的强化学习方法/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://orangesching.github.io">程子卿的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/机器学习/">机器学习    </a><a class="post-meta__tags" href="/tags/强化学习/">强化学习    </a></div><div class="post_share"><div class="social-share" data-image="/img/cover/cover2.gif" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2019/11/30/强化学习基础——基于模型的动态规划方法/"><img class="next_cover lazyload" data-src="/img/cover/cover.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>强化学习基础——基于模型的动态规划方法</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/11/21/强化学习基础——强化学习问题提出/" title="强化学习基础——强化学习问题提出"><img class="relatedPosts_cover lazyload" data-src="/img/cover/cover.png"><div class="relatedPosts_title">强化学习基础——强化学习问题提出</div></a></div><div class="relatedPosts_item"><a href="/2019/11/08/强化学习导论/" title="强化学习导论"><img class="relatedPosts_cover lazyload" data-src="/img/cover/cover.png"><div class="relatedPosts_title">强化学习导论</div></a></div><div class="relatedPosts_item"><a href="/2019/11/30/强化学习基础——基于模型的动态规划方法/" title="强化学习基础——基于模型的动态规划方法"><img class="relatedPosts_cover lazyload" data-src="/img/cover/cover.png"><div class="relatedPosts_title">强化学习基础——基于模型的动态规划方法</div></a></div><div class="relatedPosts_item"><a href="/2019/11/22/强化学习基础——强化学习问题描述/" title="强化学习基础——强化学习问题描述"><img class="relatedPosts_cover lazyload" data-src="/img/cover/cover.png"><div class="relatedPosts_title">强化学习基础——强化学习问题描述</div></a></div></div><div class="clear_both"></div></div></div></div><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2019 By 程子卿</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="/js/third-party/canvas-nest.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script></body></html>