<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>强化学习算法 | 大橙砸的日常记录</title><meta name="description" content="强化学习算法"><meta name="keywords" content="强化学习"><meta name="author" content="大橙砸"><meta name="copyright" content="大橙砸"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon2.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="强化学习算法"><meta name="twitter:description" content="强化学习算法"><meta name="twitter:image" content="https://orangesching.github.io/img/cover/cover2.gif"><meta property="og:type" content="article"><meta property="og:title" content="强化学习算法"><meta property="og:url" content="https://orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/"><meta property="og:site_name" content="大橙砸的日常记录"><meta property="og:description" content="强化学习算法"><meta property="og:image" content="https://orangesching.github.io/img/cover/cover2.gif"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/"><link rel="prev" title="JUNIT测试框架" href="https://orangesching.github.io/2020/08/12/测试/测试技术/单元测试/JUNIT测试框架/"><link rel="next" title="强化学习实践——程序建模模板及Gym使用" href="https://orangesching.github.io/2019/12/05/算法/机器学习/强化学习/强化学习实践1——程序建模模板及Gym使用/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: undefined,
  medium_zoom: 'false',
  Snackbar: undefined
  
}</script></head><body><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">大橙砸的日常记录</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="/img/my.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">50</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">23</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">28</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Q学习（离线学习）"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">Q学习（离线学习）</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#算法"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">算法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#算法理论——Q值更新公式"><span class="toc_mobile_items-number">1.1.1.</span> <span class="toc_mobile_items-text">算法理论——Q值更新公式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#算法伪代码"><span class="toc_mobile_items-number">1.1.2.</span> <span class="toc_mobile_items-text">算法伪代码</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#例"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">例</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#代码"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">代码</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Sarsa"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">Sarsa</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#算法-1"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">算法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#算法理论——Q值更新公式-1"><span class="toc_mobile_items-number">2.1.1.</span> <span class="toc_mobile_items-text">算法理论——Q值更新公式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#算法伪代码-1"><span class="toc_mobile_items-number">2.1.2.</span> <span class="toc_mobile_items-text">算法伪代码</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#例-1"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">例</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#代码-1"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">代码</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Sarsa-λ"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">Sarsa(λ)</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#算法-2"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">算法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#算法理论"><span class="toc_mobile_items-number">3.1.1.</span> <span class="toc_mobile_items-text">算法理论</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#n步sarsa"><span class="toc_mobile_items-number">3.1.1.1.</span> <span class="toc_mobile_items-text">n步sarsa</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#sarsa-λ"><span class="toc_mobile_items-number">3.1.1.2.</span> <span class="toc_mobile_items-text">sarsa(λ)</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#算法伪代码-2"><span class="toc_mobile_items-number">3.1.2.</span> <span class="toc_mobile_items-text">算法伪代码</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#例-2"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">例</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#代码-2"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">代码</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#DQN（Deep-Q-learning）"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">DQN（Deep Q learning）</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Q学习（离线学习）"><span class="toc-number">1.</span> <span class="toc-text">Q学习（离线学习）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#算法"><span class="toc-number">1.1.</span> <span class="toc-text">算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法理论——Q值更新公式"><span class="toc-number">1.1.1.</span> <span class="toc-text">算法理论——Q值更新公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法伪代码"><span class="toc-number">1.1.2.</span> <span class="toc-text">算法伪代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#例"><span class="toc-number">1.2.</span> <span class="toc-text">例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码"><span class="toc-number">1.3.</span> <span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sarsa"><span class="toc-number">2.</span> <span class="toc-text">Sarsa</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#算法-1"><span class="toc-number">2.1.</span> <span class="toc-text">算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法理论——Q值更新公式-1"><span class="toc-number">2.1.1.</span> <span class="toc-text">算法理论——Q值更新公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法伪代码-1"><span class="toc-number">2.1.2.</span> <span class="toc-text">算法伪代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#例-1"><span class="toc-number">2.2.</span> <span class="toc-text">例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码-1"><span class="toc-number">2.3.</span> <span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sarsa-λ"><span class="toc-number">3.</span> <span class="toc-text">Sarsa(λ)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#算法-2"><span class="toc-number">3.1.</span> <span class="toc-text">算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法理论"><span class="toc-number">3.1.1.</span> <span class="toc-text">算法理论</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#n步sarsa"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">n步sarsa</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sarsa-λ"><span class="toc-number">3.1.1.2.</span> <span class="toc-text">sarsa(λ)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法伪代码-2"><span class="toc-number">3.1.2.</span> <span class="toc-text">算法伪代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#例-2"><span class="toc-number">3.2.</span> <span class="toc-text">例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码-2"><span class="toc-number">3.3.</span> <span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DQN（Deep-Q-learning）"><span class="toc-number">4.</span> <span class="toc-text">DQN（Deep Q learning）</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/cover/cover2.gif)"><div id="post-info"><div id="post-title"><div class="posttitle">强化学习算法</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-12-06<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-11-12</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/算法/">算法</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/算法/机器学习/">机器学习</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/算法/机器学习/强化学习/">强化学习</a></span><div class="post-meta-wordcount"><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="Q学习（离线学习）"><a href="#Q学习（离线学习）" class="headerlink" title="Q学习（离线学习）"></a>Q学习（离线学习）</h1><p>Q学习是一种基于TD(0)的离线策略时序差分算法。</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="算法理论——Q值更新公式"><a href="#算法理论——Q值更新公式" class="headerlink" title="算法理论——Q值更新公式"></a>算法理论——Q值更新公式</h3><p>Q学习使用一张Q表来存储当前状态-行为价值，即存储在当前状态做这个行为未来可获得的收益。为了更新这张表，我们需要计算这个预估的未来奖励（Q值）和我们实际获得的奖励有多大的误差。</p>
<ul>
<li><p>预估的未来奖励，就是Q表中的数值</p>
</li>
<li><p>实际获得的奖励</p>
<p>直观上想，可以使用从这一步出发到终点的奖励来计算。但是这样需要整个奖励序列，不利于学习（这个思想是基于蒙托卡罗的思想）。</p>
<p>换一种思路，我们只利用当前步的奖励，再加上<strong>下一步的预估未来奖励（下一步的Q值）</strong>，不就是有点实际的实际获得奖励吗</p>
</li>
</ul>
<p>用公式表示这个误差就是</p>
<script type="math/tex; mode=display">
R _ { t + 1 } + \gamma Q \left( S'  , A ^ { \prime } \right) - Q \left( S _ { t } , A _ { t } \right)</script><p>由于下一步的预估未来奖励与具体的行动有关系，而我们要获取的是一个具体的值，所以我们在下一步选择行动的时候才用纯贪婪算法，即在下一状态$S_{t+1}$时选择动作的策略变成了贪婪Q最大：</p>
<script type="math/tex; mode=display">
\pi \left( S _ { t + 1 } \right) = \underset { a ^ { \prime } } { \operatorname { argmax } } Q \left( S ' , a ^ { \prime } \right)</script><p>这样$Q \left( S <em> { t + 1 } , A ^ { \prime } \right)$就可以变成具体的$\underset { a ^ { \prime } } { \operatorname { max } } Q \left( S </em> { t + 1 } , a ^ { \prime } \right)$，Q值更新公式就变成了</p>
<script type="math/tex; mode=display">
Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma \max _ { a ^ { \prime } } Q \left( S ^ { \prime } , a ^ { \prime } \right) - Q ( S , A ) \right)</script><h3 id="算法伪代码"><a href="#算法伪代码" class="headerlink" title="算法伪代码"></a>算法伪代码</h3><p><img alt="Q学习算法" data-src="//orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/Q学习算法.png" class="lazyload"></p>
<p>Q学习中动作的选择策略是这样的：当前在状态$S$选择行动使用的是Ɛ-贪婪策略，获得了奖励R到达了状态 $s’$ 。现在要评估误差时，需要使用下一步的状态-行为值，这时对$s’$的策略决策使用的是纯贪婪算法，才能获得具体的下一步状态-行为值$\max _ { a ^ { \prime } } Q \left( S ^ { \prime } , a ^ { \prime } \right) $</p>
<p>$\alpha$是学习率, 来决定这次的误差有多少是要被学习的, $\alpha$是一个小于1 的数. </p>
<p>$\gamma$ 是 reward 的衰减衰减系数</p>
<h2 id="例"><a href="#例" class="headerlink" title="例"></a>例</h2><p>举给女朋友送礼物的例子，你女朋友的状态有三种（高兴/没反应/不高兴）下面是获得的奖励（奖励只与状态有关，离开这个状态就能获得）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>状态</th>
<th>奖励（R）</th>
</tr>
</thead>
<tbody>
<tr>
<td>高兴</td>
<td>1</td>
</tr>
<tr>
<td>没反应</td>
<td>0</td>
</tr>
<tr>
<td>不高兴</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>为了记录我在这个情况下做某件事可能获得多少的好感度，这样方便我以后决策，我就可以画一个Q表，假设你当前的Q表是这样的</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>当前状态（S）</th>
<th>送礼物</th>
<th>不送礼物</th>
<th>揍一顿</th>
</tr>
</thead>
<tbody>
<tr>
<td>高兴</td>
<td>5</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>没反应</td>
<td>4</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>不高兴</td>
<td>3</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>现在，你女朋友来姨妈不高兴了，你想做点什么。看看Q表。诶！送礼物的价值最大，送个礼物。然后你送了他一个热水瓶。女朋友看完就放那了（女朋友的状态取决于女朋友，你不知道他会变成什么状态）。进过这次事之后，你想更新下你的Q表。怎么更新？看看公式</p>
<script type="math/tex; mode=display">
Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma \max _ { a ^ { \prime } } Q \left( S ^ { \prime } , a ^ { \prime } \right) - Q ( S , A ) \right)</script><p>首先我理想中会获得的好感度$Q(S,A)$是3；现实是离开【不高兴】这个状态，获得了奖励0，到达了新的状态【没反应】，我预估下我在【没反应】能做的最有意义的事还是送礼，没反应-送礼这个操作Q值是4，即$\max _ { a ^ { \prime } } Q \left( S ^ { \prime } , a ^ { \prime } \right) = 4$。假设$\gamma,\alpha=1$，那现实和理想的差距就是$（0+1<em>4-3）$，更新$Q（不高兴，送礼物）= 3+1</em>(0+1*4-3)=4$</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><img alt="Q学习算法例" data-src="//orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/Q学习算法例.png" class="lazyload"></p>
<p>玩上述游戏，黄色圆为宝藏，黑色方框为黑洞，进入黑洞奖励为-1，获得宝藏奖励+1，其他情况奖励为0。</p>
<p>迭代代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 学习 100 回合</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化 state 的观测值</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 更新可视化环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 大脑根据 state 的观测值挑选 action</span></span><br><span class="line">            action = RL.choose_action(str(observation))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 从这个序列 (state, action, reward, state_) 中学习</span></span><br><span class="line">            RL.learn(str(observation), action, reward, str(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state 的值传到下一次循环</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果掉下地狱或者升上天堂, 这回合就结束了</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 结束游戏并关闭窗口</span></span><br><span class="line">    print(<span class="string">'game over'</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 定义环境 env 和 RL 方式</span></span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = QLearningTable(actions=list(range(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始可视化环境 env</span></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>
<p>Q学习代码（个体类）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">        self.actions = actions  <span class="comment"># a list</span></span><br><span class="line">        self.lr = learning_rate <span class="comment">#学习速率</span></span><br><span class="line">        self.gamma = reward_decay <span class="comment">#衰减系数</span></span><br><span class="line">        self.epsilon = e_greedy <span class="comment">#以多少概率去探索</span></span><br><span class="line">        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)  <span class="comment">#Q表，Q学习的关键！</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#依据策略（ε-贪婪策略）选择一个动作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">        self.check_state_exist(observation)<span class="comment">#如果状态没遇到过就加入Q表</span></span><br><span class="line">        <span class="comment"># action selection</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon: </span><br><span class="line">            <span class="comment"># choose best action</span></span><br><span class="line">            state_action = self.q_table.loc[observation, :]</span><br><span class="line">            <span class="comment"># 不直接使用max函数，防止相同值时每次都选择同一个动作</span></span><br><span class="line">            action = np.random.choice(state_action[state_action == np.max(state_action)].index)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># choose random action</span></span><br><span class="line">            action = np.random.choice(self.actions)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="comment">#更新Q表中的值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, s, a, r, s_)</span>:</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.loc[s, a]  <span class="comment">#估计值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#计算现实值</span></span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">'terminal'</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, :].max()  <span class="comment"># next state is not terminal</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># next state is terminal</span></span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="comment"># update</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#检查是否有此状态，没有就添加</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            <span class="comment"># append new state to q table</span></span><br><span class="line">            self.q_table = self.q_table.append(</span><br><span class="line">                pd.Series(</span><br><span class="line">                    [<span class="number">0</span>]*len(self.actions),</span><br><span class="line">                    index=self.q_table.columns,</span><br><span class="line">                    name=state,</span><br><span class="line">                )</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<h1 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h1><h2 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h2><h3 id="算法理论——Q值更新公式-1"><a href="#算法理论——Q值更新公式-1" class="headerlink" title="算法理论——Q值更新公式"></a>算法理论——Q值更新公式</h3><p>SARSA的名称来源于一个的序列描述：针对一个状态$S$，以及一个特定的行为，$A$进而产生一个状态行为对($SA$)，与环境交互，环境收到个体的行为后会告诉个体即时奖励 $R$ 以及后续进入的状态 $S’$；接下来个体遵循<strong>现有策略</strong>产生一个行为 $A’$（下一次行动），根据当前的<strong>状态行为价值函数</strong>得到后一个状态行为对($S’A’$)的价值（$Q$），利用这个 $Q$ 值更新前一个状态行为对( $SA$ )的价值。Sarsa使用的是单步更新</p>
<p>Sarsa与Q学习类似同样使用<strong>Ɛ-贪婪探索</strong>的形式来改善策略，但Q表的更新公式不同。Q学习在$S’$状态的决策策略是纯贪婪策略，且决策出的动作不一定是真实执行的动作所以Q学习中Q现实的计算为$R+\gamma\underset { a ^ { \prime } } { \operatorname { max } } Q \left( S _ { t + 1 } , a ^ { \prime } \right)$；而Sarsa在$S$状态和$S’$状态的决策策略都是Ɛ-贪婪策略，且在$S’$决策出来的动作就是下一个执行的动作，所以Sarsa中Q现实的计算为$R + \gamma Q \left( S ^ { \prime } , A ^ { \prime } \right)$，有这个特性Sarsa也是一个On-policy学习算法</p>
<script type="math/tex; mode=display">
Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma Q \left( S ^ { \prime } , A ^ { \prime } \right) - Q ( S , A ) \right)</script><p>这个公式决定了Q学习比Sarsa更加大胆的特性</p>
<h3 id="算法伪代码-1"><a href="#算法伪代码-1" class="headerlink" title="算法伪代码"></a>算法伪代码</h3><p><img alt="Sarsa算法" data-src="//orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/Sarsa算法.png" class="lazyload"></p>
<p>$\alpha$是学习率, 来决定这次的误差有多少是要被学习的, $\alpha$是一个小于1 的数. </p>
<p>$\gamma$ 是 reward 的衰减衰减系数</p>
<h2 id="例-1"><a href="#例-1" class="headerlink" title="例"></a>例</h2><p>还是上面的例子，重新写一遍</p>
<p>举给女朋友送礼物的例子，你女朋友的状态有三种（高兴/没反应/不高兴）下面是获得的奖励（奖励只与状态有关，离开这个状态就能获得）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>状态</th>
<th>奖励（R）</th>
</tr>
</thead>
<tbody>
<tr>
<td>高兴</td>
<td>1</td>
</tr>
<tr>
<td>没反应</td>
<td>0</td>
</tr>
<tr>
<td>不高兴</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>为了记录我在这个情况下做某件事可能获得多少的好感度，这样方便我以后决策，我就可以画一个Q表，假设你当前的Q表是这样的</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>当前状态（S）</th>
<th>送礼物</th>
<th>不送礼物</th>
<th>揍一顿</th>
</tr>
</thead>
<tbody>
<tr>
<td>高兴</td>
<td>5</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>没反应</td>
<td>4</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>不高兴</td>
<td>3</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>现在，你女朋友来姨妈不高兴了，你想做点什么。看看Q表。诶！送礼物的价值最大，送个礼物。然后你送了他一个热水瓶。女朋友看完就放那了（女朋友的状态取决于女朋友，你不知道他会变成什么状态）。进过这次事之后，你想更新下你的Q表。怎么更新？看看公式</p>
<script type="math/tex; mode=display">
Q ( S , A ) \leftarrow Q ( S , A ) + \alpha \left( R + \gamma Q \left( S ^ { \prime } , A ^ { \prime } \right) - Q ( S , A ) \right)</script><p>首先我理想中会获得的好感度$Q(S,A)$是3；现实是离开【不高兴】这个状态，获得了奖励0，到达了新的状态【没反应】，我在【没反应】能做的事可以有两种：一是随便做件事看看反应，二是选Q值最大的动作（送礼）。我最后决定随便做件事（揍她一顿）没反应-揍一顿这个操作Q值是0，即$Q \left( S ^ { \prime } , A ^ { \prime } \right)=0$。假设$\gamma,\alpha=1$，那现实和理想的差距就是$（0+1<em>0-3）$，更新$Q（不高兴，送礼）= 3+1</em>(0+1*0-3)=0$，下一步我要做的事是揍一顿</p>
<h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><p><img alt="Q学习算法例" data-src="//orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/Q学习算法例.png" class="lazyload"></p>
<p>玩上述游戏，黄色圆为宝藏，黑色方框为黑洞，进入黑洞奖励为-1，获得宝藏奖励+1，其他情况奖励为0。</p>
<p>迭代代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 学习 100 回合</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化 state 的观测值</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 更新可视化环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 大脑根据 state 的观测值挑选 action</span></span><br><span class="line">            action = RL.choose_action(str(observation))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 决策下一个行动，与Q学习不同</span></span><br><span class="line">            action_ = RL.choose_action(str(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 从这个序列 (state, action, reward, state_) 中学习</span></span><br><span class="line">            RL.learn(str(observation), action, reward, str(observation_), action_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state 的值传到下一次循环</span></span><br><span class="line">            observation = observation_</span><br><span class="line">            action = action_ <span class="comment">#与Q学习不同，这个动作真实执行了</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果掉下地狱或者升上天堂, 这回合就结束了</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 结束游戏并关闭窗口</span></span><br><span class="line">    print(<span class="string">'game over'</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 定义环境 env 和 RL 方式</span></span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = QLearningTable(actions=list(range(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始可视化环境 env</span></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>
<p>Sarsa学习代码（个体类）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaTable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">        self.actions = actions  <span class="comment"># a list</span></span><br><span class="line">        self.lr = learning_rate <span class="comment">#学习速率</span></span><br><span class="line">        self.gamma = reward_decay <span class="comment">#衰减系数</span></span><br><span class="line">        self.epsilon = e_greedy <span class="comment">#以多少概率去探索</span></span><br><span class="line">        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)  <span class="comment">#Q表，学习的关键！</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#依据策略（ε-贪婪策略）选择一个动作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">        self.check_state_exist(observation)<span class="comment">#如果状态没遇到过就加入Q表</span></span><br><span class="line">        <span class="comment"># action selection</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon: </span><br><span class="line">            <span class="comment"># choose best action</span></span><br><span class="line">            state_action = self.q_table.loc[observation, :]</span><br><span class="line">            <span class="comment"># 不直接使用max函数，防止相同值时每次都选择同一个动作</span></span><br><span class="line">            action = np.random.choice(state_action[state_action == np.max(state_action)].index)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># choose random action</span></span><br><span class="line">            action = np.random.choice(self.actions)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="comment">#更新Q表中的值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, s, a, r, s_， a_)</span>:</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.ix[s, a]  <span class="comment">#估计值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#计算现实值</span></span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">'terminal'</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.ix[s_, a_] <span class="comment"># next state is not terminal</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># next state is terminal</span></span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="comment"># update</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#检查是否有此状态，没有就添加</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            <span class="comment"># append new state to q table</span></span><br><span class="line">            self.q_table = self.q_table.append(</span><br><span class="line">                pd.Series(</span><br><span class="line">                    [<span class="number">0</span>]*len(self.actions),</span><br><span class="line">                    index=self.q_table.columns,</span><br><span class="line">                    name=state,</span><br><span class="line">                )</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<h1 id="Sarsa-λ"><a href="#Sarsa-λ" class="headerlink" title="Sarsa(λ)"></a>Sarsa(λ)</h1><h2 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h2><h3 id="算法理论"><a href="#算法理论" class="headerlink" title="算法理论"></a>算法理论</h3><p>Q-Learning 和 Sarsa 都是在得到奖励后只更新上一步状态和动作对应的 Q 表值，是单步更新算法，也就是 Sarsa(0)。但是在得到当前奖励值后之前所走的每一步（即一个轨迹）都多多少少和最终得到的奖励值有关，所以不应该只更新上一步状态对应的 Q 值。于是就有了多步更新算法——Sarsa(n)。当 n 的值为一个回合（episode）的步数时就变成了回合更新。对于多步更新的 Sarsa 算法我们用 Sarsa(λ)来统一表示，其中 $\lambda$ 的取值范围是 [ 0 , 1 ]，其本质是一个衰减值。</p>
<h4 id="n步sarsa"><a href="#n步sarsa" class="headerlink" title="n步sarsa"></a>n步sarsa</h4><p>之前的sarsa只使用一步的真实奖励来估计现实Q收货，如果我多用几步呢？</p>
<p>定义<strong>n-步Q收获（Q-return）</strong>为前n个奖励的总和加上在n个步骤中达到的状态的估计值，每个步骤都适当地折扣：</p>
<script type="math/tex; mode=display">
G _ { t:t+n }= R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots + \gamma ^ { n - 1 } R _ { t + n } + \gamma ^ { n } Q_{t+n-1} \left( S _ { t + n } , A_{t+n}\right)</script><p>则可以把Sarsa用n-步Q收获来表示得到n步Sarsa更新公式，如下式：</p>
<script type="math/tex; mode=display">
Q \left( S _ { t } , A _ { t } \right) \leftarrow Q \left( S _ { t } , A _ { t } \right) + \alpha \left( G _ { t:t+n } - Q \left( S _ { t } , A _ { t } \right) \right)</script><h4 id="sarsa-λ"><a href="#sarsa-λ" class="headerlink" title="sarsa(λ)"></a>sarsa(λ)</h4><p>后来注意到有效更新不仅可以针对任何 $n$ 步回报， 而且可以针对不同 $n$ 的任何平均 $n$ 步返回。 例如，可以对目标进行更新，该目标是两步回报的一半和四步回报的一半： $\frac{1}{2}G<em>{t:t+2}+\frac{1}{2}G</em>{t:t+4}$。 任何一组 $n$ 步返回都可以用这种方式平均，即使是无限集，<strong>只要分量返回的权重为正并且总和为1</strong>。</p>
<p>sarsa(λ)算法可以理解为平均 $n$ 步更新的一种特定方式。 该平均值包含所有 $n$ 步更新，每个更新按比例加权到 $\lambda^{n-1}$（其中$ \lambda \in [0,1]$），并按因子 $1−\lambda$ 归一化，以确保权重总和为1。 结果更新是针对回报，称为 λ回报：</p>
<script type="math/tex; mode=display">
q _ { t } ^ { \lambda } = ( 1 - \lambda ) \sum _ { n = 1 } ^ { \infty } \lambda ^ { n - 1 } G _ { t:t+n }</script><p>同上面说的$\frac{1}{2}G<em>{t:t+2}+\frac{1}{2}G</em>{t:t+4}$例子一样，这个公式只是给每一步的G加了权值而已，且这个权值按照距离当前状态的远近程度赋值，距离远的权值大，距离近的权值小</p>
<p>现在用这个λ回报来更新Q值，则可以得到Sarsa(λ)的更新公式</p>
<script type="math/tex; mode=display">
Q \left( S _ { t } , A _ { t } \right) \leftarrow Q \left( S _ { t } , A _ { t } \right) + \alpha \left( q _ { t } ^ { \lambda } - Q \left( S _ { t } , A _ { t } \right) \right)</script><p>但这个公式太复杂了，还得存所有Episode，不方便学习。为了解决这个问题，我们引入效用追踪（Eligibility Trace）概念。不同的是这次的E值针对的不是一个状态，而是一个状态行为对：</p>
<script type="math/tex; mode=display">
\begin{array} { c } { E _ { 0 } ( s , a ) = 0 } \\ { E _ { t } ( s , a ) = \gamma \lambda E _ { t - 1 } ( s , a ) + 1 \left( S _ { t } = s , A _ { t } = a \right) } \end{array}</script><p>它体现的是一个结果与某一个状态行为对的因果关系，与得到结果最近的状态行为对，以及那些在此之前频繁发生的状态行为对对得到这个结果的影响最大。</p>
<p>下式是引入ET概念的SARSA(λ)之后的Q值更新描述：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
E _ { t } ( s , a ) &= \gamma \lambda E _ { t - 1 } ( s , a ) + 1 \left( S _ { t } = s , A _ { t } = a \right)\\
\delta _ { t } = & R _ { t + 1 } + \gamma Q \left( S _ { t + 1 } , A _ { t + 1 } \right) - Q \left( S _ { t } , A _ { t } \right) \\ 
& Q ( s , a ) \leftarrow Q ( s , a ) + \alpha \delta _ { t } E _ { t } ( s , a ) 
\end{aligned}</script><p>引入ET概念，同时使用SARSA(λ)将可以更有效的在线学习，因为不必要学习完整的Episode，数据用完即可丢弃。ET通常也是更多应用在在线学习算法中(online algorithm)。</p>
<h3 id="算法伪代码-2"><a href="#算法伪代码-2" class="headerlink" title="算法伪代码"></a>算法伪代码</h3><p><img alt="Sarsa(λ)算法" data-src="//orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/Sarsa(λ" class="lazyload">算法.png)</p>
<p>Sarsa(λ)在算法中引入了E表，更新时要更新Q表和E表</p>
<h2 id="例-2"><a href="#例-2" class="headerlink" title="例"></a>例</h2><h2 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h2><p><img alt="Q学习算法例" data-src="//orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/Q学习算法例.png" class="lazyload"></p>
<p>玩上述游戏，黄色圆为宝藏，黑色方框为黑洞，进入黑洞奖励为-1，获得宝藏奖励+1，其他情况奖励为0。</p>
<p>迭代代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 学习 100 回合</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化 state 的观测值</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line">	    RL.eligibility_trace *= <span class="number">0</span>   <span class="comment">#eligibility trace 只是记录每个回合的每一步, 新回合开始的时候需要将 Trace 清零</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 更新可视化环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 大脑根据 state 的观测值挑选 action</span></span><br><span class="line">            action = RL.choose_action(str(observation))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 决策下一个行动，与Q学习不同</span></span><br><span class="line">            action_ = RL.choose_action(str(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 从这个序列 (state, action, reward, state_) 中学习</span></span><br><span class="line">            RL.learn(str(observation), action, reward, str(observation_), action_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state 的值传到下一次循环</span></span><br><span class="line">            observation = observation_</span><br><span class="line">            action = action_ <span class="comment">#与Q学习不同，这个动作真实执行了</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果掉下地狱或者升上天堂, 这回合就结束了</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 结束游戏并关闭窗口</span></span><br><span class="line">    print(<span class="string">'game over'</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 定义环境 env 和 RL 方式</span></span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = QLearningTable(actions=list(range(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始可视化环境 env</span></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>
<p>Sarsa(λ)学习代码（个体类）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        self.actions = actions  <span class="comment"># a list</span></span><br><span class="line">        self.lr = learning_rate  <span class="comment"># 学习速率</span></span><br><span class="line">        self.gamma = reward_decay  <span class="comment"># 衰减系数</span></span><br><span class="line">        self.epsilon = e_greedy  <span class="comment"># 以多少概率去探索</span></span><br><span class="line">        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)  <span class="comment"># Q表，学习的关键！</span></span><br><span class="line">        self.lambda_ = trace_decay</span><br><span class="line">        self.eligibility_trace = self.q_table.copy()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 依据策略（ε-贪婪策略）选择一个动作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, observation)</span>:</span></span><br><span class="line">        self.check_state_exist(observation)  <span class="comment"># 如果状态没遇到过就加入Q表</span></span><br><span class="line">        <span class="comment"># action selection</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            <span class="comment"># choose best action</span></span><br><span class="line">            state_action = self.q_table.loc[observation, :]</span><br><span class="line">            <span class="comment"># 不直接使用max函数，防止相同值时每次都选择同一个动作</span></span><br><span class="line">            action = np.random.choice(state_action[state_action == np.max(state_action)].index)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># choose random action</span></span><br><span class="line">            action = np.random.choice(self.actions)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新Q表中的值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, s, a, r, s_, a_)</span>:</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.ix[s, a]  <span class="comment"># 估计值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算现实值</span></span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">'terminal'</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.ix[s_, a_]  <span class="comment"># next state is not terminal</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># next state is terminal</span></span><br><span class="line">        error = q_target - q_predict  <span class="comment"># update</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Method 1:</span></span><br><span class="line">        self.eligibility_trace.ix[s, a] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Method 2:</span></span><br><span class="line">        self.eligibility_trace.ix[s, a] *= <span class="number">0</span></span><br><span class="line">        self.eligibility_trace.ix[s, a] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q update</span></span><br><span class="line">        self.q_table += self.lr * error * self.eligibility_trace</span><br><span class="line"></span><br><span class="line">        self.eligibility_trace *= self.gamma * self.lambda_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查是否有此状态，没有就添加</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            to_be_append = pd.Series(</span><br><span class="line">                [<span class="number">0</span>] * len(self.actions),</span><br><span class="line">                index=self.q_table.columns,</span><br><span class="line">                name=state,</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># append new state to q table</span></span><br><span class="line">            self.q_table = self.q_table.append(to_be_append)</span><br><span class="line">            <span class="comment"># append new state to e table</span></span><br><span class="line">            self.eligibility_trace = self.eligibility_trace.append(to_be_append)</span><br></pre></td></tr></table></figure>
<p><img alt="Sarsa(λ)两种更新算法" data-src="//orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/Sarsa(λ" class="lazyload">两种更新算法.png)</p>
<h1 id="DQN（Deep-Q-learning）"><a href="#DQN（Deep-Q-learning）" class="headerlink" title="DQN（Deep Q learning）"></a>DQN（Deep Q learning）</h1></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">大橙砸</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/">https://orangesching.github.io/2019/12/06/算法/机器学习/强化学习/强化学习算法/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://orangesching.github.io">大橙砸的日常记录</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/强化学习/">强化学习    </a></div><div class="post_share"><div class="social-share" data-image="/img/cover/cover2.gif" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/08/12/测试/测试技术/单元测试/JUNIT测试框架/"><img class="prev_cover lazyload" data-src="/img/cover/cover2.gif" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>JUNIT测试框架</span></div></a></div><div class="next-post pull_right"><a href="/2019/12/05/算法/机器学习/强化学习/强化学习实践1——程序建模模板及Gym使用/"><img class="next_cover lazyload" data-src="/img/cover/cover.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>强化学习实践——程序建模模板及Gym使用</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/11/22/算法/机器学习/强化学习/强化学习基础2——强化学习问题描述/" title="强化学习基础2——强化学习问题描述"><img class="relatedPosts_cover lazyload" data-src="/img/cover/cover2.gif"><div class="relatedPosts_title">强化学习基础2——强化学习问题描述</div></a></div><div class="relatedPosts_item"><a href="/2019/11/30/算法/机器学习/强化学习/强化学习基础3——基于模型的动态规划方法/" title="强化学习基础3——基于模型的动态规划方法"><img class="relatedPosts_cover lazyload" data-src="/img/cover/cover.png"><div class="relatedPosts_title">强化学习基础3——基于模型的动态规划方法</div></a></div><div class="relatedPosts_item"><a href="/2019/12/05/算法/机器学习/强化学习/强化学习实践1——程序建模模板及Gym使用/" title="强化学习实践——程序建模模板及Gym使用"><img class="relatedPosts_cover lazyload" data-src="/img/cover/cover.png"><div class="relatedPosts_title">强化学习实践——程序建模模板及Gym使用</div></a></div><div class="relatedPosts_item"><a href="/2019/12/04/算法/机器学习/强化学习/强化学习基础5——总结/" title="强化学习基础5——总结"><img class="relatedPosts_cover lazyload" data-src="/img/cover/cover2.gif"><div class="relatedPosts_title">强化学习基础5——总结</div></a></div><div class="relatedPosts_item"><a href="/2019/11/08/算法/机器学习/强化学习/强化学习导论/" title="强化学习导论"><img class="relatedPosts_cover lazyload" data-src="/img/cover/cover.png"><div class="relatedPosts_title">强化学习导论</div></a></div><div class="relatedPosts_item"><a href="/2019/12/02/算法/机器学习/强化学习/强化学习基础4——基于无模型的强化学习方法/" title="强化学习基础4——基于无模型的强化学习方法"><img class="relatedPosts_cover lazyload" data-src="/img/cover/cover.png"><div class="relatedPosts_title">强化学习基础4——基于无模型的强化学习方法</div></a></div></div><div class="clear_both"></div></div></div></div><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2021 By 大橙砸</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="/js/search/local-search.js"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="/js/third-party/canvas-nest.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>